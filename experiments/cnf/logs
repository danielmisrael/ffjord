/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 5.2199(5.2199) | Bit/dim 30.7890(30.7890) | Steps 406(406.00) | Grad Norm 229.6729(229.6729) | Total Time 10.00(10.00)
Iter 0010 | Time 2.9250(4.5964) | Bit/dim 28.3557(30.5327) | Steps 406(406.04) | Grad Norm 213.6236(228.1130) | Total Time 10.00(10.00)
Iter 0020 | Time 2.6278(4.1180) | Bit/dim 22.8424(29.1593) | Steps 400(405.37) | Grad Norm 161.2016(216.8189) | Total Time 10.00(10.00)
Iter 0030 | Time 2.6232(3.7313) | Bit/dim 17.2150(26.6279) | Steps 400(403.96) | Grad Norm 104.4029(193.7339) | Total Time 10.00(10.00)
Iter 0040 | Time 2.6198(3.4488) | Bit/dim 13.5379(23.5915) | Steps 400(402.92) | Grad Norm 57.6177(162.4140) | Total Time 10.00(10.00)
Iter 0050 | Time 2.6195(3.2312) | Bit/dim 10.9065(20.5281) | Steps 400(402.15) | Grad Norm 32.7756(130.8262) | Total Time 10.00(10.00)
Iter 0060 | Time 2.6208(3.0711) | Bit/dim 8.8331(17.6621) | Steps 400(401.59) | Grad Norm 20.1758(102.9658) | Total Time 10.00(10.00)
Iter 0070 | Time 2.6240(2.9538) | Bit/dim 7.3720(15.0905) | Steps 400(401.17) | Grad Norm 16.0665(80.5190) | Total Time 10.00(10.00)
Iter 0080 | Time 2.7980(2.9017) | Bit/dim 6.1489(12.8484) | Steps 406(402.02) | Grad Norm 13.6789(63.2636) | Total Time 10.00(10.00)
Iter 0090 | Time 2.6516(2.8467) | Bit/dim 4.9478(10.8781) | Steps 400(401.91) | Grad Norm 11.2757(49.8159) | Total Time 10.00(10.00)
Iter 0100 | Time 2.6557(2.7967) | Bit/dim 3.9561(9.1504) | Steps 400(401.41) | Grad Norm 9.3500(39.3725) | Total Time 10.00(10.00)
Iter 0110 | Time 2.8690(2.7787) | Bit/dim 3.3229(7.6793) | Steps 412(402.55) | Grad Norm 7.4316(31.2116) | Total Time 10.00(10.00)
Iter 0120 | Time 3.0275(2.8064) | Bit/dim 2.8548(6.4546) | Steps 418(405.21) | Grad Norm 4.9813(24.5871) | Total Time 10.00(10.00)
Iter 0130 | Time 3.0685(2.8753) | Bit/dim 2.6027(5.4651) | Steps 424(410.14) | Grad Norm 3.4202(19.1840) | Total Time 10.00(10.00)
Iter 0140 | Time 3.1102(2.9351) | Bit/dim 2.4199(4.6844) | Steps 430(414.93) | Grad Norm 2.2387(14.8499) | Total Time 10.00(10.00)
Iter 0150 | Time 3.2676(3.0112) | Bit/dim 2.2943(4.0726) | Steps 436(420.04) | Grad Norm 1.4103(11.4035) | Total Time 10.00(10.00)
Iter 0160 | Time 3.2719(3.0791) | Bit/dim 2.2948(3.6071) | Steps 436(424.23) | Grad Norm 1.0937(8.7289) | Total Time 10.00(10.00)
Iter 0170 | Time 3.2733(3.1298) | Bit/dim 2.2793(3.2535) | Steps 436(427.32) | Grad Norm 0.8811(6.6928) | Total Time 10.00(10.00)
Iter 0180 | Time 3.3247(3.1747) | Bit/dim 2.2126(2.9845) | Steps 442(430.60) | Grad Norm 0.8181(5.1521) | Total Time 10.00(10.00)
Iter 0190 | Time 3.3248(3.2131) | Bit/dim 2.2296(2.7854) | Steps 442(433.59) | Grad Norm 0.7320(3.9872) | Total Time 10.00(10.00)
Iter 0200 | Time 3.3266(3.2425) | Bit/dim 2.2021(2.6320) | Steps 442(435.80) | Grad Norm 0.6048(3.1081) | Total Time 10.00(10.00)
Iter 0210 | Time 3.3287(3.2656) | Bit/dim 2.2106(2.5127) | Steps 442(437.43) | Grad Norm 0.5482(2.4453) | Total Time 10.00(10.00)
Iter 0220 | Time 3.3306(3.2828) | Bit/dim 2.1396(2.4228) | Steps 442(438.63) | Grad Norm 0.5828(1.9528) | Total Time 10.00(10.00)
Iter 0230 | Time 3.1707(3.2688) | Bit/dim 2.1494(2.3556) | Steps 436(438.51) | Grad Norm 0.5510(1.5835) | Total Time 10.00(10.00)
Iter 0240 | Time 3.1891(3.2441) | Bit/dim 2.1344(2.3000) | Steps 436(437.85) | Grad Norm 0.4723(1.3045) | Total Time 10.00(10.00)
Iter 0250 | Time 3.1758(3.2259) | Bit/dim 2.1113(2.2572) | Steps 436(437.37) | Grad Norm 0.4689(1.0950) | Total Time 10.00(10.00)
Iter 0260 | Time 3.1778(3.2123) | Bit/dim 2.1248(2.2244) | Steps 436(437.01) | Grad Norm 0.5224(0.9447) | Total Time 10.00(10.00)
Iter 0270 | Time 3.0798(3.2000) | Bit/dim 2.0961(2.1988) | Steps 424(436.38) | Grad Norm 0.4620(0.8202) | Total Time 10.00(10.00)
Iter 0280 | Time 3.1644(3.1866) | Bit/dim 2.1486(2.1794) | Steps 436(435.65) | Grad Norm 0.4660(0.7247) | Total Time 10.00(10.00)
Iter 0290 | Time 3.0893(3.1737) | Bit/dim 2.0772(2.1588) | Steps 424(434.39) | Grad Norm 0.6045(0.6622) | Total Time 10.00(10.00)
validating...
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print('device', device)
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 4.0471(4.0471) | Bit/dim 29.0425(29.0425) | Steps 406(406.00) | Grad Norm 234.5916(234.5916) | Total Time 10.00(10.00)
Iter 0010 | Time 2.7944(3.7003) | Bit/dim 26.3894(28.7377) | Steps 406(405.05) | Grad Norm 211.5579(232.1727) | Total Time 10.00(10.00)
Iter 0020 | Time 2.7015(3.4366) | Bit/dim 20.3099(27.2207) | Steps 400(404.17) | Grad Norm 157.5224(218.7900) | Total Time 10.00(10.00)
Iter 0030 | Time 2.6375(3.2377) | Bit/dim 14.6642(24.5258) | Steps 400(403.08) | Grad Norm 88.8408(191.9870) | Total Time 10.00(10.00)
Iter 0040 | Time 2.6434(3.1003) | Bit/dim 11.7679(21.4464) | Steps 400(402.27) | Grad Norm 34.6565(156.0498) | Total Time 10.00(10.00)
Iter 0050 | Time 2.7127(2.9998) | Bit/dim 9.7264(18.5905) | Steps 400(401.67) | Grad Norm 18.6470(121.3512) | Total Time 10.00(10.00)
Iter 0060 | Time 2.6516(2.9407) | Bit/dim 8.3824(16.0555) | Steps 400(401.23) | Grad Norm 14.3265(93.5929) | Total Time 10.00(10.00)
Iter 0070 | Time 2.7526(2.8934) | Bit/dim 7.1232(13.8234) | Steps 400(400.91) | Grad Norm 15.2192(72.9500) | Total Time 10.00(10.00)
Iter 0080 | Time 2.6497(2.8327) | Bit/dim 5.9127(11.8719) | Steps 400(400.67) | Grad Norm 12.8990(57.4924) | Total Time 10.00(10.00)
Iter 0090 | Time 2.6597(2.7871) | Bit/dim 4.7824(10.1229) | Steps 400(400.49) | Grad Norm 10.4716(45.3649) | Total Time 10.00(10.00)
Iter 0100 | Time 2.7081(2.7608) | Bit/dim 3.8919(8.5835) | Steps 406(401.21) | Grad Norm 8.7076(35.9064) | Total Time 10.00(10.00)
Iter 0110 | Time 2.8745(2.7872) | Bit/dim 3.2422(7.2511) | Steps 412(403.91) | Grad Norm 6.7388(28.4886) | Total Time 10.00(10.00)
Iter 0120 | Time 2.8814(2.8115) | Bit/dim 2.8473(6.1407) | Steps 412(406.03) | Grad Norm 4.5350(22.4368) | Total Time 10.00(10.00)
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,8,8', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 19782
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 2.1478(2.1478) | Bit/dim 23.2540(23.2540) | Steps 400(400.00) | Grad Norm 73.4190(73.4190) | Total Time 10.00(10.00)
Iter 0010 | Time 0.9832(1.8398) | Bit/dim 23.0189(23.2363) | Steps 406(400.32) | Grad Norm 71.5889(73.1621) | Total Time 10.00(10.00)
Iter 0020 | Time 0.9674(1.6113) | Bit/dim 22.6117(23.1237) | Steps 400(400.55) | Grad Norm 70.4891(72.7021) | Total Time 10.00(10.00)
Iter 0030 | Time 0.9559(1.4382) | Bit/dim 21.7213(22.8687) | Steps 400(400.41) | Grad Norm 68.6073(71.8275) | Total Time 10.00(10.00)
Iter 0040 | Time 1.0177(1.3153) | Bit/dim 20.9008(22.4561) | Steps 400(400.62) | Grad Norm 65.1522(70.4501) | Total Time 10.00(10.00)
Iter 0050 | Time 0.9859(1.2264) | Bit/dim 19.6484(21.8598) | Steps 400(400.46) | Grad Norm 61.6565(68.6619) | Total Time 10.00(10.00)
Iter 0060 | Time 0.9902(1.1579) | Bit/dim 18.3806(21.1057) | Steps 400(400.34) | Grad Norm 57.8603(66.1759) | Total Time 10.00(10.00)
Iter 0070 | Time 1.0549(1.1125) | Bit/dim 17.1567(20.2054) | Steps 400(400.25) | Grad Norm 52.9400(63.2728) | Total Time 10.00(10.00)
Iter 0080 | Time 0.9824(1.0824) | Bit/dim 15.9863(19.2105) | Steps 400(400.18) | Grad Norm 47.4452(59.8664) | Total Time 10.00(10.00)
Iter 0090 | Time 0.9869(1.0592) | Bit/dim 14.6511(18.1449) | Steps 400(400.14) | Grad Norm 43.6385(56.1843) | Total Time 10.00(10.00)
Iter 0100 | Time 0.9645(1.0372) | Bit/dim 13.4709(17.0572) | Steps 400(400.10) | Grad Norm 39.1968(52.1300) | Total Time 10.00(10.00)
Iter 0110 | Time 0.9923(1.0200) | Bit/dim 12.3213(15.9481) | Steps 400(400.07) | Grad Norm 34.3925(47.9572) | Total Time 10.00(10.00)
Iter 0120 | Time 0.9767(1.0147) | Bit/dim 11.6617(14.8999) | Steps 406(400.40) | Grad Norm 28.5923(43.4552) | Total Time 10.00(10.00)
Iter 0130 | Time 0.9623(1.0068) | Bit/dim 10.6478(13.8726) | Steps 406(402.02) | Grad Norm 24.9146(39.0192) | Total Time 10.00(10.00)
Iter 0140 | Time 1.0283(1.0082) | Bit/dim 9.9036(12.9322) | Steps 406(403.21) | Grad Norm 20.6722(34.5476) | Total Time 10.00(10.00)
Iter 0150 | Time 0.9778(1.0060) | Bit/dim 9.1851(12.0270) | Steps 406(403.94) | Grad Norm 16.9267(30.3001) | Total Time 10.00(10.00)
Iter 0160 | Time 0.9619(0.9941) | Bit/dim 8.3348(11.1769) | Steps 412(404.97) | Grad Norm 14.0918(26.2918) | Total Time 10.00(10.00)
Iter 0170 | Time 0.9663(0.9889) | Bit/dim 8.2039(10.3900) | Steps 418(408.12) | Grad Norm 10.9029(22.6059) | Total Time 10.00(10.00)
Iter 0180 | Time 0.9871(0.9881) | Bit/dim 7.4770(9.6691) | Steps 418(410.71) | Grad Norm 9.4837(19.3482) | Total Time 10.00(10.00)
Iter 0190 | Time 0.9864(0.9885) | Bit/dim 6.6685(8.9822) | Steps 418(412.63) | Grad Norm 8.5286(16.6347) | Total Time 10.00(10.00)
Iter 0200 | Time 0.9868(0.9884) | Bit/dim 6.2676(8.3184) | Steps 418(414.04) | Grad Norm 8.0903(14.4526) | Total Time 10.00(10.00)
Iter 0210 | Time 0.9880(0.9885) | Bit/dim 5.6402(7.6832) | Steps 418(415.08) | Grad Norm 7.6642(12.7092) | Total Time 10.00(10.00)
Iter 0220 | Time 0.9500(0.9826) | Bit/dim 5.1638(7.0736) | Steps 406(414.47) | Grad Norm 7.2370(11.3385) | Total Time 10.00(10.00)
Iter 0230 | Time 0.9493(0.9757) | Bit/dim 4.5591(6.4867) | Steps 406(412.24) | Grad Norm 6.5194(10.1832) | Total Time 10.00(10.00)
Iter 0240 | Time 1.0550(0.9867) | Bit/dim 4.4454(5.9505) | Steps 424(413.88) | Grad Norm 6.2865(9.1803) | Total Time 10.00(10.00)
Iter 0250 | Time 1.0265(1.0050) | Bit/dim 3.9302(5.4543) | Steps 424(418.14) | Grad Norm 4.8880(8.1869) | Total Time 10.00(10.00)
Iter 0260 | Time 1.0442(1.0110) | Bit/dim 3.6394(5.0042) | Steps 424(419.84) | Grad Norm 3.8332(7.1503) | Total Time 10.00(10.00)
Iter 0270 | Time 1.0463(1.0175) | Bit/dim 3.4579(4.6140) | Steps 430(422.22) | Grad Norm 3.0001(6.1309) | Total Time 10.00(10.00)
Iter 0280 | Time 1.0326(1.0210) | Bit/dim 3.2377(4.2770) | Steps 436(425.08) | Grad Norm 2.3960(5.2018) | Total Time 10.00(10.00)
Iter 0290 | Time 1.0375(1.0221) | Bit/dim 3.0962(3.9873) | Steps 442(428.60) | Grad Norm 2.1059(4.4139) | Total Time 10.00(10.00)
validating...
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses.cpu())
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=1600, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,8,8', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(9, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 19782
===> Using batch size 1600. Total 37 iterations/epoch.
Iter 0000 | Time 6.8943(6.8943) | Bit/dim 35.4352(35.4352) | Steps 436(436.00) | Grad Norm 111.9659(111.9659) | Total Time 10.00(10.00)
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses.cpu())
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='1', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=1, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(2, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): SqueezeLayer()
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(2, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 587
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 1.3378(1.3378) | Bit/dim 25.8685(25.8685) | Steps 212(212.00) | Grad Norm 30.4479(30.4479) | Total Time 5.00(5.00)
Iter 0010 | Time 0.2938(1.0658) | Bit/dim 25.8642(25.8648) | Steps 206(211.16) | Grad Norm 30.4168(30.3841) | Total Time 5.00(5.00)
Iter 0020 | Time 0.2935(0.8650) | Bit/dim 25.8062(25.8523) | Steps 206(210.89) | Grad Norm 30.1700(30.3364) | Total Time 5.00(5.00)
Iter 0030 | Time 0.2966(0.7171) | Bit/dim 25.6048(25.8170) | Steps 206(210.66) | Grad Norm 29.8457(30.2870) | Total Time 5.00(5.00)
Iter 0040 | Time 0.3022(0.6075) | Bit/dim 25.4425(25.7587) | Steps 206(209.92) | Grad Norm 29.7405(30.2411) | Total Time 5.00(5.00)
Iter 0050 | Time 0.2941(0.5272) | Bit/dim 25.3545(25.6755) | Steps 206(209.53) | Grad Norm 30.2084(30.1857) | Total Time 5.00(5.00)
Iter 0060 | Time 0.2931(0.4675) | Bit/dim 25.2619(25.5687) | Steps 206(209.39) | Grad Norm 30.5077(30.2060) | Total Time 5.00(5.00)
Iter 0070 | Time 0.2871(0.4210) | Bit/dim 24.9478(25.4290) | Steps 206(208.78) | Grad Norm 30.4781(30.2353) | Total Time 5.00(5.00)
Iter 0080 | Time 0.2902(0.3880) | Bit/dim 24.7226(25.2646) | Steps 206(208.67) | Grad Norm 31.1308(30.3853) | Total Time 5.00(5.00)
Iter 0090 | Time 0.3078(0.3639) | Bit/dim 24.2849(25.0540) | Steps 218(209.23) | Grad Norm 31.1594(30.4850) | Total Time 5.00(5.00)
Iter 0100 | Time 0.2976(0.3460) | Bit/dim 23.8947(24.8053) | Steps 212(209.64) | Grad Norm 31.5891(30.7119) | Total Time 5.00(5.00)
Iter 0110 | Time 0.2963(0.3320) | Bit/dim 23.4978(24.5076) | Steps 212(209.64) | Grad Norm 32.4024(31.0358) | Total Time 5.00(5.00)
Iter 0120 | Time 0.2986(0.3236) | Bit/dim 22.9730(24.1715) | Steps 212(210.40) | Grad Norm 33.2807(31.5344) | Total Time 5.00(5.00)
Iter 0130 | Time 0.3157(0.3205) | Bit/dim 22.3705(23.7603) | Steps 218(211.95) | Grad Norm 34.5589(32.1535) | Total Time 5.00(5.00)
Iter 0140 | Time 0.2971(0.3167) | Bit/dim 21.5669(23.2811) | Steps 212(212.57) | Grad Norm 35.1895(32.8891) | Total Time 5.00(5.00)
Iter 0150 | Time 0.3067(0.3134) | Bit/dim 20.8138(22.7269) | Steps 212(213.00) | Grad Norm 36.9002(33.7661) | Total Time 5.00(5.00)
Iter 0160 | Time 0.3130(0.3126) | Bit/dim 19.8814(22.0856) | Steps 218(213.72) | Grad Norm 37.8369(34.7044) | Total Time 5.00(5.00)
Iter 0170 | Time 0.3055(0.3113) | Bit/dim 18.9161(21.3459) | Steps 212(213.74) | Grad Norm 38.4902(35.5929) | Total Time 5.00(5.00)
Iter 0180 | Time 0.3195(0.3117) | Bit/dim 17.6190(20.5009) | Steps 218(214.43) | Grad Norm 37.7367(36.2670) | Total Time 5.00(5.00)
Iter 0190 | Time 0.3069(0.3108) | Bit/dim 16.5091(19.5902) | Steps 212(213.46) | Grad Norm 37.7241(36.6501) | Total Time 5.00(5.00)
Iter 0200 | Time 0.2961(0.3093) | Bit/dim 15.3684(18.5990) | Steps 206(212.57) | Grad Norm 36.2224(36.6852) | Total Time 5.00(5.00)
Iter 0210 | Time 0.3066(0.3081) | Bit/dim 14.3096(17.5814) | Steps 212(211.95) | Grad Norm 34.2909(36.2574) | Total Time 5.00(5.00)
Iter 0220 | Time 0.2975(0.3075) | Bit/dim 13.2399(16.5529) | Steps 206(211.76) | Grad Norm 32.2329(35.4764) | Total Time 5.00(5.00)
Iter 0230 | Time 0.3056(0.3075) | Bit/dim 12.2209(15.5470) | Steps 212(211.86) | Grad Norm 30.4997(34.3669) | Total Time 5.00(5.00)
Iter 0240 | Time 0.3056(0.3073) | Bit/dim 11.4690(14.5810) | Steps 212(211.90) | Grad Norm 28.3393(32.9994) | Total Time 5.00(5.00)
Iter 0250 | Time 0.3181(0.3084) | Bit/dim 10.9266(13.6735) | Steps 212(211.93) | Grad Norm 26.0903(31.4546) | Total Time 5.00(5.00)
Iter 0260 | Time 0.3104(0.3082) | Bit/dim 10.5476(12.8505) | Steps 212(211.95) | Grad Norm 24.2876(29.7905) | Total Time 5.00(5.00)
Iter 0270 | Time 0.3082(0.3080) | Bit/dim 9.9123(12.0901) | Steps 212(211.96) | Grad Norm 22.6675(28.0994) | Total Time 5.00(5.00)
Iter 0280 | Time 0.3078(0.3079) | Bit/dim 9.1866(11.3875) | Steps 212(211.97) | Grad Norm 20.9364(26.4007) | Total Time 5.00(5.00)
Iter 0290 | Time 0.3073(0.3078) | Bit/dim 8.8610(10.7709) | Steps 212(211.98) | Grad Norm 19.3386(24.7305) | Total Time 5.00(5.00)
validating...
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='1', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=1, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(2, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): SqueezeLayer()
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(2, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 587
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 1.3185(1.3185) | Bit/dim 38.9384(38.9384) | Steps 224(224.00) | Grad Norm 107.5565(107.5565) | Total Time 5.00(5.00)
Iter 0010 | Time 0.3301(1.0580) | Bit/dim 38.7173(38.8187) | Steps 224(223.86) | Grad Norm 107.0939(106.9224) | Total Time 5.00(5.00)
Iter 0020 | Time 0.3275(0.8684) | Bit/dim 38.1635(38.6718) | Steps 224(223.90) | Grad Norm 102.9168(106.0491) | Total Time 5.00(5.00)
Iter 0030 | Time 0.3234(0.7300) | Bit/dim 37.8573(38.4993) | Steps 224(223.92) | Grad Norm 101.0554(104.9964) | Total Time 5.00(5.00)
Iter 0040 | Time 0.3415(0.6311) | Bit/dim 37.0478(38.2301) | Steps 224(223.78) | Grad Norm 95.5997(103.2301) | Total Time 5.00(5.00)
Iter 0050 | Time 0.3293(0.5531) | Bit/dim 36.8907(37.8651) | Steps 224(223.84) | Grad Norm 94.1976(100.7673) | Total Time 5.00(5.00)
Iter 0060 | Time 0.3261(0.4935) | Bit/dim 35.7879(37.4133) | Steps 224(223.71) | Grad Norm 86.7988(97.7472) | Total Time 5.00(5.00)
Iter 0070 | Time 0.3284(0.4496) | Bit/dim 35.2471(36.8931) | Steps 224(223.64) | Grad Norm 83.2825(94.2831) | Total Time 5.00(5.00)
Iter 0080 | Time 0.3255(0.4168) | Bit/dim 33.9427(36.2583) | Steps 224(223.45) | Grad Norm 75.1385(90.0574) | Total Time 5.00(5.00)
Iter 0090 | Time 0.3162(0.3911) | Bit/dim 33.2936(35.5447) | Steps 218(222.46) | Grad Norm 70.3954(85.3342) | Total Time 5.00(5.00)
Iter 0100 | Time 0.3278(0.3717) | Bit/dim 32.5962(34.8196) | Steps 224(221.47) | Grad Norm 66.1246(80.6003) | Total Time 5.00(5.00)
Iter 0110 | Time 0.3174(0.3562) | Bit/dim 31.4558(34.0567) | Steps 218(219.93) | Grad Norm 58.8653(75.6809) | Total Time 5.00(5.00)
Iter 0120 | Time 0.3159(0.3455) | Bit/dim 30.8428(33.2948) | Steps 218(219.14) | Grad Norm 55.0038(70.8010) | Total Time 5.00(5.00)
Iter 0130 | Time 0.3202(0.3373) | Bit/dim 30.1534(32.5436) | Steps 218(218.40) | Grad Norm 51.0661(66.0358) | Total Time 5.00(5.00)
Iter 0140 | Time 0.3107(0.3306) | Bit/dim 29.2399(31.7961) | Steps 212(216.71) | Grad Norm 45.5489(61.3874) | Total Time 5.00(5.00)
Iter 0150 | Time 0.2983(0.3240) | Bit/dim 28.7136(31.0483) | Steps 206(214.67) | Grad Norm 42.6442(56.8509) | Total Time 5.00(5.00)
Iter 0160 | Time 0.2877(0.3162) | Bit/dim 28.0680(30.3276) | Steps 200(211.62) | Grad Norm 39.4001(52.5652) | Total Time 5.00(5.00)
Iter 0170 | Time 0.2965(0.3097) | Bit/dim 27.5536(29.6424) | Steps 206(209.39) | Grad Norm 36.7652(48.6212) | Total Time 5.00(5.00)
Iter 0180 | Time 0.2853(0.3047) | Bit/dim 26.8110(28.9810) | Steps 200(207.74) | Grad Norm 33.3011(44.9381) | Total Time 5.00(5.00)
Iter 0190 | Time 0.2868(0.3003) | Bit/dim 26.3960(28.3557) | Steps 200(205.88) | Grad Norm 31.3661(41.5938) | Total Time 5.00(5.00)
Iter 0200 | Time 0.2952(0.2992) | Bit/dim 25.8978(27.7512) | Steps 206(205.78) | Grad Norm 29.1110(38.5090) | Total Time 5.00(5.00)
Iter 0210 | Time 0.2942(0.2979) | Bit/dim 25.3493(27.1762) | Steps 206(205.56) | Grad Norm 27.0194(35.7419) | Total Time 5.00(5.00)
Iter 0220 | Time 0.2888(0.2966) | Bit/dim 25.0049(26.6219) | Steps 200(205.16) | Grad Norm 25.7771(33.2000) | Total Time 5.00(5.00)
Iter 0230 | Time 0.2867(0.2952) | Bit/dim 24.3810(26.0838) | Steps 200(204.43) | Grad Norm 24.0324(30.9543) | Total Time 5.00(5.00)
Iter 0240 | Time 0.2866(0.2938) | Bit/dim 23.9014(25.5621) | Steps 200(203.59) | Grad Norm 22.6901(28.9315) | Total Time 5.00(5.00)
Iter 0250 | Time 0.2825(0.2918) | Bit/dim 23.4433(25.0696) | Steps 200(202.80) | Grad Norm 21.7887(27.1684) | Total Time 5.00(5.00)
Iter 0260 | Time 0.3003(0.2918) | Bit/dim 23.1035(24.5805) | Steps 206(202.71) | Grad Norm 20.7426(25.5692) | Total Time 5.00(5.00)
Iter 0270 | Time 0.2879(0.2919) | Bit/dim 22.5380(24.0982) | Steps 200(202.47) | Grad Norm 19.8572(24.1373) | Total Time 5.00(5.00)
Iter 0280 | Time 0.3113(0.2930) | Bit/dim 22.0903(23.6244) | Steps 218(203.70) | Grad Norm 18.9981(22.8543) | Total Time 5.00(5.00)
Iter 0290 | Time 0.3000(0.2949) | Bit/dim 21.5575(23.1469) | Steps 212(206.04) | Grad Norm 17.8632(21.6810) | Total Time 5.00(5.00)
validating...
Epoch 0001 | Time 6.4869, Bit/dim 21.2034
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0300 | Time 0.2955(0.2953) | Bit/dim 21.1763(22.6850) | Steps 206(207.07) | Grad Norm 17.3520(20.6242) | Total Time 5.00(5.00)
Iter 0310 | Time 0.3023(0.2960) | Bit/dim 20.7164(22.2252) | Steps 212(207.58) | Grad Norm 16.6219(19.6789) | Total Time 5.00(5.00)
Iter 0320 | Time 0.3041(0.2962) | Bit/dim 20.3334(21.7799) | Steps 212(207.82) | Grad Norm 16.2849(18.8516) | Total Time 5.00(5.00)
Iter 0330 | Time 0.3030(0.2994) | Bit/dim 19.9144(21.3377) | Steps 212(208.78) | Grad Norm 15.9011(18.0969) | Total Time 5.00(5.00)
Iter 0340 | Time 0.3046(0.3003) | Bit/dim 19.5041(20.9017) | Steps 212(209.63) | Grad Norm 15.3922(17.4308) | Total Time 5.00(5.00)
Iter 0350 | Time 0.3059(0.3017) | Bit/dim 19.0662(20.4717) | Steps 212(210.25) | Grad Norm 14.8953(16.8403) | Total Time 5.00(5.00)
Iter 0360 | Time 0.3109(0.3040) | Bit/dim 18.7011(20.0446) | Steps 218(212.01) | Grad Norm 14.7535(16.2907) | Total Time 5.00(5.00)
Iter 0370 | Time 0.3111(0.3068) | Bit/dim 18.2460(19.6237) | Steps 218(213.76) | Grad Norm 14.1934(15.8039) | Total Time 5.00(5.00)
Iter 0380 | Time 0.3419(0.3132) | Bit/dim 17.8178(19.2087) | Steps 236(217.96) | Grad Norm 13.9742(15.3711) | Total Time 5.00(5.00)
Iter 0390 | Time 0.3302(0.3200) | Bit/dim 17.4683(18.8005) | Steps 230(222.21) | Grad Norm 13.8254(14.9957) | Total Time 5.00(5.00)
Iter 0400 | Time 0.3396(0.3256) | Bit/dim 17.1065(18.3971) | Steps 236(225.83) | Grad Norm 13.7056(14.6670) | Total Time 5.00(5.00)
Iter 0410 | Time 0.3357(0.3287) | Bit/dim 16.7638(18.0035) | Steps 236(228.50) | Grad Norm 13.6252(14.3980) | Total Time 5.00(5.00)
Iter 0420 | Time 0.3399(0.3319) | Bit/dim 16.2435(17.6072) | Steps 236(230.47) | Grad Norm 13.2518(14.1545) | Total Time 5.00(5.00)
Iter 0430 | Time 0.3398(0.3342) | Bit/dim 15.9393(17.2141) | Steps 236(231.92) | Grad Norm 13.4590(13.9702) | Total Time 5.00(5.00)
Iter 0440 | Time 0.3398(0.3364) | Bit/dim 15.5612(16.8281) | Steps 236(233.17) | Grad Norm 13.4118(13.8322) | Total Time 5.00(5.00)
Iter 0450 | Time 0.3426(0.3376) | Bit/dim 15.1420(16.4365) | Steps 242(235.05) | Grad Norm 13.5026(13.7354) | Total Time 5.00(5.00)
Iter 0460 | Time 0.3413(0.3389) | Bit/dim 14.8026(16.0437) | Steps 242(236.87) | Grad Norm 13.6263(13.6765) | Total Time 5.00(5.00)
Iter 0470 | Time 0.3502(0.3411) | Bit/dim 14.3988(15.6506) | Steps 242(238.37) | Grad Norm 13.8007(13.6777) | Total Time 5.00(5.00)
Iter 0480 | Time 0.3506(0.3436) | Bit/dim 14.0226(15.2540) | Steps 248(240.32) | Grad Norm 13.9487(13.7219) | Total Time 5.00(5.00)
Iter 0490 | Time 0.3599(0.3459) | Bit/dim 13.5345(14.8468) | Steps 254(242.11) | Grad Norm 14.1018(13.8006) | Total Time 5.00(5.00)
Iter 0500 | Time 0.3662(0.3506) | Bit/dim 13.0428(14.4304) | Steps 254(245.13) | Grad Norm 14.3472(13.9112) | Total Time 5.00(5.00)
Iter 0510 | Time 0.3661(0.3543) | Bit/dim 12.5702(14.0084) | Steps 254(247.46) | Grad Norm 14.4378(14.0691) | Total Time 5.00(5.00)
Iter 0520 | Time 0.3577(0.3563) | Bit/dim 12.1766(13.5724) | Steps 248(248.17) | Grad Norm 14.8356(14.2457) | Total Time 5.00(5.00)
Iter 0530 | Time 0.3596(0.3575) | Bit/dim 11.6606(13.1247) | Steps 248(248.13) | Grad Norm 14.9776(14.4374) | Total Time 5.00(5.00)
Iter 0540 | Time 0.3485(0.3572) | Bit/dim 11.1460(12.6639) | Steps 248(248.09) | Grad Norm 15.0997(14.6118) | Total Time 5.00(5.00)
Iter 0550 | Time 0.3830(0.3584) | Bit/dim 10.6713(12.1900) | Steps 254(248.42) | Grad Norm 15.1067(14.7610) | Total Time 5.00(5.00)
Iter 0560 | Time 0.3639(0.3610) | Bit/dim 10.1599(11.7084) | Steps 260(250.21) | Grad Norm 15.1144(14.8696) | Total Time 5.00(5.00)
Iter 0570 | Time 0.3740(0.3614) | Bit/dim 9.5915(11.2093) | Steps 266(252.39) | Grad Norm 15.0175(14.9328) | Total Time 5.00(5.00)
Iter 0580 | Time 0.3748(0.3646) | Bit/dim 9.0610(10.7124) | Steps 266(255.96) | Grad Norm 14.9201(14.9368) | Total Time 5.00(5.00)
Iter 0590 | Time 0.3725(0.3687) | Bit/dim 8.6067(10.2109) | Steps 266(259.18) | Grad Norm 14.5381(14.8596) | Total Time 5.00(5.00)
validating...
Epoch 0002 | Time 7.7323, Bit/dim 8.0957
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0600 | Time 0.4040(0.3700) | Bit/dim 8.1229(9.7120) | Steps 266(260.97) | Grad Norm 14.1894(14.7125) | Total Time 5.00(5.00)
Iter 0610 | Time 0.4134(0.3777) | Bit/dim 7.6974(9.2257) | Steps 266(262.29) | Grad Norm 13.5323(14.4736) | Total Time 5.00(5.00)
Iter 0620 | Time 0.3895(0.3812) | Bit/dim 7.2194(8.7477) | Steps 266(263.26) | Grad Norm 13.2217(14.1895) | Total Time 5.00(5.00)
Iter 0630 | Time 0.3894(0.3834) | Bit/dim 6.8543(8.2895) | Steps 266(264.15) | Grad Norm 12.4069(13.8141) | Total Time 5.00(5.00)
Iter 0640 | Time 0.3713(0.3830) | Bit/dim 6.4104(7.8469) | Steps 272(266.21) | Grad Norm 12.0416(13.3847) | Total Time 5.00(5.00)
Iter 0650 | Time 0.3713(0.3805) | Bit/dim 6.1281(7.4273) | Steps 272(267.73) | Grad Norm 11.3548(12.9098) | Total Time 5.00(5.00)
Iter 0660 | Time 0.3890(0.3795) | Bit/dim 5.7737(7.0294) | Steps 272(268.85) | Grad Norm 10.5570(12.3812) | Total Time 5.00(5.00)
Iter 0670 | Time 0.3727(0.3793) | Bit/dim 5.4902(6.6576) | Steps 272(269.84) | Grad Norm 9.9643(11.8253) | Total Time 5.00(5.00)
Iter 0680 | Time 0.4381(0.3839) | Bit/dim 5.2836(6.3115) | Steps 272(270.90) | Grad Norm 9.3161(11.2418) | Total Time 5.00(5.00)
Iter 0690 | Time 0.3854(0.3859) | Bit/dim 4.9416(5.9938) | Steps 278(272.31) | Grad Norm 8.7502(10.6382) | Total Time 5.00(5.00)
Iter 0700 | Time 0.3888(0.3875) | Bit/dim 4.8007(5.6954) | Steps 278(273.80) | Grad Norm 7.9966(10.0354) | Total Time 5.00(5.00)
Iter 0710 | Time 0.3896(0.3883) | Bit/dim 4.5722(5.4190) | Steps 278(274.91) | Grad Norm 7.4836(9.4274) | Total Time 5.00(5.00)
Iter 0720 | Time 0.3914(0.3901) | Bit/dim 4.3911(5.1632) | Steps 278(275.72) | Grad Norm 6.8138(8.8233) | Total Time 5.00(5.00)
Iter 0730 | Time 0.4127(0.3948) | Bit/dim 4.1249(4.9251) | Steps 278(276.32) | Grad Norm 6.3604(8.2246) | Total Time 5.00(5.00)
Iter 0740 | Time 0.3901(0.3971) | Bit/dim 4.0852(4.7212) | Steps 278(276.76) | Grad Norm 5.7225(7.6304) | Total Time 5.00(5.00)
Iter 0750 | Time 0.3886(0.3971) | Bit/dim 3.9089(4.5277) | Steps 278(277.42) | Grad Norm 5.1848(7.0510) | Total Time 5.00(5.00)
Iter 0760 | Time 0.3999(0.3961) | Bit/dim 3.8126(4.3569) | Steps 284(278.39) | Grad Norm 4.7199(6.4891) | Total Time 5.00(5.00)
Iter 0770 | Time 0.3955(0.3947) | Bit/dim 3.7054(4.2012) | Steps 284(279.86) | Grad Norm 4.3123(5.9616) | Total Time 5.00(5.00)
Iter 0780 | Time 0.4082(0.3945) | Bit/dim 3.6375(4.0568) | Steps 278(280.46) | Grad Norm 3.8972(5.4605) | Total Time 5.00(5.00)
Iter 0790 | Time 0.3990(0.3965) | Bit/dim 3.5568(3.9240) | Steps 284(280.90) | Grad Norm 3.4728(4.9868) | Total Time 5.00(5.00)
Iter 0800 | Time 0.4097(0.3987) | Bit/dim 3.4560(3.8094) | Steps 278(280.56) | Grad Norm 3.1744(4.5465) | Total Time 5.00(5.00)
Iter 0810 | Time 0.4147(0.4032) | Bit/dim 3.3944(3.7093) | Steps 278(280.06) | Grad Norm 2.9051(4.1442) | Total Time 5.00(5.00)
Iter 0820 | Time 0.3947(0.4055) | Bit/dim 3.3196(3.6188) | Steps 278(279.52) | Grad Norm 2.6445(3.7780) | Total Time 5.00(5.00)
Iter 0830 | Time 0.4140(0.4033) | Bit/dim 3.2430(3.5308) | Steps 278(279.12) | Grad Norm 2.4112(3.4478) | Total Time 5.00(5.00)
Iter 0840 | Time 0.4058(0.4031) | Bit/dim 3.2591(3.4619) | Steps 278(278.83) | Grad Norm 2.2603(3.1574) | Total Time 5.00(5.00)
Iter 0850 | Time 0.4162(0.4072) | Bit/dim 3.2078(3.3920) | Steps 278(278.61) | Grad Norm 2.1185(2.9007) | Total Time 5.00(5.00)
Iter 0860 | Time 0.3870(0.4065) | Bit/dim 3.1489(3.3299) | Steps 278(278.45) | Grad Norm 2.0207(2.6788) | Total Time 5.00(5.00)
Iter 0870 | Time 0.4007(0.4055) | Bit/dim 3.0546(3.2739) | Steps 278(278.33) | Grad Norm 1.8883(2.4872) | Total Time 5.00(5.00)
Iter 0880 | Time 0.3946(0.4042) | Bit/dim 3.0627(3.2244) | Steps 278(278.55) | Grad Norm 1.7993(2.3198) | Total Time 5.00(5.00)
Iter 0890 | Time 0.3942(0.4026) | Bit/dim 3.0390(3.1782) | Steps 278(279.49) | Grad Norm 1.7511(2.1786) | Total Time 5.00(5.00)
validating...
Epoch 0003 | Time 8.2547, Bit/dim 2.9994
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0900 | Time 0.4029(0.4028) | Bit/dim 3.0427(3.1359) | Steps 284(280.67) | Grad Norm 1.6924(2.0580) | Total Time 5.00(5.00)
Iter 0910 | Time 0.4183(0.4032) | Bit/dim 2.9911(3.0969) | Steps 284(281.55) | Grad Norm 1.6287(1.9506) | Total Time 5.00(5.00)
Iter 0920 | Time 0.4058(0.4044) | Bit/dim 2.9831(3.0614) | Steps 284(281.88) | Grad Norm 1.5938(1.8584) | Total Time 5.00(5.00)
Iter 0930 | Time 0.4042(0.4037) | Bit/dim 2.8697(3.0290) | Steps 284(281.85) | Grad Norm 1.5084(1.7779) | Total Time 5.00(5.00)
Iter 0940 | Time 0.4097(0.4035) | Bit/dim 2.9032(2.9983) | Steps 284(281.64) | Grad Norm 1.4844(1.7080) | Total Time 5.00(5.00)
Iter 0950 | Time 0.4058(0.4030) | Bit/dim 2.8929(2.9702) | Steps 284(281.33) | Grad Norm 1.4645(1.6461) | Total Time 5.00(5.00)
Iter 0960 | Time 0.3976(0.4019) | Bit/dim 2.8134(2.9430) | Steps 278(280.45) | Grad Norm 1.3964(1.5899) | Total Time 5.00(5.00)
Iter 0970 | Time 0.4259(0.4024) | Bit/dim 2.8287(2.9168) | Steps 284(280.47) | Grad Norm 1.3760(1.5379) | Total Time 5.00(5.00)
Iter 0980 | Time 0.4043(0.4057) | Bit/dim 2.8152(2.8963) | Steps 284(280.63) | Grad Norm 1.3386(1.4919) | Total Time 5.00(5.00)
Iter 0990 | Time 0.4182(0.4079) | Bit/dim 2.7993(2.8673) | Steps 284(281.06) | Grad Norm 1.3077(1.4475) | Total Time 5.00(5.00)
Iter 1000 | Time 0.3982(0.4103) | Bit/dim 2.8223(2.8478) | Steps 284(281.83) | Grad Norm 1.2999(1.4096) | Total Time 5.00(5.00)
Iter 1010 | Time 0.3989(0.4083) | Bit/dim 2.7605(2.8285) | Steps 284(282.40) | Grad Norm 1.2616(1.3738) | Total Time 5.00(5.00)
Iter 1020 | Time 0.3950(0.4055) | Bit/dim 2.7643(2.8115) | Steps 284(282.82) | Grad Norm 1.2391(1.3400) | Total Time 5.00(5.00)
Iter 1030 | Time 0.4028(0.4050) | Bit/dim 2.7340(2.7922) | Steps 284(283.13) | Grad Norm 1.1892(1.3063) | Total Time 5.00(5.00)
Iter 1040 | Time 0.3985(0.4055) | Bit/dim 2.7034(2.7759) | Steps 284(283.36) | Grad Norm 1.1687(1.2767) | Total Time 5.00(5.00)
Iter 1050 | Time 0.3958(0.4045) | Bit/dim 2.7167(2.7602) | Steps 284(283.53) | Grad Norm 1.1542(1.2496) | Total Time 5.00(5.00)
Iter 1060 | Time 0.4003(0.4037) | Bit/dim 2.6717(2.7437) | Steps 284(283.65) | Grad Norm 1.1323(1.2225) | Total Time 5.00(5.00)
Iter 1070 | Time 0.4363(0.4073) | Bit/dim 2.6966(2.7261) | Steps 284(283.74) | Grad Norm 1.1074(1.1957) | Total Time 5.00(5.00)
Iter 1080 | Time 0.4165(0.4109) | Bit/dim 2.6487(2.7134) | Steps 284(283.81) | Grad Norm 1.0897(1.1719) | Total Time 5.00(5.00)
Iter 1090 | Time 0.4137(0.4118) | Bit/dim 2.6556(2.7003) | Steps 284(283.86) | Grad Norm 1.0695(1.1479) | Total Time 5.00(5.00)
Iter 1100 | Time 0.4001(0.4096) | Bit/dim 2.6217(2.6880) | Steps 284(283.90) | Grad Norm 1.0572(1.1263) | Total Time 5.00(5.00)
Iter 1110 | Time 0.4278(0.4109) | Bit/dim 2.6356(2.6765) | Steps 290(284.10) | Grad Norm 1.0317(1.1048) | Total Time 5.00(5.00)
Iter 1120 | Time 0.4159(0.4141) | Bit/dim 2.6064(2.6626) | Steps 290(285.23) | Grad Norm 1.0210(1.0836) | Total Time 5.00(5.00)
Iter 1130 | Time 0.4144(0.4157) | Bit/dim 2.6206(2.6502) | Steps 290(286.34) | Grad Norm 1.0154(1.0658) | Total Time 5.00(5.00)
Iter 1140 | Time 0.4284(0.4201) | Bit/dim 2.5877(2.6392) | Steps 290(287.30) | Grad Norm 0.9805(1.0470) | Total Time 5.00(5.00)
Iter 1150 | Time 0.3981(0.4172) | Bit/dim 2.6130(2.6291) | Steps 290(287.84) | Grad Norm 0.9723(1.0296) | Total Time 5.00(5.00)
Iter 1160 | Time 0.4073(0.4164) | Bit/dim 2.5730(2.6178) | Steps 290(288.41) | Grad Norm 0.9632(1.0124) | Total Time 5.00(5.00)
Iter 1170 | Time 0.4431(0.4237) | Bit/dim 2.6253(2.6104) | Steps 290(288.83) | Grad Norm 0.9526(0.9964) | Total Time 5.00(5.00)
Iter 1180 | Time 0.4179(0.4236) | Bit/dim 2.5540(2.5965) | Steps 290(289.14) | Grad Norm 0.9225(0.9799) | Total Time 5.00(5.00)
Iter 1190 | Time 0.4086(0.4202) | Bit/dim 2.5254(2.5877) | Steps 290(289.36) | Grad Norm 0.9090(0.9645) | Total Time 5.00(5.00)
validating...
Epoch 0004 | Time 8.0555, Bit/dim 2.5443
===> Using batch size 200. Total 300 iterations/epoch.
Iter 1200 | Time 0.4189(0.4173) | Bit/dim 2.5774(2.5797) | Steps 290(289.53) | Grad Norm 0.9111(0.9497) | Total Time 5.00(5.00)
Iter 1210 | Time 0.4330(0.4176) | Bit/dim 2.5097(2.5699) | Steps 290(289.65) | Grad Norm 0.8883(0.9349) | Total Time 5.00(5.00)
Iter 1220 | Time 0.4041(0.4188) | Bit/dim 2.4979(2.5604) | Steps 290(289.90) | Grad Norm 0.8823(0.9214) | Total Time 5.00(5.00)
Iter 1230 | Time 0.4213(0.4181) | Bit/dim 2.5309(2.5524) | Steps 290(290.23) | Grad Norm 0.8596(0.9075) | Total Time 5.00(5.00)
Iter 1240 | Time 0.4201(0.4198) | Bit/dim 2.5513(2.5448) | Steps 290(290.34) | Grad Norm 0.8512(0.8934) | Total Time 5.00(5.00)
Iter 1250 | Time 0.4182(0.4217) | Bit/dim 2.5142(2.5358) | Steps 290(290.69) | Grad Norm 0.8312(0.8817) | Total Time 5.00(5.00)
Iter 1260 | Time 0.4210(0.4221) | Bit/dim 2.4761(2.5289) | Steps 290(291.55) | Grad Norm 0.8287(0.8691) | Total Time 5.00(5.00)
Iter 1270 | Time 0.4289(0.4274) | Bit/dim 2.4855(2.5215) | Steps 296(292.59) | Grad Norm 0.8096(0.8571) | Total Time 5.00(5.00)
Iter 1280 | Time 0.4163(0.4352) | Bit/dim 2.4752(2.5149) | Steps 290(293.32) | Grad Norm 0.8066(0.8461) | Total Time 5.00(5.00)
Iter 1290 | Time 0.4187(0.4356) | Bit/dim 2.4953(2.5073) | Steps 296(294.35) | Grad Norm 0.8058(0.8356) | Total Time 5.00(5.00)
Iter 1300 | Time 0.4350(0.4369) | Bit/dim 2.4473(2.4992) | Steps 296(295.26) | Grad Norm 0.7782(0.8239) | Total Time 5.00(5.00)
Iter 1310 | Time 0.4209(0.4355) | Bit/dim 2.4603(2.4939) | Steps 296(296.24) | Grad Norm 0.7851(0.8146) | Total Time 5.00(5.00)
Iter 1320 | Time 0.4335(0.4361) | Bit/dim 2.4328(2.4888) | Steps 296(297.41) | Grad Norm 0.7696(0.8050) | Total Time 5.00(5.00)
Iter 1330 | Time 0.4620(0.4368) | Bit/dim 2.4712(2.4823) | Steps 302(297.82) | Grad Norm 0.7671(0.7956) | Total Time 5.00(5.00)
Iter 1340 | Time 0.4316(0.4370) | Bit/dim 2.4452(2.4726) | Steps 302(298.62) | Grad Norm 0.7697(0.7856) | Total Time 5.00(5.00)
Iter 1350 | Time 0.4183(0.4369) | Bit/dim 2.4622(2.4695) | Steps 296(299.02) | Grad Norm 0.7426(0.7765) | Total Time 5.00(5.00)
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 4.2407(4.2407) | Bit/dim 27.7579(27.7579) | Steps 412(412.00) | Grad Norm 209.2864(209.2864) | Total Time 10.00(10.00)
Iter 0010 | Time 2.6574(3.8351) | Bit/dim 25.4932(27.5058) | Steps 400(409.16) | Grad Norm 187.6944(207.1702) | Total Time 10.00(10.00)
Iter 0020 | Time 2.6496(3.5402) | Bit/dim 20.4534(26.2467) | Steps 400(407.22) | Grad Norm 143.0589(196.2723) | Total Time 10.00(10.00)
Iter 0030 | Time 2.7180(3.3086) | Bit/dim 15.7304(23.9536) | Steps 400(405.33) | Grad Norm 85.0221(173.8021) | Total Time 10.00(10.00)
Iter 0040 | Time 2.6452(3.1377) | Bit/dim 12.3197(21.2752) | Steps 400(403.93) | Grad Norm 46.0751(143.9958) | Total Time 10.00(10.00)
Iter 0050 | Time 2.6447(3.0086) | Bit/dim 10.0535(18.5452) | Steps 400(402.90) | Grad Norm 22.5270(114.4683) | Total Time 10.00(10.00)
Iter 0060 | Time 2.6530(2.9138) | Bit/dim 8.6057(16.0377) | Steps 400(402.14) | Grad Norm 14.3024(88.7889) | Total Time 10.00(10.00)
Iter 0070 | Time 2.6639(2.8471) | Bit/dim 7.2908(13.8506) | Steps 400(401.58) | Grad Norm 14.2045(69.3176) | Total Time 10.00(10.00)
Iter 0080 | Time 2.6751(2.8048) | Bit/dim 5.7327(11.8896) | Steps 400(401.34) | Grad Norm 11.3147(54.4434) | Total Time 10.00(10.00)
Iter 0090 | Time 2.6826(2.7787) | Bit/dim 4.7822(10.1516) | Steps 400(401.26) | Grad Norm 9.7829(42.8806) | Total Time 10.00(10.00)
Iter 0100 | Time 2.6818(2.7527) | Bit/dim 3.9653(8.6030) | Steps 400(400.93) | Grad Norm 8.5325(34.0090) | Total Time 10.00(10.00)
Iter 0110 | Time 2.8401(2.7385) | Bit/dim 3.2152(7.2620) | Steps 406(400.87) | Grad Norm 6.1297(26.9690) | Total Time 10.00(10.00)
Iter 0120 | Time 3.1016(2.8129) | Bit/dim 2.8600(6.1432) | Steps 424(406.21) | Grad Norm 4.4181(21.2226) | Total Time 10.00(10.00)
Iter 0130 | Time 3.1035(2.8886) | Bit/dim 2.6088(5.2377) | Steps 424(410.88) | Grad Norm 3.1277(16.6108) | Total Time 10.00(10.00)
Iter 0140 | Time 3.1023(2.9453) | Bit/dim 2.4395(4.5177) | Steps 424(414.33) | Grad Norm 2.0430(12.8981) | Total Time 10.00(10.00)
Iter 0150 | Time 3.1043(2.9879) | Bit/dim 2.3484(3.9571) | Steps 424(416.87) | Grad Norm 1.4666(9.9580) | Total Time 10.00(10.00)
Iter 0160 | Time 3.1033(3.0186) | Bit/dim 2.3074(3.5250) | Steps 424(418.74) | Grad Norm 1.0721(7.6662) | Total Time 10.00(10.00)
Iter 0170 | Time 3.1056(3.0412) | Bit/dim 2.2287(3.1928) | Steps 424(420.12) | Grad Norm 0.8149(5.8967) | Total Time 10.00(10.00)
Iter 0180 | Time 3.0954(3.0559) | Bit/dim 2.1726(2.9415) | Steps 424(421.14) | Grad Norm 0.6639(4.5380) | Total Time 10.00(10.00)
Iter 0190 | Time 3.0978(3.0671) | Bit/dim 2.2147(2.7505) | Steps 424(421.89) | Grad Norm 0.6356(3.5224) | Total Time 10.00(10.00)
Iter 0200 | Time 3.1390(3.0802) | Bit/dim 2.1610(2.6052) | Steps 424(422.44) | Grad Norm 0.5320(2.7487) | Total Time 10.00(10.00)
Iter 0210 | Time 3.1029(3.1134) | Bit/dim 2.1977(2.4954) | Steps 424(422.85) | Grad Norm 0.5026(2.1685) | Total Time 10.00(10.00)
Iter 0220 | Time 3.1114(3.1193) | Bit/dim 2.1560(2.4076) | Steps 424(423.15) | Grad Norm 0.4961(1.7342) | Total Time 10.00(10.00)
Iter 0230 | Time 3.1015(3.1149) | Bit/dim 2.1562(2.3428) | Steps 424(423.38) | Grad Norm 0.4459(1.4106) | Total Time 10.00(10.00)
Iter 0240 | Time 3.1128(3.1150) | Bit/dim 2.1559(2.2904) | Steps 424(423.54) | Grad Norm 0.4340(1.1746) | Total Time 10.00(10.00)
Iter 0250 | Time 3.1210(3.1162) | Bit/dim 2.1200(2.2470) | Steps 424(423.66) | Grad Norm 0.6240(1.0098) | Total Time 10.00(10.00)
Iter 0260 | Time 3.0994(3.1134) | Bit/dim 2.0918(2.2139) | Steps 424(423.75) | Grad Norm 0.6121(0.8987) | Total Time 10.00(10.00)
Iter 0270 | Time 3.1094(3.1134) | Bit/dim 2.1006(2.1873) | Steps 424(423.82) | Grad Norm 0.4883(0.8215) | Total Time 10.00(10.00)
Iter 0280 | Time 3.1167(3.1133) | Bit/dim 2.0858(2.1646) | Steps 424(423.86) | Grad Norm 0.7485(0.7360) | Total Time 10.00(10.00)
Iter 0290 | Time 3.1174(3.1139) | Bit/dim 2.0849(2.1451) | Steps 424(423.90) | Grad Norm 0.6153(0.6781) | Total Time 10.00(10.00)
validating...
Epoch 0001 | Time 37.4003, Bit/dim 2.0725
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0300 | Time 3.1275(3.1147) | Bit/dim 2.0937(2.1332) | Steps 424(423.93) | Grad Norm 0.6198(0.6532) | Total Time 10.00(10.00)
Iter 0310 | Time 3.1193(3.1166) | Bit/dim 2.0574(2.1138) | Steps 424(423.95) | Grad Norm 0.3787(0.6483) | Total Time 10.00(10.00)
Iter 0320 | Time 3.1239(3.1182) | Bit/dim 2.0758(2.1013) | Steps 424(423.96) | Grad Norm 0.5192(0.6300) | Total Time 10.00(10.00)
Iter 0330 | Time 3.1276(3.1213) | Bit/dim 2.0552(2.0918) | Steps 424(423.97) | Grad Norm 0.4415(0.5818) | Total Time 10.00(10.00)
Iter 0340 | Time 3.1607(3.1300) | Bit/dim 2.0577(2.0753) | Steps 430(425.13) | Grad Norm 0.5661(0.5576) | Total Time 10.00(10.00)
Iter 0350 | Time 3.1682(3.1390) | Bit/dim 2.0051(2.0614) | Steps 430(426.41) | Grad Norm 0.5697(0.5622) | Total Time 10.00(10.00)
Iter 0360 | Time 3.1698(3.1469) | Bit/dim 2.0109(2.0473) | Steps 430(427.35) | Grad Norm 0.5649(0.5527) | Total Time 10.00(10.00)
Iter 0370 | Time 3.1703(3.1526) | Bit/dim 2.0034(2.0388) | Steps 430(428.05) | Grad Norm 0.4800(0.5711) | Total Time 10.00(10.00)
Iter 0380 | Time 3.1265(3.1469) | Bit/dim 2.0155(2.0262) | Steps 424(427.26) | Grad Norm 0.8953(0.5770) | Total Time 10.00(10.00)
Iter 0390 | Time 3.1234(3.1415) | Bit/dim 1.9721(2.0128) | Steps 424(426.41) | Grad Norm 0.5637(0.5741) | Total Time 10.00(10.00)
Iter 0400 | Time 3.1720(3.1463) | Bit/dim 1.9890(2.0065) | Steps 430(426.93) | Grad Norm 0.6724(0.5601) | Total Time 10.00(10.00)
Iter 0410 | Time 3.2185(3.1606) | Bit/dim 1.9734(1.9944) | Steps 436(428.74) | Grad Norm 1.1147(0.6016) | Total Time 10.00(10.00)
Iter 0420 | Time 3.3734(3.2100) | Bit/dim 1.9474(1.9828) | Steps 442(431.94) | Grad Norm 1.2319(0.7188) | Total Time 10.00(10.00)
Iter 0430 | Time 3.3900(3.2536) | Bit/dim 1.9527(1.9703) | Steps 442(434.58) | Grad Norm 0.6885(0.7068) | Total Time 10.00(10.00)
Iter 0440 | Time 3.5854(3.3286) | Bit/dim 1.9073(1.9566) | Steps 454(438.49) | Grad Norm 0.7269(0.6804) | Total Time 10.00(10.00)
Iter 0450 | Time 3.5908(3.3964) | Bit/dim 1.8974(1.9462) | Steps 454(442.56) | Grad Norm 0.9640(0.6868) | Total Time 10.00(10.00)
Iter 0460 | Time 3.5766(3.4453) | Bit/dim 1.8854(1.9392) | Steps 454(445.57) | Grad Norm 0.6534(0.6719) | Total Time 10.00(10.00)
Iter 0470 | Time 3.7885(3.4967) | Bit/dim 1.9005(1.9271) | Steps 466(448.82) | Grad Norm 0.4977(0.6639) | Total Time 10.00(10.00)
Iter 0480 | Time 3.9496(3.6012) | Bit/dim 1.8859(1.9184) | Steps 472(454.35) | Grad Norm 0.4132(0.6463) | Total Time 10.00(10.00)
Iter 0490 | Time 3.9402(3.6926) | Bit/dim 1.8693(1.9089) | Steps 472(458.98) | Grad Norm 0.6158(0.6400) | Total Time 10.00(10.00)
Iter 0500 | Time 3.9402(3.7588) | Bit/dim 1.8995(1.8969) | Steps 472(462.40) | Grad Norm 0.9290(0.6897) | Total Time 10.00(10.00)
Iter 0510 | Time 3.9392(3.8067) | Bit/dim 1.8473(1.8856) | Steps 472(464.92) | Grad Norm 0.7027(0.6954) | Total Time 10.00(10.00)
Iter 0520 | Time 3.9833(3.8511) | Bit/dim 1.8302(1.8762) | Steps 478(468.08) | Grad Norm 0.8732(0.7193) | Total Time 10.00(10.00)
Iter 0530 | Time 3.8238(3.8814) | Bit/dim 1.8473(1.8698) | Steps 472(470.50) | Grad Norm 0.6096(0.7499) | Total Time 10.00(10.00)
Iter 0540 | Time 3.8321(3.8691) | Bit/dim 1.8415(1.8613) | Steps 472(470.90) | Grad Norm 0.8700(0.7928) | Total Time 10.00(10.00)
Iter 0550 | Time 3.8284(3.8592) | Bit/dim 1.8612(1.8563) | Steps 472(471.19) | Grad Norm 0.8646(0.7835) | Total Time 10.00(10.00)
Iter 0560 | Time 3.8305(3.8522) | Bit/dim 1.8394(1.8477) | Steps 472(471.40) | Grad Norm 0.7828(0.7990) | Total Time 10.00(10.00)
Iter 0570 | Time 3.8339(3.8644) | Bit/dim 1.7889(1.8399) | Steps 472(472.20) | Grad Norm 0.6203(0.8568) | Total Time 10.00(10.00)
Iter 0580 | Time 3.9904(3.8987) | Bit/dim 1.7747(1.8300) | Steps 478(473.72) | Grad Norm 0.7303(0.9359) | Total Time 10.00(10.00)
Iter 0590 | Time 3.9993(3.9242) | Bit/dim 1.7920(1.8206) | Steps 478(474.84) | Grad Norm 0.4743(0.9180) | Total Time 10.00(10.00)
validating...
Epoch 0002 | Time 48.3739, Bit/dim 1.7883
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0600 | Time 3.8296(3.9209) | Bit/dim 1.7450(1.8127) | Steps 472(474.80) | Grad Norm 0.4755(0.9404) | Total Time 10.00(10.00)
Iter 0610 | Time 3.8308(3.8820) | Bit/dim 1.7031(1.8034) | Steps 472(473.49) | Grad Norm 2.2260(1.1198) | Total Time 10.00(10.00)
Iter 0620 | Time 3.8382(3.8688) | Bit/dim 1.7498(1.7981) | Steps 472(473.10) | Grad Norm 2.6844(1.2684) | Total Time 10.00(10.00)
Iter 0630 | Time 3.8377(3.8612) | Bit/dim 1.8294(1.7953) | Steps 472(472.81) | Grad Norm 2.0778(1.3828) | Total Time 10.00(10.00)
Iter 0640 | Time 3.7875(3.8469) | Bit/dim 1.7911(1.7852) | Steps 466(471.59) | Grad Norm 1.2856(1.3909) | Total Time 10.00(10.00)
Iter 0650 | Time 3.7920(3.8234) | Bit/dim 1.7415(1.7756) | Steps 466(469.80) | Grad Norm 2.1822(1.8613) | Total Time 10.00(10.00)
Iter 0660 | Time 3.6736(3.7957) | Bit/dim 1.7715(1.7670) | Steps 466(469.13) | Grad Norm 7.3414(2.1901) | Total Time 10.00(10.00)
Iter 0670 | Time 3.8290(3.7808) | Bit/dim 1.7545(1.7628) | Steps 472(468.94) | Grad Norm 6.4378(3.6745) | Total Time 10.00(10.00)
Iter 0680 | Time 3.6713(3.7533) | Bit/dim 1.7189(1.7555) | Steps 466(468.34) | Grad Norm 2.9481(3.4184) | Total Time 10.00(10.00)
Iter 0690 | Time 3.7152(3.7408) | Bit/dim 1.7226(1.7492) | Steps 472(468.98) | Grad Norm 8.6825(3.6638) | Total Time 10.00(10.00)
Iter 0700 | Time 3.7168(3.7394) | Bit/dim 1.6895(1.7422) | Steps 472(470.51) | Grad Norm 6.6207(4.4484) | Total Time 10.00(10.00)
Iter 0710 | Time 3.8504(3.7415) | Bit/dim 1.6918(1.7306) | Steps 490(472.01) | Grad Norm 7.1928(4.5753) | Total Time 10.00(10.00)
Iter 0720 | Time 3.7103(3.7496) | Bit/dim 1.6937(1.7257) | Steps 472(473.90) | Grad Norm 8.5719(6.6952) | Total Time 10.00(10.00)
Iter 0730 | Time 3.7641(3.7501) | Bit/dim 1.6643(1.7149) | Steps 478(473.21) | Grad Norm 7.8569(6.7035) | Total Time 10.00(10.00)
Iter 0740 | Time 3.8619(3.7548) | Bit/dim 1.6636(1.7008) | Steps 478(472.16) | Grad Norm 6.7124(6.3797) | Total Time 10.00(10.00)
Iter 0750 | Time 3.7679(3.7646) | Bit/dim 1.6635(1.6896) | Steps 472(473.45) | Grad Norm 6.5433(6.8990) | Total Time 10.00(10.00)
Iter 0760 | Time 3.8090(3.7881) | Bit/dim 1.7034(1.6783) | Steps 484(474.36) | Grad Norm 20.0164(7.0131) | Total Time 10.00(10.00)
Iter 0770 | Time 3.8423(3.7914) | Bit/dim 1.6055(1.6653) | Steps 490(476.10) | Grad Norm 11.0013(8.5345) | Total Time 10.00(10.00)
Iter 0780 | Time 3.7540(3.7811) | Bit/dim 1.5626(1.6464) | Steps 478(476.43) | Grad Norm 1.2470(7.7728) | Total Time 10.00(10.00)
Iter 0790 | Time 3.7651(3.7765) | Bit/dim 1.5675(1.6220) | Steps 478(477.13) | Grad Norm 4.5835(6.9022) | Total Time 10.00(10.00)
Iter 0800 | Time 3.7628(3.8050) | Bit/dim 1.7036(1.6137) | Steps 478(479.98) | Grad Norm 40.8445(9.8998) | Total Time 10.00(10.00)
Iter 0810 | Time 3.8736(3.8376) | Bit/dim 1.6545(1.6234) | Steps 478(482.20) | Grad Norm 16.9477(12.4457) | Total Time 10.00(10.00)
Iter 0820 | Time 3.8006(3.8167) | Bit/dim 1.4997(1.6042) | Steps 484(481.12) | Grad Norm 2.8841(11.0944) | Total Time 10.00(10.00)
Iter 0830 | Time 3.6346(3.7950) | Bit/dim 1.5076(1.5808) | Steps 478(480.45) | Grad Norm 6.3974(9.5461) | Total Time 10.00(10.00)
Iter 0840 | Time 3.8006(3.7890) | Bit/dim 1.4727(1.5583) | Steps 484(481.26) | Grad Norm 1.4089(7.8716) | Total Time 10.00(10.00)
Iter 0850 | Time 3.9611(3.8285) | Bit/dim 1.4924(1.5384) | Steps 490(484.03) | Grad Norm 1.8751(6.6236) | Total Time 10.00(10.00)
Iter 0860 | Time 4.0480(3.8797) | Bit/dim 1.4762(1.5197) | Steps 502(487.85) | Grad Norm 2.5549(5.6412) | Total Time 10.00(10.00)
Iter 0870 | Time 4.0484(3.9240) | Bit/dim 1.4960(1.5044) | Steps 502(491.56) | Grad Norm 1.4325(4.6008) | Total Time 10.00(10.00)
Iter 0880 | Time 4.0482(3.9566) | Bit/dim 1.4106(1.4841) | Steps 502(494.14) | Grad Norm 2.7713(4.0829) | Total Time 10.00(10.00)
Iter 0890 | Time 4.0547(3.9913) | Bit/dim 1.4403(1.4717) | Steps 502(497.48) | Grad Norm 3.4519(4.4769) | Total Time 10.00(10.00)
validating...
Epoch 0003 | Time 59.5612, Bit/dim 1.4253
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0900 | Time 4.0541(4.0212) | Bit/dim 1.4409(1.4650) | Steps 502(499.79) | Grad Norm 8.6159(5.8881) | Total Time 10.00(10.00)
Iter 0910 | Time 3.9587(4.0472) | Bit/dim 1.4225(1.4546) | Steps 490(501.71) | Grad Norm 13.1709(6.8603) | Total Time 10.00(10.00)
Iter 0920 | Time 4.0514(4.0678) | Bit/dim 1.4250(1.4457) | Steps 502(503.35) | Grad Norm 8.9487(7.2929) | Total Time 10.00(10.00)
Iter 0930 | Time 4.3010(4.0879) | Bit/dim 1.4211(1.4370) | Steps 520(505.06) | Grad Norm 10.8779(7.5233) | Total Time 10.00(10.00)
Iter 0940 | Time 4.4482(4.1014) | Bit/dim 1.4610(1.4354) | Steps 526(505.90) | Grad Norm 22.1320(8.6113) | Total Time 10.00(10.00)
Iter 0950 | Time 3.9503(4.1065) | Bit/dim 1.4226(1.4300) | Steps 490(506.11) | Grad Norm 10.3534(9.0943) | Total Time 10.00(10.00)
Iter 0960 | Time 3.9897(4.1116) | Bit/dim 1.3938(1.4215) | Steps 496(506.88) | Grad Norm 5.8025(8.4768) | Total Time 10.00(10.00)
Iter 0970 | Time 4.0592(4.1180) | Bit/dim 1.3826(1.4129) | Steps 502(507.77) | Grad Norm 11.5902(8.1718) | Total Time 10.00(10.00)
Iter 0980 | Time 4.1430(4.1394) | Bit/dim 1.3960(1.4089) | Steps 514(508.58) | Grad Norm 3.2320(8.8909) | Total Time 10.00(10.00)
Iter 0990 | Time 4.0462(4.1426) | Bit/dim 1.4228(1.4071) | Steps 502(508.54) | Grad Norm 8.4791(9.3871) | Total Time 10.00(10.00)
Iter 1000 | Time 4.0538(4.1394) | Bit/dim 1.3453(1.3975) | Steps 502(508.70) | Grad Norm 5.7223(8.5076) | Total Time 10.00(10.00)
Iter 1010 | Time 4.3090(4.1478) | Bit/dim 1.3719(1.3982) | Steps 520(508.47) | Grad Norm 10.4920(9.2891) | Total Time 10.00(10.00)
Iter 1020 | Time 4.0791(4.1646) | Bit/dim 1.4706(1.4018) | Steps 490(508.94) | Grad Norm 21.4015(10.6796) | Total Time 10.00(10.00)
Iter 1030 | Time 4.0001(4.1913) | Bit/dim 1.4005(1.4034) | Steps 496(509.26) | Grad Norm 13.9317(11.2463) | Total Time 10.00(10.00)
Iter 1040 | Time 4.1301(4.1913) | Bit/dim 1.3645(1.3983) | Steps 514(509.82) | Grad Norm 4.1969(10.4676) | Total Time 10.00(10.00)
Iter 1050 | Time 4.2510(4.1839) | Bit/dim 1.3770(1.3891) | Steps 514(509.97) | Grad Norm 7.4268(9.6074) | Total Time 10.00(10.00)
Iter 1060 | Time 4.0041(4.1672) | Bit/dim 1.3568(1.3819) | Steps 496(508.78) | Grad Norm 2.9961(9.0402) | Total Time 10.00(10.00)
Iter 1070 | Time 4.2250(4.1709) | Bit/dim 1.3618(1.3732) | Steps 496(508.33) | Grad Norm 9.6424(8.4654) | Total Time 10.00(10.00)
Iter 1080 | Time 4.0855(4.1901) | Bit/dim 1.3337(1.3670) | Steps 496(508.21) | Grad Norm 3.5217(7.8095) | Total Time 10.00(10.00)
Iter 1090 | Time 4.0065(4.2026) | Bit/dim 1.3083(1.3578) | Steps 496(508.06) | Grad Norm 4.2026(7.4479) | Total Time 10.00(10.00)
Iter 1100 | Time 4.6471(4.2331) | Bit/dim 1.3593(1.3585) | Steps 526(508.75) | Grad Norm 16.2573(8.5243) | Total Time 10.00(10.00)
Iter 1110 | Time 4.2918(4.2282) | Bit/dim 1.3170(1.3558) | Steps 520(508.99) | Grad Norm 3.4846(8.8383) | Total Time 10.00(10.00)
Iter 1120 | Time 4.6017(4.2302) | Bit/dim 1.3313(1.3544) | Steps 538(509.91) | Grad Norm 20.7158(9.3538) | Total Time 10.00(10.00)
Iter 1130 | Time 4.2483(4.2599) | Bit/dim 1.3543(1.3642) | Steps 514(512.05) | Grad Norm 3.9617(10.9985) | Total Time 10.00(10.00)
Iter 1140 | Time 4.4757(4.2947) | Bit/dim 1.3854(1.3658) | Steps 526(513.11) | Grad Norm 12.0738(11.3456) | Total Time 10.00(10.00)
Iter 1150 | Time 4.5691(4.2849) | Bit/dim 1.3163(1.3602) | Steps 526(512.31) | Grad Norm 4.4831(10.3064) | Total Time 10.00(10.00)
Iter 1160 | Time 3.9930(4.2563) | Bit/dim 1.2968(1.3485) | Steps 496(510.58) | Grad Norm 8.7785(9.0375) | Total Time 10.00(10.00)
Iter 1170 | Time 4.3170(4.2433) | Bit/dim 1.3118(1.3420) | Steps 514(510.24) | Grad Norm 0.7811(7.8143) | Total Time 10.00(10.00)
Iter 1180 | Time 4.0085(4.2304) | Bit/dim 1.3330(1.3373) | Steps 496(509.65) | Grad Norm 4.9669(6.8425) | Total Time 10.00(10.00)
Iter 1190 | Time 4.4072(4.2272) | Bit/dim 1.3058(1.3330) | Steps 520(509.57) | Grad Norm 2.1014(6.9987) | Total Time 10.00(10.00)
validating...
Epoch 0004 | Time 66.5943, Bit/dim 1.3310
===> Using batch size 200. Total 300 iterations/epoch.
Iter 1200 | Time 4.5136(4.2578) | Bit/dim 1.3367(1.3314) | Steps 532(511.54) | Grad Norm 15.9605(7.3384) | Total Time 10.00(10.00)
Iter 1210 | Time 4.2576(4.2893) | Bit/dim 1.4140(1.3476) | Steps 514(513.24) | Grad Norm 15.6998(10.4855) | Total Time 10.00(10.00)
Iter 1220 | Time 4.2526(4.3022) | Bit/dim 1.3350(1.3553) | Steps 514(514.14) | Grad Norm 4.2255(10.9578) | Total Time 10.00(10.00)
Iter 1230 | Time 4.0088(4.2913) | Bit/dim 1.3330(1.3492) | Steps 496(513.47) | Grad Norm 6.6679(10.6509) | Total Time 10.00(10.00)
Iter 1240 | Time 4.0919(4.2687) | Bit/dim 1.3658(1.3407) | Steps 508(512.45) | Grad Norm 1.4710(9.4592) | Total Time 10.00(10.00)
Iter 1250 | Time 4.4138(4.2802) | Bit/dim 1.3209(1.3324) | Steps 520(512.71) | Grad Norm 10.8600(8.8901) | Total Time 10.00(10.00)
Iter 1260 | Time 4.1579(4.2729) | Bit/dim 1.3422(1.3283) | Steps 502(512.79) | Grad Norm 17.1375(9.7553) | Total Time 10.00(10.00)
Iter 1270 | Time 3.9962(4.2856) | Bit/dim 1.3156(1.3265) | Steps 496(513.61) | Grad Norm 10.4797(10.1755) | Total Time 10.00(10.00)
Iter 1280 | Time 4.0938(4.2616) | Bit/dim 1.2736(1.3189) | Steps 508(513.33) | Grad Norm 4.5858(9.9102) | Total Time 10.00(10.00)
Iter 1290 | Time 4.5580(4.2615) | Bit/dim 1.3509(1.3186) | Steps 538(513.88) | Grad Norm 20.1448(10.9077) | Total Time 10.00(10.00)
Iter 1300 | Time 4.4123(4.2636) | Bit/dim 1.2616(1.3110) | Steps 520(513.84) | Grad Norm 3.2259(9.9745) | Total Time 10.00(10.00)
Iter 1310 | Time 4.4193(4.2672) | Bit/dim 1.2948(1.3025) | Steps 520(514.05) | Grad Norm 2.1028(8.5158) | Total Time 10.00(10.00)
Iter 1320 | Time 4.4152(4.2895) | Bit/dim 1.3010(1.2976) | Steps 520(514.99) | Grad Norm 3.2990(6.9138) | Total Time 10.00(10.00)
Iter 1330 | Time 4.5704(4.3246) | Bit/dim 1.2909(1.2941) | Steps 526(516.33) | Grad Norm 0.9930(5.6874) | Total Time 10.00(10.00)
Iter 1340 | Time 4.4987(4.3440) | Bit/dim 1.2934(1.2918) | Steps 532(517.92) | Grad Norm 12.6098(5.9648) | Total Time 10.00(10.00)
Iter 1350 | Time 4.4149(4.3670) | Bit/dim 1.2914(1.2895) | Steps 520(519.28) | Grad Norm 9.1868(5.8839) | Total Time 10.00(10.00)
Iter 1360 | Time 4.4156(4.3759) | Bit/dim 1.3022(1.2949) | Steps 520(519.90) | Grad Norm 8.7268(8.5523) | Total Time 10.00(10.00)
Iter 1370 | Time 4.1703(4.4080) | Bit/dim 1.4127(1.3148) | Steps 502(521.43) | Grad Norm 25.0952(11.3534) | Total Time 10.00(10.00)
Iter 1380 | Time 4.2524(4.4086) | Bit/dim 1.2899(1.3120) | Steps 514(522.11) | Grad Norm 7.6890(10.6769) | Total Time 10.00(10.00)
Iter 1390 | Time 4.5973(4.4105) | Bit/dim 1.2746(1.3044) | Steps 514(522.20) | Grad Norm 6.6685(9.6175) | Total Time 10.00(10.00)
Iter 1400 | Time 4.6450(4.4312) | Bit/dim 1.2676(1.2993) | Steps 538(523.55) | Grad Norm 10.1002(9.1250) | Total Time 10.00(10.00)
Iter 1410 | Time 4.1063(4.4353) | Bit/dim 1.3276(1.2957) | Steps 496(523.85) | Grad Norm 17.2493(10.3556) | Total Time 10.00(10.00)
Iter 1420 | Time 4.5281(4.4599) | Bit/dim 1.2564(1.2952) | Steps 538(526.18) | Grad Norm 8.9089(10.9981) | Total Time 10.00(10.00)
Iter 1430 | Time 4.8953(4.4706) | Bit/dim 1.3325(1.2988) | Steps 556(527.70) | Grad Norm 25.2973(11.7866) | Total Time 10.00(10.00)
Iter 1440 | Time 4.3447(4.4370) | Bit/dim 1.2590(1.2979) | Steps 520(526.16) | Grad Norm 3.5714(11.6002) | Total Time 10.00(10.00)
Iter 1450 | Time 4.2725(4.4497) | Bit/dim 1.2613(1.2923) | Steps 520(526.43) | Grad Norm 2.6501(10.2168) | Total Time 10.00(10.00)
Iter 1460 | Time 4.5807(4.4787) | Bit/dim 1.2686(1.2819) | Steps 538(527.58) | Grad Norm 4.9316(8.8410) | Total Time 10.00(10.00)
Iter 1470 | Time 4.4338(4.5493) | Bit/dim 1.2735(1.2749) | Steps 526(528.12) | Grad Norm 6.2733(7.4011) | Total Time 10.00(10.00)
Iter 1480 | Time 4.5271(4.5279) | Bit/dim 1.2384(1.2701) | Steps 538(528.99) | Grad Norm 2.7797(6.5367) | Total Time 10.00(10.00)
Iter 1490 | Time 4.5093(4.5410) | Bit/dim 1.2312(1.2658) | Steps 532(528.90) | Grad Norm 6.0538(6.2733) | Total Time 10.00(10.00)
validating...
Epoch 0005 | Time 61.4282, Bit/dim 1.2470
===> Using batch size 200. Total 300 iterations/epoch.
Iter 1500 | Time 4.5816(4.5250) | Bit/dim 1.2541(1.2638) | Steps 526(528.14) | Grad Norm 5.8451(6.4063) | Total Time 10.00(10.00)
Iter 1510 | Time 4.2345(4.5019) | Bit/dim 1.3243(1.2645) | Steps 496(526.84) | Grad Norm 21.3706(7.8711) | Total Time 10.00(10.00)
Iter 1520 | Time 4.5010(4.5149) | Bit/dim 1.3149(1.2774) | Steps 502(526.76) | Grad Norm 21.1505(10.6207) | Total Time 10.00(10.00)
Iter 1530 | Time 5.8693(4.6317) | Bit/dim 1.2710(1.2817) | Steps 568(528.68) | Grad Norm 21.5177(11.7198) | Total Time 10.00(10.00)
Iter 1540 | Time 4.8510(4.6373) | Bit/dim 1.2535(1.2786) | Steps 538(528.97) | Grad Norm 9.3088(11.5565) | Total Time 10.00(10.00)
Iter 1550 | Time 4.9151(4.6598) | Bit/dim 1.2490(1.2737) | Steps 538(529.65) | Grad Norm 7.9737(10.0465) | Total Time 10.00(10.00)
Iter 1560 | Time 4.5697(4.6423) | Bit/dim 1.2249(1.2662) | Steps 526(530.26) | Grad Norm 3.1737(8.3607) | Total Time 10.00(10.00)
Iter 1570 | Time 4.6562(4.6362) | Bit/dim 1.2205(1.2609) | Steps 538(531.86) | Grad Norm 3.3099(6.9631) | Total Time 10.00(10.00)
Iter 1580 | Time 4.6035(4.6488) | Bit/dim 1.2550(1.2559) | Steps 532(532.95) | Grad Norm 3.0498(5.9364) | Total Time 10.00(10.00)
Iter 1590 | Time 4.5285(4.6348) | Bit/dim 1.2280(1.2493) | Steps 532(532.41) | Grad Norm 6.8511(5.1802) | Total Time 10.00(10.00)
Iter 1600 | Time 4.4843(4.6283) | Bit/dim 1.2511(1.2488) | Steps 514(530.69) | Grad Norm 10.1225(5.5591) | Total Time 10.00(10.00)
Iter 1610 | Time 4.9074(4.6667) | Bit/dim 1.2417(1.2473) | Steps 544(530.60) | Grad Norm 10.5236(6.4601) | Total Time 10.00(10.00)
Iter 1620 | Time 4.8335(4.6839) | Bit/dim 1.2529(1.2467) | Steps 532(530.58) | Grad Norm 6.6622(7.7487) | Total Time 10.00(10.00)
Iter 1630 | Time 4.2089(4.6495) | Bit/dim 1.2376(1.2477) | Steps 502(528.69) | Grad Norm 14.9113(8.4895) | Total Time 10.00(10.00)
Iter 1640 | Time 4.0091(4.5969) | Bit/dim 1.2517(1.2449) | Steps 496(527.49) | Grad Norm 16.2616(8.8819) | Total Time 10.00(10.00)
Iter 1650 | Time 4.6863(4.5992) | Bit/dim 1.2716(1.2467) | Steps 526(527.70) | Grad Norm 9.9146(9.2085) | Total Time 10.00(10.00)
Iter 1660 | Time 3.9989(4.5641) | Bit/dim 1.2664(1.2465) | Steps 496(526.90) | Grad Norm 13.9947(8.9567) | Total Time 10.00(10.00)
Iter 1670 | Time 4.5477(4.5834) | Bit/dim 1.2517(1.2560) | Steps 526(529.28) | Grad Norm 4.5490(11.0554) | Total Time 10.00(10.00)
Iter 1680 | Time 4.6301(4.6013) | Bit/dim 1.2908(1.2688) | Steps 538(531.72) | Grad Norm 7.5322(11.8978) | Total Time 10.00(10.00)
Iter 1690 | Time 4.3047(4.5733) | Bit/dim 1.2456(1.2633) | Steps 526(531.15) | Grad Norm 4.6550(11.1481) | Total Time 10.00(10.00)
Iter 1700 | Time 4.3776(4.5581) | Bit/dim 1.2349(1.2578) | Steps 520(530.75) | Grad Norm 6.8282(9.9363) | Total Time 10.00(10.00)
Iter 1710 | Time 4.4827(4.5152) | Bit/dim 1.1917(1.2500) | Steps 532(529.76) | Grad Norm 2.1821(9.2153) | Total Time 10.00(10.00)
Iter 1720 | Time 4.7203(4.5143) | Bit/dim 1.2642(1.2530) | Steps 550(530.32) | Grad Norm 7.6715(10.7551) | Total Time 10.00(10.00)
Iter 1730 | Time 4.3847(4.5261) | Bit/dim 1.2812(1.2633) | Steps 520(531.01) | Grad Norm 13.0627(12.2882) | Total Time 10.00(10.00)
Iter 1740 | Time 4.6049(4.5041) | Bit/dim 1.2351(1.2602) | Steps 550(530.68) | Grad Norm 11.2457(12.3808) | Total Time 10.00(10.00)
Iter 1750 | Time 4.5438(4.4806) | Bit/dim 1.2269(1.2551) | Steps 526(529.41) | Grad Norm 2.7716(11.2856) | Total Time 10.00(10.00)
Iter 1760 | Time 4.4727(4.4993) | Bit/dim 1.2407(1.2472) | Steps 532(530.21) | Grad Norm 3.4300(9.7065) | Total Time 10.00(10.00)
Iter 1770 | Time 4.5212(4.5014) | Bit/dim 1.2031(1.2364) | Steps 538(530.69) | Grad Norm 3.9522(7.9535) | Total Time 10.00(10.00)
Iter 1780 | Time 4.5513(4.5022) | Bit/dim 1.2247(1.2332) | Steps 526(530.71) | Grad Norm 5.4877(7.1808) | Total Time 10.00(10.00)
Iter 1790 | Time 4.6330(4.5184) | Bit/dim 1.2039(1.2308) | Steps 538(531.84) | Grad Norm 2.2674(6.1285) | Total Time 10.00(10.00)
validating...
Epoch 0006 | Time 64.1024, Bit/dim 1.2099
===> Using batch size 200. Total 300 iterations/epoch.
Iter 1800 | Time 4.7979(4.5171) | Bit/dim 1.2369(1.2276) | Steps 544(531.97) | Grad Norm 4.0392(5.2338) | Total Time 10.00(10.00)
Iter 1810 | Time 4.7920(4.5332) | Bit/dim 1.2296(1.2277) | Steps 544(532.63) | Grad Norm 9.5836(5.0783) | Total Time 10.00(10.00)
Iter 1820 | Time 5.0864(4.5702) | Bit/dim 1.2148(1.2239) | Steps 568(534.19) | Grad Norm 22.9640(6.5473) | Total Time 10.00(10.00)
Iter 1830 | Time 4.7904(4.5981) | Bit/dim 1.3095(1.2515) | Steps 544(535.21) | Grad Norm 8.9900(9.9310) | Total Time 10.00(10.00)
Iter 1840 | Time 4.8752(4.5928) | Bit/dim 1.2692(1.2576) | Steps 556(535.85) | Grad Norm 22.6124(10.8852) | Total Time 10.00(10.00)
Iter 1850 | Time 4.6708(4.5701) | Bit/dim 1.2419(1.2558) | Steps 544(534.10) | Grad Norm 4.0255(11.4081) | Total Time 10.00(10.00)
Iter 1860 | Time 4.5423(4.5678) | Bit/dim 1.2071(1.2476) | Steps 526(533.00) | Grad Norm 4.7784(10.4452) | Total Time 10.00(10.00)
Iter 1870 | Time 4.4768(4.5613) | Bit/dim 1.2547(1.2427) | Steps 532(533.31) | Grad Norm 2.0834(9.2558) | Total Time 10.00(10.00)
Iter 1880 | Time 4.8017(4.5763) | Bit/dim 1.2045(1.2332) | Steps 544(534.25) | Grad Norm 8.9669(8.1117) | Total Time 10.00(10.00)
Iter 1890 | Time 4.2277(4.5730) | Bit/dim 1.2509(1.2316) | Steps 514(534.49) | Grad Norm 15.2029(8.6520) | Total Time 10.00(10.00)
Iter 1900 | Time 4.7092(4.5952) | Bit/dim 1.2188(1.2294) | Steps 532(534.76) | Grad Norm 9.3098(9.6402) | Total Time 10.00(10.00)
Iter 1910 | Time 4.4775(4.5773) | Bit/dim 1.1948(1.2244) | Steps 532(533.97) | Grad Norm 5.0344(8.2354) | Total Time 10.00(10.00)
Iter 1920 | Time 4.7980(4.5716) | Bit/dim 1.2324(1.2196) | Steps 544(533.78) | Grad Norm 7.7971(7.3541) | Total Time 10.00(10.00)
Iter 1930 | Time 4.4731(4.5697) | Bit/dim 1.2229(1.2165) | Steps 532(534.20) | Grad Norm 1.8124(6.2993) | Total Time 10.00(10.00)
Iter 1940 | Time 4.4759(4.5617) | Bit/dim 1.2172(1.2130) | Steps 532(534.26) | Grad Norm 2.2105(5.3725) | Total Time 10.00(10.00)
Iter 1950 | Time 4.6320(4.5521) | Bit/dim 1.2227(1.2087) | Steps 538(534.17) | Grad Norm 2.9996(4.5298) | Total Time 10.00(10.00)
Iter 1960 | Time 4.7926(4.5809) | Bit/dim 1.2018(1.2080) | Steps 544(535.21) | Grad Norm 11.2085(5.1452) | Total Time 10.00(10.00)
Iter 1970 | Time 5.0428(4.6083) | Bit/dim 1.2110(1.2084) | Steps 562(536.43) | Grad Norm 13.8964(6.3809) | Total Time 10.00(10.00)
Iter 1980 | Time 4.5401(4.6325) | Bit/dim 1.2676(1.2144) | Steps 526(537.26) | Grad Norm 16.9498(7.9230) | Total Time 10.00(10.00)
Iter 1990 | Time 4.5522(4.6505) | Bit/dim 1.2171(1.2151) | Steps 526(538.15) | Grad Norm 5.6896(8.6754) | Total Time 10.00(10.00)
Iter 2000 | Time 4.8021(4.6348) | Bit/dim 1.2025(1.2117) | Steps 544(537.41) | Grad Norm 6.5933(7.6940) | Total Time 10.00(10.00)
Iter 2010 | Time 4.4868(4.6479) | Bit/dim 1.2233(1.2126) | Steps 532(537.54) | Grad Norm 1.0889(7.3325) | Total Time 10.00(10.00)
Iter 2020 | Time 4.6427(4.6582) | Bit/dim 1.2144(1.2122) | Steps 538(538.20) | Grad Norm 5.5379(7.9057) | Total Time 10.00(10.00)
Iter 2030 | Time 4.8212(4.7182) | Bit/dim 1.3296(1.2314) | Steps 532(540.19) | Grad Norm 16.8382(10.9066) | Total Time 10.00(10.00)
Iter 2040 | Time 5.0458(4.7533) | Bit/dim 1.2518(1.2475) | Steps 562(542.17) | Grad Norm 12.3160(12.2538) | Total Time 10.00(10.00)
Iter 2050 | Time 4.8205(4.7215) | Bit/dim 1.2417(1.2521) | Steps 532(540.41) | Grad Norm 10.1342(13.0606) | Total Time 10.00(10.00)
Iter 2060 | Time 4.8613(4.6916) | Bit/dim 1.2391(1.2538) | Steps 538(538.72) | Grad Norm 9.4164(12.7143) | Total Time 10.00(10.00)
Iter 2070 | Time 4.6295(4.6481) | Bit/dim 1.2584(1.2477) | Steps 538(536.80) | Grad Norm 12.8627(12.5543) | Total Time 10.00(10.00)
Iter 2080 | Time 4.4781(4.5830) | Bit/dim 1.2314(1.2406) | Steps 532(533.86) | Grad Norm 5.0611(11.7368) | Total Time 10.00(10.00)
Iter 2090 | Time 4.4793(4.5535) | Bit/dim 1.2033(1.2319) | Steps 532(532.72) | Grad Norm 2.5175(10.6383) | Total Time 10.00(10.00)
validating...
Epoch 0007 | Time 65.4056, Bit/dim 1.1925
===> Using batch size 200. Total 300 iterations/epoch.
Iter 2100 | Time 4.5353(4.5656) | Bit/dim 1.1819(1.2220) | Steps 538(533.14) | Grad Norm 3.9228(9.5080) | Total Time 10.00(10.00)
Iter 2110 | Time 4.4743(4.5497) | Bit/dim 1.2086(1.2150) | Steps 532(533.02) | Grad Norm 1.5815(7.7829) | Total Time 10.00(10.00)
Iter 2120 | Time 4.5176(4.5345) | Bit/dim 1.1934(1.2104) | Steps 538(533.07) | Grad Norm 2.9535(6.3458) | Total Time 10.00(10.00)
Iter 2130 | Time 4.4750(4.5404) | Bit/dim 1.1682(1.2036) | Steps 532(533.72) | Grad Norm 5.9195(5.6995) | Total Time 10.00(10.00)
Iter 2140 | Time 4.6470(4.5383) | Bit/dim 1.2460(1.2048) | Steps 538(533.73) | Grad Norm 2.4257(4.8171) | Total Time 10.00(10.00)
Iter 2150 | Time 4.4806(4.5288) | Bit/dim 1.1936(1.2020) | Steps 532(533.42) | Grad Norm 1.0381(4.1358) | Total Time 10.00(10.00)
Iter 2160 | Time 4.4786(4.5489) | Bit/dim 1.2076(1.1978) | Steps 532(534.28) | Grad Norm 2.3484(3.9984) | Total Time 10.00(10.00)
Iter 2170 | Time 4.7093(4.5864) | Bit/dim 1.1974(1.1965) | Steps 532(535.62) | Grad Norm 11.2302(4.9236) | Total Time 10.00(10.00)
Iter 2180 | Time 4.2989(4.6235) | Bit/dim 1.3821(1.2200) | Steps 508(537.05) | Grad Norm 15.5066(8.9500) | Total Time 10.00(10.00)
Iter 2190 | Time 4.4979(4.6338) | Bit/dim 1.2559(1.2348) | Steps 532(537.46) | Grad Norm 8.5777(9.7147) | Total Time 10.00(10.00)
Iter 2200 | Time 4.1371(4.6063) | Bit/dim 1.2088(1.2369) | Steps 502(535.00) | Grad Norm 13.1592(10.8944) | Total Time 10.00(10.00)
Iter 2210 | Time 4.5478(4.5994) | Bit/dim 1.1933(1.2287) | Steps 526(534.14) | Grad Norm 5.9032(10.0218) | Total Time 10.00(10.00)
Iter 2220 | Time 4.3986(4.5828) | Bit/dim 1.1998(1.2230) | Steps 520(533.52) | Grad Norm 4.9858(9.1679) | Total Time 10.00(10.00)
Iter 2230 | Time 4.6878(4.5868) | Bit/dim 1.2357(1.2164) | Steps 544(533.63) | Grad Norm 13.7294(9.0174) | Total Time 10.00(10.00)
Iter 2240 | Time 4.5616(4.6073) | Bit/dim 1.2178(1.2145) | Steps 526(534.41) | Grad Norm 10.1746(9.8382) | Total Time 10.00(10.00)
Iter 2250 | Time 4.7149(4.6174) | Bit/dim 1.1705(1.2110) | Steps 532(534.54) | Grad Norm 5.7801(9.5754) | Total Time 10.00(10.00)
Iter 2260 | Time 4.4948(4.5977) | Bit/dim 1.1565(1.2033) | Steps 532(534.31) | Grad Norm 1.0892(7.6649) | Total Time 10.00(10.00)
Iter 2270 | Time 4.4845(4.5681) | Bit/dim 1.1510(1.1956) | Steps 532(533.70) | Grad Norm 1.6408(6.0058) | Total Time 10.00(10.00)
Iter 2280 | Time 4.4741(4.5642) | Bit/dim 1.1808(1.1913) | Steps 532(534.24) | Grad Norm 2.8765(5.1174) | Total Time 10.00(10.00)
Iter 2290 | Time 4.7953(4.5941) | Bit/dim 1.1775(1.1889) | Steps 544(535.54) | Grad Norm 7.3574(5.2873) | Total Time 10.00(10.00)
Iter 2300 | Time 4.7107(4.6175) | Bit/dim 1.2163(1.1892) | Steps 532(536.14) | Grad Norm 9.7338(5.6514) | Total Time 10.00(10.00)
Iter 2310 | Time 4.8012(4.6350) | Bit/dim 1.1979(1.1937) | Steps 544(537.15) | Grad Norm 8.8850(6.8513) | Total Time 10.00(10.00)
Iter 2320 | Time 4.8000(4.6526) | Bit/dim 1.1949(1.1948) | Steps 544(537.71) | Grad Norm 7.4249(7.6323) | Total Time 10.00(10.00)
Iter 2330 | Time 4.8019(4.6602) | Bit/dim 1.1824(1.1946) | Steps 544(537.83) | Grad Norm 11.3959(7.2255) | Total Time 10.00(10.00)
Iter 2340 | Time 4.4936(4.6565) | Bit/dim 1.1947(1.1951) | Steps 532(537.55) | Grad Norm 2.6224(8.1728) | Total Time 10.00(10.00)
Iter 2350 | Time 4.8952(4.6897) | Bit/dim 1.2499(1.2209) | Steps 556(539.25) | Grad Norm 7.6346(10.2213) | Total Time 10.00(10.00)
Iter 2360 | Time 4.6392(4.6709) | Bit/dim 1.1874(1.2199) | Steps 538(538.69) | Grad Norm 3.8378(8.9249) | Total Time 10.00(10.00)
Iter 2370 | Time 4.6411(4.6173) | Bit/dim 1.1869(1.2105) | Steps 538(536.12) | Grad Norm 7.5227(7.8613) | Total Time 10.00(10.00)
Iter 2380 | Time 4.8097(4.6130) | Bit/dim 1.2007(1.2037) | Steps 544(536.01) | Grad Norm 9.5531(7.1232) | Total Time 10.00(10.00)
Iter 2390 | Time 4.3084(4.6174) | Bit/dim 1.2544(1.2023) | Steps 508(535.55) | Grad Norm 20.0283(8.1889) | Total Time 10.00(10.00)
validating...
Epoch 0008 | Time 69.4249, Bit/dim 1.2215
===> Using batch size 200. Total 300 iterations/epoch.
Iter 2400 | Time 5.0617(4.6405) | Bit/dim 1.2510(1.2223) | Steps 562(536.30) | Grad Norm 15.0056(10.7487) | Total Time 10.00(10.00)
Iter 2410 | Time 5.1069(4.6661) | Bit/dim 1.2368(1.2272) | Steps 568(537.35) | Grad Norm 11.4694(11.7122) | Total Time 10.00(10.00)
Iter 2420 | Time 4.3175(4.6094) | Bit/dim 1.1805(1.2254) | Steps 526(534.10) | Grad Norm 3.4554(11.5805) | Total Time 10.00(10.00)
Iter 2430 | Time 4.5449(4.6107) | Bit/dim 1.2067(1.2192) | Steps 526(533.94) | Grad Norm 7.2177(10.7439) | Total Time 10.00(10.00)
Iter 2440 | Time 4.4084(4.5982) | Bit/dim 1.1985(1.2109) | Steps 526(534.26) | Grad Norm 9.2908(10.0008) | Total Time 10.00(10.00)
Iter 2450 | Time 4.4672(4.5784) | Bit/dim 1.1614(1.2025) | Steps 532(533.74) | Grad Norm 1.4174(8.6017) | Total Time 10.00(10.00)
Iter 2460 | Time 5.0911(4.6008) | Bit/dim 1.2058(1.1964) | Steps 568(535.28) | Grad Norm 20.0661(8.6617) | Total Time 10.00(10.00)
Iter 2470 | Time 4.5450(4.6049) | Bit/dim 1.1947(1.1964) | Steps 526(534.87) | Grad Norm 5.2304(9.2827) | Total Time 10.00(10.00)
Iter 2480 | Time 4.5688(4.6210) | Bit/dim 1.1990(1.1971) | Steps 532(535.52) | Grad Norm 7.4758(9.3390) | Total Time 10.00(10.00)
Iter 2490 | Time 4.5529(4.6633) | Bit/dim 1.2423(1.2030) | Steps 526(537.33) | Grad Norm 13.1051(11.0632) | Total Time 10.00(10.00)
Iter 2500 | Time 4.3938(4.6421) | Bit/dim 1.1782(1.2014) | Steps 520(535.97) | Grad Norm 4.8487(10.4131) | Total Time 10.00(10.00)
Iter 2510 | Time 4.4939(4.6131) | Bit/dim 1.1759(1.1974) | Steps 538(535.63) | Grad Norm 3.2282(9.0080) | Total Time 10.00(10.00)
Iter 2520 | Time 4.4838(4.5834) | Bit/dim 1.1643(1.1925) | Steps 532(535.31) | Grad Norm 2.6563(7.4259) | Total Time 10.00(10.00)
Iter 2530 | Time 4.4944(4.5679) | Bit/dim 1.1572(1.1880) | Steps 538(536.23) | Grad Norm 2.2650(6.1626) | Total Time 10.00(10.00)
Iter 2540 | Time 4.5228(4.5563) | Bit/dim 1.1637(1.1816) | Steps 538(536.51) | Grad Norm 2.3160(5.2473) | Total Time 10.00(10.00)
Iter 2550 | Time 4.8350(4.5822) | Bit/dim 1.1517(1.1801) | Steps 550(538.09) | Grad Norm 7.0354(5.1584) | Total Time 10.00(10.00)
Iter 2560 | Time 4.7086(4.5949) | Bit/dim 1.1754(1.1796) | Steps 532(537.46) | Grad Norm 6.6079(5.1621) | Total Time 10.00(10.00)
Iter 2570 | Time 4.8560(4.6015) | Bit/dim 1.1762(1.1791) | Steps 556(538.46) | Grad Norm 8.5455(4.7551) | Total Time 10.00(10.00)
Iter 2580 | Time 4.9247(4.6244) | Bit/dim 1.2211(1.1824) | Steps 562(539.42) | Grad Norm 28.2043(6.6530) | Total Time 10.00(10.00)
Iter 2590 | Time 4.2118(4.6144) | Bit/dim 1.3418(1.2069) | Steps 502(537.77) | Grad Norm 14.3741(9.9603) | Total Time 10.00(10.00)
Iter 2600 | Time 4.7058(4.6203) | Bit/dim 1.2385(1.2194) | Steps 532(536.53) | Grad Norm 7.5184(10.7253) | Total Time 10.00(10.00)
Iter 2610 | Time 4.2954(4.6116) | Bit/dim 1.1698(1.2182) | Steps 508(535.03) | Grad Norm 7.1457(11.1576) | Total Time 10.00(10.00)
Iter 2620 | Time 4.5366(4.5953) | Bit/dim 1.1776(1.2091) | Steps 544(534.09) | Grad Norm 1.9633(9.9134) | Total Time 10.00(10.00)
Iter 2630 | Time 4.5577(4.5910) | Bit/dim 1.1595(1.2032) | Steps 532(534.02) | Grad Norm 6.3404(9.1654) | Total Time 10.00(10.00)
Iter 2640 | Time 4.5585(4.6019) | Bit/dim 1.1755(1.1970) | Steps 532(535.01) | Grad Norm 2.6528(8.5499) | Total Time 10.00(10.00)
Iter 2650 | Time 5.0994(4.6370) | Bit/dim 1.2070(1.1945) | Steps 574(537.81) | Grad Norm 23.8084(9.5397) | Total Time 10.00(10.00)
Iter 2660 | Time 4.9525(4.6466) | Bit/dim 1.2229(1.1965) | Steps 568(539.34) | Grad Norm 12.2564(9.2608) | Total Time 10.00(10.00)
Iter 2670 | Time 4.5540(4.6531) | Bit/dim 1.1989(1.1928) | Steps 532(540.26) | Grad Norm 10.9648(10.2949) | Total Time 10.00(10.00)
Iter 2680 | Time 4.5465(4.6498) | Bit/dim 1.1677(1.1890) | Steps 532(540.48) | Grad Norm 4.6598(9.2498) | Total Time 10.00(10.00)
Iter 2690 | Time 4.5305(4.6046) | Bit/dim 1.1978(1.1865) | Steps 544(539.34) | Grad Norm 2.2038(7.5073) | Total Time 10.00(10.00)
validating...
Epoch 0009 | Time 66.0345, Bit/dim 1.1607
===> Using batch size 200. Total 300 iterations/epoch.
Iter 2700 | Time 4.5367(4.5737) | Bit/dim 1.1638(1.1806) | Steps 544(538.50) | Grad Norm 2.6419(6.2824) | Total Time 10.00(10.00)
Iter 2710 | Time 4.4005(4.5576) | Bit/dim 1.1705(1.1772) | Steps 526(537.82) | Grad Norm 2.1759(5.3414) | Total Time 10.00(10.00)
Iter 2720 | Time 4.5571(4.5669) | Bit/dim 1.1683(1.1772) | Steps 532(538.36) | Grad Norm 5.2850(5.0289) | Total Time 10.00(10.00)
Iter 2730 | Time 4.3896(4.5605) | Bit/dim 1.1569(1.1739) | Steps 526(538.22) | Grad Norm 2.6372(4.4275) | Total Time 10.00(10.00)
Iter 2740 | Time 4.7172(4.5992) | Bit/dim 1.1930(1.1723) | Steps 538(539.26) | Grad Norm 10.6648(5.2249) | Total Time 10.00(10.00)
Iter 2750 | Time 4.5521(4.6275) | Bit/dim 1.1996(1.1703) | Steps 532(540.66) | Grad Norm 13.2289(6.2830) | Total Time 10.00(10.00)
Iter 2760 | Time 4.9371(4.6575) | Bit/dim 1.2716(1.1823) | Steps 568(541.82) | Grad Norm 9.9244(8.5576) | Total Time 10.00(10.00)
Iter 2770 | Time 5.3539(4.6647) | Bit/dim 1.3545(1.2073) | Steps 574(541.59) | Grad Norm 40.4856(11.3224) | Total Time 10.00(10.00)
Iter 2780 | Time 5.0809(4.6341) | Bit/dim 1.2434(1.2263) | Steps 568(538.79) | Grad Norm 12.8058(11.1799) | Total Time 10.00(10.00)
Iter 2790 | Time 4.6803(4.6311) | Bit/dim 1.2032(1.2288) | Steps 550(537.74) | Grad Norm 3.9968(11.4060) | Total Time 10.00(10.00)
Iter 2800 | Time 4.3203(4.5894) | Bit/dim 1.1503(1.2207) | Steps 526(535.54) | Grad Norm 4.6034(10.9928) | Total Time 10.00(10.00)
Iter 2810 | Time 4.3087(4.5645) | Bit/dim 1.2061(1.2120) | Steps 520(534.95) | Grad Norm 6.6922(10.0658) | Total Time 10.00(10.00)
Iter 2820 | Time 4.3019(4.5343) | Bit/dim 1.1639(1.2048) | Steps 520(534.51) | Grad Norm 4.3256(8.9631) | Total Time 10.00(10.00)
Iter 2830 | Time 4.4273(4.5248) | Bit/dim 1.2097(1.1989) | Steps 532(535.73) | Grad Norm 2.7045(7.6370) | Total Time 10.00(10.00)
Iter 2840 | Time 4.8375(4.5361) | Bit/dim 1.1745(1.1916) | Steps 550(536.34) | Grad Norm 7.8114(6.8819) | Total Time 10.00(10.00)
Iter 2850 | Time 4.1484(4.5296) | Bit/dim 1.2217(1.1857) | Steps 508(535.46) | Grad Norm 17.1778(7.7175) | Total Time 10.00(10.00)
Iter 2860 | Time 4.8621(4.5433) | Bit/dim 1.2050(1.1863) | Steps 562(536.95) | Grad Norm 9.3650(8.8204) | Total Time 10.00(10.00)
Iter 2870 | Time 4.3180(4.5350) | Bit/dim 1.1938(1.1961) | Steps 520(535.25) | Grad Norm 6.9081(9.8736) | Total Time 10.00(10.00)
Iter 2880 | Time 4.4804(4.5308) | Bit/dim 1.2138(1.2047) | Steps 538(535.87) | Grad Norm 6.0001(10.3499) | Total Time 10.00(10.00)
Iter 2890 | Time 4.5268(4.5185) | Bit/dim 1.1591(1.1962) | Steps 544(534.56) | Grad Norm 5.6719(9.2997) | Total Time 10.00(10.00)
Iter 2900 | Time 4.8648(4.5234) | Bit/dim 1.1713(1.1904) | Steps 562(535.65) | Grad Norm 9.7500(8.5265) | Total Time 10.00(10.00)
Iter 2910 | Time 4.3933(4.5303) | Bit/dim 1.1989(1.1828) | Steps 526(537.09) | Grad Norm 5.2265(7.7120) | Total Time 10.00(10.00)
Iter 2920 | Time 4.4104(4.5052) | Bit/dim 1.1344(1.1815) | Steps 532(536.43) | Grad Norm 9.0761(8.0888) | Total Time 10.00(10.00)
Iter 2930 | Time 4.2484(4.5023) | Bit/dim 1.2341(1.1858) | Steps 526(536.92) | Grad Norm 12.8644(9.8693) | Total Time 10.00(10.00)
Iter 2940 | Time 4.8571(4.5175) | Bit/dim 1.1818(1.1879) | Steps 562(538.53) | Grad Norm 17.4948(10.7512) | Total Time 10.00(10.00)
Iter 2950 | Time 4.4661(4.4918) | Bit/dim 1.1646(1.1878) | Steps 538(537.03) | Grad Norm 2.3259(10.8269) | Total Time 10.00(10.00)
Iter 2960 | Time 4.4423(4.4894) | Bit/dim 1.1596(1.1816) | Steps 538(537.13) | Grad Norm 6.3684(10.3213) | Total Time 10.00(10.00)
Iter 2970 | Time 4.4534(4.5033) | Bit/dim 1.1848(1.1793) | Steps 538(538.70) | Grad Norm 1.8416(9.2015) | Total Time 10.00(10.00)
Iter 2980 | Time 4.7067(4.5227) | Bit/dim 1.1847(1.1752) | Steps 556(540.20) | Grad Norm 15.0530(9.0345) | Total Time 10.00(10.00)
Iter 2990 | Time 4.4549(4.5061) | Bit/dim 1.1754(1.1750) | Steps 538(539.66) | Grad Norm 5.2267(9.4553) | Total Time 10.00(10.00)
validating...
Epoch 0010 | Time 62.2924, Bit/dim 1.1520
===> Using batch size 200. Total 300 iterations/epoch.
Iter 3000 | Time 4.4632(4.5285) | Bit/dim 1.1556(1.1710) | Steps 538(540.91) | Grad Norm 1.8763(8.4252) | Total Time 10.00(10.00)
Iter 3010 | Time 5.0647(4.5531) | Bit/dim 1.2359(1.1717) | Steps 574(541.45) | Grad Norm 24.5893(8.8769) | Total Time 10.00(10.00)
Iter 3020 | Time 5.0797(4.5965) | Bit/dim 1.1873(1.1732) | Steps 574(540.47) | Grad Norm 18.4748(9.7249) | Total Time 10.00(10.00)
Iter 3030 | Time 4.5054(4.6220) | Bit/dim 1.1989(1.1722) | Steps 532(541.25) | Grad Norm 4.2662(9.1410) | Total Time 10.00(10.00)
Iter 3040 | Time 4.4570(4.5912) | Bit/dim 1.1792(1.1685) | Steps 538(540.23) | Grad Norm 3.6160(7.6263) | Total Time 10.00(10.00)
Iter 3050 | Time 4.6159(4.5603) | Bit/dim 1.1549(1.1637) | Steps 532(539.32) | Grad Norm 1.1708(6.1076) | Total Time 10.00(10.00)
Iter 3060 | Time 4.5092(4.5472) | Bit/dim 1.1194(1.1611) | Steps 538(538.84) | Grad Norm 3.2950(5.1845) | Total Time 10.00(10.00)
Iter 3070 | Time 4.4348(4.5747) | Bit/dim 1.1653(1.1604) | Steps 532(538.76) | Grad Norm 1.3207(4.4681) | Total Time 10.00(10.00)
Iter 3080 | Time 4.7172(4.5907) | Bit/dim 1.1375(1.1568) | Steps 538(539.26) | Grad Norm 0.7072(4.2889) | Total Time 10.00(10.00)
Iter 3090 | Time 4.5130(4.5975) | Bit/dim 1.1746(1.1547) | Steps 526(539.59) | Grad Norm 10.9075(4.6259) | Total Time 10.00(10.00)
Iter 3100 | Time 4.8542(4.6105) | Bit/dim 1.1802(1.1589) | Steps 562(540.36) | Grad Norm 10.7812(6.1838) | Total Time 10.00(10.00)
Iter 3110 | Time 4.9396(4.5913) | Bit/dim 1.2038(1.1750) | Steps 568(540.04) | Grad Norm 29.7281(9.3102) | Total Time 10.00(10.00)
Iter 3120 | Time 4.0954(4.5671) | Bit/dim 1.2538(1.1805) | Steps 502(539.03) | Grad Norm 13.2659(10.1101) | Total Time 10.00(10.00)
Iter 3130 | Time 4.3569(4.5582) | Bit/dim 1.1960(1.1785) | Steps 526(539.18) | Grad Norm 7.1348(9.7659) | Total Time 10.00(10.00)
Iter 3140 | Time 4.4100(4.5497) | Bit/dim 1.2024(1.1769) | Steps 532(539.58) | Grad Norm 7.4790(9.7876) | Total Time 10.00(10.00)
Iter 3150 | Time 4.4501(4.5474) | Bit/dim 1.1450(1.1720) | Steps 538(540.39) | Grad Norm 2.5738(9.1943) | Total Time 10.00(10.00)
Iter 3160 | Time 4.9652(4.5241) | Bit/dim 1.1996(1.1706) | Steps 574(539.70) | Grad Norm 28.1287(9.5676) | Total Time 10.00(10.00)
Iter 3170 | Time 4.1418(4.5213) | Bit/dim 1.2392(1.1826) | Steps 508(538.60) | Grad Norm 15.1224(10.7761) | Total Time 10.00(10.00)
Iter 3180 | Time 4.8755(4.5281) | Bit/dim 1.1993(1.1928) | Steps 562(538.89) | Grad Norm 13.4010(11.4563) | Total Time 10.00(10.00)
Iter 3190 | Time 4.0491(4.5079) | Bit/dim 1.1787(1.1907) | Steps 514(537.95) | Grad Norm 9.1423(11.0269) | Total Time 10.00(10.00)
Iter 3200 | Time 4.3715(4.4771) | Bit/dim 1.1474(1.1860) | Steps 526(536.11) | Grad Norm 9.4875(10.6534) | Total Time 10.00(10.00)
Iter 3210 | Time 4.4198(4.4887) | Bit/dim 1.1508(1.1801) | Steps 532(536.55) | Grad Norm 5.5188(9.5796) | Total Time 10.00(10.00)
Iter 3220 | Time 4.4589(4.5050) | Bit/dim 1.1683(1.1759) | Steps 538(537.69) | Grad Norm 4.6005(8.6108) | Total Time 10.00(10.00)
Iter 3230 | Time 4.7018(4.5108) | Bit/dim 1.1622(1.1701) | Steps 556(538.64) | Grad Norm 12.7042(8.1583) | Total Time 10.00(10.00)
Iter 3240 | Time 4.3105(4.4959) | Bit/dim 1.1597(1.1717) | Steps 514(537.10) | Grad Norm 3.4274(9.0048) | Total Time 10.00(10.00)
Iter 3250 | Time 4.4508(4.4851) | Bit/dim 1.1218(1.1669) | Steps 538(537.00) | Grad Norm 2.6229(7.8008) | Total Time 10.00(10.00)
Iter 3260 | Time 4.2980(4.4520) | Bit/dim 1.1473(1.1622) | Steps 532(535.87) | Grad Norm 2.1580(6.5831) | Total Time 10.00(10.00)
Iter 3270 | Time 4.4573(4.4490) | Bit/dim 1.1464(1.1545) | Steps 538(536.13) | Grad Norm 1.4464(5.5229) | Total Time 10.00(10.00)
Iter 3280 | Time 4.3942(4.4487) | Bit/dim 1.1954(1.1548) | Steps 526(535.81) | Grad Norm 2.4909(5.0716) | Total Time 10.00(10.00)
Iter 3290 | Time 4.4524(4.4605) | Bit/dim 1.1472(1.1535) | Steps 538(536.46) | Grad Norm 3.5147(4.9695) | Total Time 10.00(10.00)
validating...
Epoch 0011 | Time 67.0390, Bit/dim 1.1446
===> Using batch size 200. Total 300 iterations/epoch.
Iter 3300 | Time 4.8886(4.4784) | Bit/dim 1.1697(1.1530) | Steps 562(537.13) | Grad Norm 8.5521(5.1791) | Total Time 10.00(10.00)
Iter 3310 | Time 4.1118(4.4794) | Bit/dim 1.1768(1.1558) | Steps 508(535.64) | Grad Norm 11.7971(6.9504) | Total Time 10.00(10.00)
Iter 3320 | Time 4.9433(4.5083) | Bit/dim 1.2055(1.1768) | Steps 568(536.50) | Grad Norm 12.8320(9.5587) | Total Time 10.00(10.00)
Iter 3330 | Time 4.5421(4.5039) | Bit/dim 1.1944(1.1981) | Steps 544(535.62) | Grad Norm 7.0676(10.1500) | Total Time 10.00(10.00)
Iter 3340 | Time 3.9887(4.4939) | Bit/dim 1.2677(1.2056) | Steps 502(534.92) | Grad Norm 16.0529(10.5465) | Total Time 10.00(10.00)
Iter 3350 | Time 4.3633(4.4691) | Bit/dim 1.1659(1.2041) | Steps 526(533.55) | Grad Norm 2.5578(10.4391) | Total Time 10.00(10.00)
Iter 3360 | Time 4.3518(4.4363) | Bit/dim 1.1661(1.1941) | Steps 520(531.49) | Grad Norm 6.8724(9.5649) | Total Time 10.00(10.00)
Iter 3370 | Time 4.3344(4.4247) | Bit/dim 1.1861(1.1829) | Steps 514(529.31) | Grad Norm 3.5273(8.1619) | Total Time 10.00(10.00)
Iter 3380 | Time 4.1683(4.3666) | Bit/dim 1.1456(1.1760) | Steps 508(524.71) | Grad Norm 0.7414(6.5690) | Total Time 10.00(10.00)
Iter 3390 | Time 4.1412(4.3320) | Bit/dim 1.1154(1.1656) | Steps 508(522.16) | Grad Norm 2.6197(5.5967) | Total Time 10.00(10.00)
Iter 3400 | Time 4.4954(4.3386) | Bit/dim 1.1594(1.1635) | Steps 526(521.95) | Grad Norm 2.3051(4.7874) | Total Time 10.00(10.00)
Iter 3410 | Time 4.3979(4.3423) | Bit/dim 1.1593(1.1585) | Steps 514(521.79) | Grad Norm 1.6186(4.0523) | Total Time 10.00(10.00)
Iter 3420 | Time 4.3397(4.3661) | Bit/dim 1.1353(1.1533) | Steps 508(522.42) | Grad Norm 10.0645(4.1582) | Total Time 10.00(10.00)
Iter 3430 | Time 4.1957(4.3985) | Bit/dim 1.1651(1.1524) | Steps 508(524.87) | Grad Norm 12.4596(4.9804) | Total Time 10.00(10.00)
Iter 3440 | Time 5.2631(4.4387) | Bit/dim 1.2208(1.1544) | Steps 574(527.69) | Grad Norm 38.2459(6.9947) | Total Time 10.00(10.00)
Iter 3450 | Time 4.3403(4.4530) | Bit/dim 1.2078(1.1774) | Steps 532(528.52) | Grad Norm 7.0604(8.1240) | Total Time 10.00(10.00)
Iter 3460 | Time 4.1722(4.4335) | Bit/dim 1.2469(1.1940) | Steps 520(527.44) | Grad Norm 7.3238(9.7267) | Total Time 10.00(10.00)
Iter 3470 | Time 4.4955(4.4570) | Bit/dim 1.1752(1.1964) | Steps 532(528.90) | Grad Norm 7.3800(10.0149) | Total Time 10.00(10.00)
Iter 3480 | Time 4.3271(4.4221) | Bit/dim 1.1480(1.1884) | Steps 514(525.73) | Grad Norm 8.6152(9.8771) | Total Time 10.00(10.00)
Iter 3490 | Time 4.3262(4.4164) | Bit/dim 1.1575(1.1788) | Steps 514(524.25) | Grad Norm 6.3911(8.9909) | Total Time 10.00(10.00)
Iter 3500 | Time 4.1306(4.4011) | Bit/dim 1.1622(1.1722) | Steps 508(522.58) | Grad Norm 2.7189(7.8800) | Total Time 10.00(10.00)
Iter 3510 | Time 4.8265(4.4074) | Bit/dim 1.1806(1.1689) | Steps 550(522.40) | Grad Norm 18.2542(8.7726) | Total Time 10.00(10.00)
Iter 3520 | Time 4.8675(4.3893) | Bit/dim 1.1554(1.1634) | Steps 550(521.85) | Grad Norm 10.4576(8.1304) | Total Time 10.00(10.00)
Iter 3530 | Time 5.2285(4.4008) | Bit/dim 1.2277(1.1679) | Steps 568(521.92) | Grad Norm 30.4043(9.7326) | Total Time 10.00(10.00)
Iter 3540 | Time 3.9916(4.3977) | Bit/dim 1.1834(1.1724) | Steps 496(521.91) | Grad Norm 9.2905(10.1967) | Total Time 10.00(10.00)
Iter 3550 | Time 4.3386(4.3910) | Bit/dim 1.1485(1.1667) | Steps 508(521.93) | Grad Norm 5.5258(8.9018) | Total Time 10.00(10.00)
Iter 3560 | Time 4.3889(4.3910) | Bit/dim 1.1218(1.1616) | Steps 520(521.60) | Grad Norm 4.2974(8.1736) | Total Time 10.00(10.00)
Iter 3570 | Time 4.7767(4.3961) | Bit/dim 1.1385(1.1602) | Steps 544(522.27) | Grad Norm 7.1171(8.6203) | Total Time 10.00(10.00)
Iter 3580 | Time 4.3373(4.3746) | Bit/dim 1.1435(1.1550) | Steps 520(521.51) | Grad Norm 4.4089(7.1792) | Total Time 10.00(10.00)
Iter 3590 | Time 4.1759(4.3609) | Bit/dim 1.1526(1.1527) | Steps 514(521.70) | Grad Norm 1.3716(6.3707) | Total Time 10.00(10.00)
validating...
Epoch 0012 | Time 60.7583, Bit/dim 1.1316
===> Using batch size 200. Total 300 iterations/epoch.
Iter 3600 | Time 4.3054(4.3571) | Bit/dim 1.1632(1.1506) | Steps 520(522.17) | Grad Norm 1.4877(5.5499) | Total Time 10.00(10.00)
Iter 3610 | Time 4.3078(4.3261) | Bit/dim 1.1150(1.1451) | Steps 526(521.27) | Grad Norm 1.9362(4.5557) | Total Time 10.00(10.00)
Iter 3620 | Time 4.2848(4.3474) | Bit/dim 1.1144(1.1447) | Steps 520(521.73) | Grad Norm 2.4060(4.4245) | Total Time 10.00(10.00)
Iter 3630 | Time 4.2676(4.3732) | Bit/dim 1.1484(1.1469) | Steps 520(521.61) | Grad Norm 3.4921(4.5464) | Total Time 10.00(10.00)
Iter 3640 | Time 4.3561(4.3678) | Bit/dim 1.1877(1.1441) | Steps 514(520.94) | Grad Norm 6.5372(4.7364) | Total Time 10.00(10.00)
Iter 3650 | Time 4.1089(4.3829) | Bit/dim 1.2266(1.1536) | Steps 502(521.41) | Grad Norm 17.3019(7.7170) | Total Time 10.00(10.00)
Iter 3660 | Time 4.7650(4.4230) | Bit/dim 1.2784(1.1754) | Steps 538(524.15) | Grad Norm 8.0949(9.3462) | Total Time 10.00(10.00)
Iter 3670 | Time 4.6007(4.4371) | Bit/dim 1.1761(1.1803) | Steps 538(525.37) | Grad Norm 10.5189(9.0535) | Total Time 10.00(10.00)
Iter 3680 | Time 3.8712(4.4102) | Bit/dim 1.2336(1.1880) | Steps 484(522.67) | Grad Norm 13.8370(9.9647) | Total Time 10.00(10.00)
Iter 3690 | Time 3.8696(4.3861) | Bit/dim 1.2022(1.1981) | Steps 484(520.98) | Grad Norm 12.1715(10.5072) | Total Time 10.00(10.00)
Iter 3700 | Time 4.0019(4.3616) | Bit/dim 1.1559(1.1927) | Steps 496(519.36) | Grad Norm 7.5949(10.2557) | Total Time 10.00(10.00)
Iter 3710 | Time 4.2173(4.3430) | Bit/dim 1.1411(1.1829) | Steps 508(517.30) | Grad Norm 2.1130(9.4135) | Total Time 10.00(10.00)
Iter 3720 | Time 4.3354(4.3398) | Bit/dim 1.1547(1.1750) | Steps 514(516.19) | Grad Norm 4.8800(8.1381) | Total Time 10.00(10.00)
Iter 3730 | Time 3.9679(4.3189) | Bit/dim 1.1507(1.1647) | Steps 502(515.20) | Grad Norm 6.6273(7.9170) | Total Time 10.00(10.00)
Iter 3740 | Time 4.2233(4.3097) | Bit/dim 1.1392(1.1612) | Steps 514(515.72) | Grad Norm 2.3567(8.7259) | Total Time 10.00(10.00)
Iter 3750 | Time 4.8793(4.3320) | Bit/dim 1.1810(1.1569) | Steps 556(518.13) | Grad Norm 14.7783(9.3451) | Total Time 10.00(10.00)
Iter 3760 | Time 4.0619(4.3357) | Bit/dim 1.1542(1.1535) | Steps 508(518.82) | Grad Norm 8.5776(9.7125) | Total Time 10.00(10.00)
Iter 3770 | Time 4.3741(4.3516) | Bit/dim 1.1458(1.1523) | Steps 520(519.74) | Grad Norm 2.7998(9.0982) | Total Time 10.00(10.00)
Iter 3780 | Time 4.1523(4.3554) | Bit/dim 1.1318(1.1521) | Steps 508(520.51) | Grad Norm 4.8046(9.5805) | Total Time 10.00(10.00)
Iter 3790 | Time 4.7449(4.3563) | Bit/dim 1.1616(1.1497) | Steps 544(520.53) | Grad Norm 14.0511(9.9862) | Total Time 10.00(10.00)
Iter 3800 | Time 3.9757(4.3580) | Bit/dim 1.1383(1.1474) | Steps 502(521.01) | Grad Norm 7.9453(10.3779) | Total Time 10.00(10.00)
Iter 3810 | Time 4.3350(4.3677) | Bit/dim 1.1643(1.1464) | Steps 520(521.67) | Grad Norm 2.4732(8.9815) | Total Time 10.00(10.00)
Iter 3820 | Time 4.2239(4.3415) | Bit/dim 1.1678(1.1462) | Steps 514(520.91) | Grad Norm 2.1804(7.5545) | Total Time 10.00(10.00)
Iter 3830 | Time 4.3848(4.3355) | Bit/dim 1.1122(1.1406) | Steps 520(521.30) | Grad Norm 3.4622(6.4982) | Total Time 10.00(10.00)
Iter 3840 | Time 4.3225(4.3170) | Bit/dim 1.1439(1.1376) | Steps 508(519.98) | Grad Norm 4.6821(5.5228) | Total Time 10.00(10.00)
Iter 3850 | Time 4.2663(4.3209) | Bit/dim 1.1431(1.1359) | Steps 526(519.50) | Grad Norm 2.7243(5.3811) | Total Time 10.00(10.00)
Iter 3860 | Time 4.2649(4.3371) | Bit/dim 1.1204(1.1328) | Steps 520(519.47) | Grad Norm 1.7773(5.2150) | Total Time 10.00(10.00)
Iter 3870 | Time 4.7416(4.3496) | Bit/dim 1.1215(1.1298) | Steps 538(519.98) | Grad Norm 10.2005(5.0328) | Total Time 10.00(10.00)
Iter 3880 | Time 5.3107(4.3834) | Bit/dim 1.1845(1.1360) | Steps 574(521.50) | Grad Norm 34.0942(6.8736) | Total Time 10.00(10.00)
Iter 3890 | Time 4.4829(4.4225) | Bit/dim 1.2263(1.1652) | Steps 538(523.90) | Grad Norm 6.7102(9.2570) | Total Time 10.00(10.00)
validating...
Epoch 0013 | Time 64.4437, Bit/dim 1.2347
===> Using batch size 200. Total 300 iterations/epoch.
Iter 3900 | Time 4.5177(4.4051) | Bit/dim 1.2525(1.1889) | Steps 538(523.05) | Grad Norm 6.1323(9.8518) | Total Time 10.00(10.00)
Iter 3910 | Time 4.9231(4.4205) | Bit/dim 1.1530(1.1857) | Steps 550(523.96) | Grad Norm 12.5715(9.3821) | Total Time 10.00(10.00)
Iter 3920 | Time 4.4873(4.4079) | Bit/dim 1.1565(1.1805) | Steps 520(522.08) | Grad Norm 9.6732(9.7022) | Total Time 10.00(10.00)
Iter 3930 | Time 4.0090(4.3530) | Bit/dim 1.1606(1.1745) | Steps 496(518.83) | Grad Norm 5.7825(8.9319) | Total Time 10.00(10.00)
Iter 3940 | Time 4.3431(4.3300) | Bit/dim 1.1594(1.1659) | Steps 514(517.35) | Grad Norm 2.3101(7.9515) | Total Time 10.00(10.00)
Iter 3950 | Time 4.4364(4.3300) | Bit/dim 1.1528(1.1625) | Steps 532(518.50) | Grad Norm 9.4267(8.8695) | Total Time 10.00(10.00)
Iter 3960 | Time 4.1312(4.3308) | Bit/dim 1.1417(1.1613) | Steps 502(518.84) | Grad Norm 14.3745(10.1067) | Total Time 10.00(10.00)
Iter 3970 | Time 4.1821(4.3313) | Bit/dim 1.1575(1.1663) | Steps 514(519.60) | Grad Norm 3.4041(10.4906) | Total Time 10.00(10.00)
Iter 3980 | Time 5.0331(4.3423) | Bit/dim 1.2115(1.1718) | Steps 550(519.70) | Grad Norm 23.5438(11.3970) | Total Time 10.00(10.00)
Iter 3990 | Time 4.0534(4.3484) | Bit/dim 1.1400(1.1690) | Steps 508(520.45) | Grad Norm 3.7306(10.8511) | Total Time 10.00(10.00)
Iter 4000 | Time 4.2378(4.3377) | Bit/dim 1.1301(1.1652) | Steps 514(520.04) | Grad Norm 3.8950(10.3542) | Total Time 10.00(10.00)
Iter 4010 | Time 3.9801(4.3138) | Bit/dim 1.1309(1.1604) | Steps 502(519.44) | Grad Norm 6.1878(9.5417) | Total Time 10.00(10.00)
Iter 4020 | Time 4.2988(4.3224) | Bit/dim 1.1150(1.1546) | Steps 514(519.89) | Grad Norm 5.0203(8.6135) | Total Time 10.00(10.00)
Iter 4030 | Time 4.1958(4.3233) | Bit/dim 1.1098(1.1466) | Steps 514(519.02) | Grad Norm 3.8484(7.4458) | Total Time 10.00(10.00)
Iter 4040 | Time 4.2235(4.3004) | Bit/dim 1.1032(1.1398) | Steps 508(516.88) | Grad Norm 2.1740(6.0501) | Total Time 10.00(10.00)
Iter 4050 | Time 4.3220(4.2799) | Bit/dim 1.1344(1.1364) | Steps 508(515.03) | Grad Norm 3.9569(5.0577) | Total Time 10.00(10.00)
Iter 4060 | Time 4.2615(4.2611) | Bit/dim 1.1185(1.1299) | Steps 520(513.53) | Grad Norm 2.3646(4.2088) | Total Time 10.00(10.00)
Iter 4070 | Time 4.3338(4.2632) | Bit/dim 1.1311(1.1266) | Steps 508(512.91) | Grad Norm 6.0535(3.9964) | Total Time 10.00(10.00)
Iter 4080 | Time 4.3168(4.3051) | Bit/dim 1.1217(1.1257) | Steps 508(514.27) | Grad Norm 9.0852(4.3485) | Total Time 10.00(10.00)
Iter 4090 | Time 4.2864(4.3209) | Bit/dim 1.2272(1.1409) | Steps 526(514.63) | Grad Norm 8.7302(6.8184) | Total Time 10.00(10.00)
Iter 4100 | Time 4.7635(4.3623) | Bit/dim 1.1662(1.1544) | Steps 532(517.61) | Grad Norm 17.2294(7.9190) | Total Time 10.00(10.00)
Iter 4110 | Time 4.1630(4.3421) | Bit/dim 1.1889(1.1574) | Steps 502(516.45) | Grad Norm 12.2821(8.6052) | Total Time 10.00(10.00)
Iter 4120 | Time 4.0397(4.3522) | Bit/dim 1.1409(1.1535) | Steps 520(519.72) | Grad Norm 7.9567(8.6898) | Total Time 10.00(10.00)
Iter 4130 | Time 4.1262(4.3525) | Bit/dim 1.1352(1.1494) | Steps 508(520.76) | Grad Norm 10.4190(9.0650) | Total Time 10.00(10.00)
Iter 4140 | Time 4.1799(4.3438) | Bit/dim 1.1176(1.1425) | Steps 514(520.14) | Grad Norm 0.9959(7.9298) | Total Time 10.00(10.00)
Iter 4150 | Time 4.2261(4.3232) | Bit/dim 1.0986(1.1372) | Steps 520(520.68) | Grad Norm 0.9851(6.6557) | Total Time 10.00(10.00)
Iter 4160 | Time 4.6309(4.3231) | Bit/dim 1.1436(1.1358) | Steps 532(520.41) | Grad Norm 7.1513(5.8950) | Total Time 10.00(10.00)
Iter 4170 | Time 4.6841(4.3215) | Bit/dim 1.1256(1.1323) | Steps 550(519.12) | Grad Norm 8.9094(5.9590) | Total Time 10.00(10.00)
Iter 4180 | Time 4.5785(4.3434) | Bit/dim 1.1205(1.1318) | Steps 532(520.42) | Grad Norm 6.2698(5.7730) | Total Time 10.00(10.00)
Iter 4190 | Time 4.3566(4.3444) | Bit/dim 1.1300(1.1299) | Steps 526(520.39) | Grad Norm 4.7260(5.2940) | Total Time 10.00(10.00)
validating...
Epoch 0014 | Time 62.5658, Bit/dim 1.1161
===> Using batch size 200. Total 300 iterations/epoch.
Iter 4200 | Time 4.6231(4.3555) | Bit/dim 1.1066(1.1285) | Steps 544(523.26) | Grad Norm 5.6402(5.2780) | Total Time 10.00(10.00)
Iter 4210 | Time 4.4511(4.3670) | Bit/dim 1.1208(1.1286) | Steps 514(522.89) | Grad Norm 3.5720(5.1754) | Total Time 10.00(10.00)
Iter 4220 | Time 5.1514(4.3999) | Bit/dim 1.1526(1.1266) | Steps 586(526.65) | Grad Norm 30.3848(6.8425) | Total Time 10.00(10.00)
Iter 4230 | Time 4.3537(4.3839) | Bit/dim 1.3903(1.1575) | Steps 514(526.22) | Grad Norm 7.9690(9.3654) | Total Time 10.00(10.00)
Iter 4240 | Time 4.3397(4.3578) | Bit/dim 1.2842(1.2031) | Steps 526(523.83) | Grad Norm 3.2863(8.0574) | Total Time 10.00(10.00)
Iter 4250 | Time 4.2342(4.3779) | Bit/dim 1.1499(1.1966) | Steps 514(524.88) | Grad Norm 3.5582(6.8746) | Total Time 10.00(10.00)
Iter 4260 | Time 4.1256(4.3301) | Bit/dim 1.1327(1.1833) | Steps 502(519.74) | Grad Norm 0.9020(5.8690) | Total Time 10.00(10.00)
Iter 4270 | Time 4.2597(4.3446) | Bit/dim 1.1390(1.1688) | Steps 526(521.93) | Grad Norm 1.8687(4.6660) | Total Time 10.00(10.00)
Iter 4280 | Time 4.2067(4.3147) | Bit/dim 1.1096(1.1551) | Steps 508(518.88) | Grad Norm 2.1824(3.9676) | Total Time 10.00(10.00)
Iter 4290 | Time 4.4020(4.3261) | Bit/dim 1.1106(1.1444) | Steps 532(521.20) | Grad Norm 5.4025(4.0140) | Total Time 10.00(10.00)
Iter 4300 | Time 4.2719(4.3094) | Bit/dim 1.1354(1.1382) | Steps 514(522.14) | Grad Norm 1.2709(3.4465) | Total Time 10.00(10.00)
Iter 4310 | Time 4.4260(4.3250) | Bit/dim 1.1572(1.1348) | Steps 538(522.89) | Grad Norm 7.3774(3.6732) | Total Time 10.00(10.00)
Iter 4320 | Time 4.2099(4.3216) | Bit/dim 1.1323(1.1312) | Steps 526(524.11) | Grad Norm 10.8985(4.6281) | Total Time 10.00(10.00)
Iter 4330 | Time 4.3348(4.3343) | Bit/dim 1.1167(1.1306) | Steps 514(524.22) | Grad Norm 5.1768(5.1607) | Total Time 10.00(10.00)
Iter 4340 | Time 4.2717(4.3396) | Bit/dim 1.0950(1.1274) | Steps 538(525.66) | Grad Norm 3.2236(4.9831) | Total Time 10.00(10.00)
Iter 4350 | Time 4.2662(4.3468) | Bit/dim 1.1285(1.1276) | Steps 526(526.49) | Grad Norm 2.1946(5.8459) | Total Time 10.00(10.00)
Iter 4360 | Time 4.0471(4.3526) | Bit/dim 1.2511(1.1468) | Steps 508(527.12) | Grad Norm 17.7473(8.5463) | Total Time 10.00(10.00)
Iter 4370 | Time 4.1357(4.3566) | Bit/dim 1.3139(1.1686) | Steps 502(525.97) | Grad Norm 9.4115(10.1674) | Total Time 10.00(10.00)
Iter 4380 | Time 4.4131(4.3405) | Bit/dim 1.1965(1.1921) | Steps 520(523.61) | Grad Norm 8.7359(9.4295) | Total Time 10.00(10.00)
Iter 4390 | Time 4.1287(4.3363) | Bit/dim 1.1494(1.1850) | Steps 502(523.70) | Grad Norm 9.0025(8.9766) | Total Time 10.00(10.00)
Iter 4400 | Time 4.1191(4.3119) | Bit/dim 1.1160(1.1783) | Steps 496(519.25) | Grad Norm 9.5238(8.9743) | Total Time 10.00(10.00)
Iter 4410 | Time 4.0521(4.2986) | Bit/dim 1.1117(1.1671) | Steps 496(519.25) | Grad Norm 6.5506(8.5162) | Total Time 10.00(10.00)
Iter 4420 | Time 3.9885(4.2660) | Bit/dim 1.1170(1.1579) | Steps 502(517.22) | Grad Norm 9.3492(8.7700) | Total Time 10.00(10.00)
Iter 4430 | Time 4.3766(4.2611) | Bit/dim 1.1254(1.1537) | Steps 532(517.06) | Grad Norm 10.6442(9.0289) | Total Time 10.00(10.00)
Iter 4440 | Time 4.1654(4.2677) | Bit/dim 1.1553(1.1484) | Steps 502(517.76) | Grad Norm 11.3240(9.3271) | Total Time 10.00(10.00)
Iter 4450 | Time 4.2497(4.2637) | Bit/dim 1.1366(1.1424) | Steps 520(517.66) | Grad Norm 9.4106(8.7498) | Total Time 10.00(10.00)
Iter 4460 | Time 4.0493(4.3021) | Bit/dim 1.1565(1.1420) | Steps 508(520.73) | Grad Norm 8.5140(9.0487) | Total Time 10.00(10.00)
Iter 4470 | Time 4.1524(4.3263) | Bit/dim 1.1457(1.1423) | Steps 508(523.70) | Grad Norm 12.8885(9.7550) | Total Time 10.00(10.00)
Iter 4480 | Time 4.0067(4.3237) | Bit/dim 1.1901(1.1495) | Steps 514(524.70) | Grad Norm 13.7042(10.7718) | Total Time 10.00(10.00)
Iter 4490 | Time 4.3706(4.3341) | Bit/dim 1.1579(1.1505) | Steps 514(525.05) | Grad Norm 4.7316(10.6408) | Total Time 10.00(10.00)
validating...
Epoch 0015 | Time 65.2691, Bit/dim 1.1752
===> Using batch size 200. Total 300 iterations/epoch.
Iter 4500 | Time 4.7302(4.3525) | Bit/dim 1.1755(1.1563) | Steps 532(525.60) | Grad Norm 21.4502(11.3078) | Total Time 10.00(10.00)
Iter 4510 | Time 4.2074(4.3310) | Bit/dim 1.1513(1.1588) | Steps 520(525.76) | Grad Norm 8.4106(11.4317) | Total Time 10.00(10.00)
Iter 4520 | Time 4.2128(4.3241) | Bit/dim 1.1821(1.1600) | Steps 526(525.28) | Grad Norm 10.2885(11.2322) | Total Time 10.00(10.00)
Iter 4530 | Time 4.2358(4.3151) | Bit/dim 1.0958(1.1554) | Steps 520(524.82) | Grad Norm 3.4897(10.2411) | Total Time 10.00(10.00)
Iter 4540 | Time 4.4164(4.3111) | Bit/dim 1.1432(1.1505) | Steps 532(525.61) | Grad Norm 4.5237(9.1514) | Total Time 10.00(10.00)
Iter 4550 | Time 4.3589(4.3068) | Bit/dim 1.1045(1.1440) | Steps 526(525.40) | Grad Norm 3.1539(8.1332) | Total Time 10.00(10.00)
Iter 4560 | Time 4.5733(4.3433) | Bit/dim 1.1333(1.1403) | Steps 538(526.97) | Grad Norm 8.6716(7.4647) | Total Time 10.00(10.00)
Iter 4570 | Time 4.1290(4.3561) | Bit/dim 1.1704(1.1388) | Steps 508(527.90) | Grad Norm 13.8619(7.9628) | Total Time 10.00(10.00)
Iter 4580 | Time 4.6592(4.3790) | Bit/dim 1.1624(1.1468) | Steps 556(529.53) | Grad Norm 6.5399(8.9405) | Total Time 10.00(10.00)
Iter 4590 | Time 4.4792(4.3709) | Bit/dim 1.1636(1.1500) | Steps 520(527.43) | Grad Norm 16.6622(9.9339) | Total Time 10.00(10.00)
Iter 4600 | Time 4.2536(4.3526) | Bit/dim 1.1229(1.1483) | Steps 526(525.60) | Grad Norm 9.8070(10.3239) | Total Time 10.00(10.00)
Iter 4610 | Time 4.1940(4.3320) | Bit/dim 1.1355(1.1465) | Steps 514(525.64) | Grad Norm 6.3644(10.3634) | Total Time 10.00(10.00)
Iter 4620 | Time 4.0385(4.3421) | Bit/dim 1.1160(1.1405) | Steps 508(525.85) | Grad Norm 6.6048(9.7013) | Total Time 10.00(10.00)
Iter 4630 | Time 4.3741(4.3218) | Bit/dim 1.1061(1.1363) | Steps 526(524.75) | Grad Norm 1.4256(8.7545) | Total Time 10.00(10.00)
Iter 4640 | Time 4.3678(4.3376) | Bit/dim 1.0755(1.1311) | Steps 526(524.81) | Grad Norm 4.4154(7.7005) | Total Time 10.00(10.00)
Iter 4650 | Time 4.2088(4.3490) | Bit/dim 1.1180(1.1298) | Steps 520(525.25) | Grad Norm 7.1552(7.7462) | Total Time 10.00(10.00)
Iter 4660 | Time 4.5003(4.3469) | Bit/dim 1.1395(1.1248) | Steps 550(525.68) | Grad Norm 12.4039(8.0122) | Total Time 10.00(10.00)
Iter 4670 | Time 4.4036(4.3481) | Bit/dim 1.1144(1.1249) | Steps 526(525.79) | Grad Norm 4.5150(8.5964) | Total Time 10.00(10.00)
Iter 4680 | Time 4.1375(4.3378) | Bit/dim 1.0979(1.1203) | Steps 520(525.34) | Grad Norm 6.1877(8.2195) | Total Time 10.00(10.00)
Iter 4690 | Time 4.4526(4.3408) | Bit/dim 1.0938(1.1219) | Steps 544(526.49) | Grad Norm 12.3286(9.0454) | Total Time 10.00(10.00)
Iter 4700 | Time 4.0518(4.3485) | Bit/dim 1.1269(1.1209) | Steps 514(526.70) | Grad Norm 7.6784(8.3023) | Total Time 10.00(10.00)
Iter 4710 | Time 4.1971(4.3718) | Bit/dim 1.1317(1.1235) | Steps 520(529.65) | Grad Norm 10.2474(8.7261) | Total Time 10.00(10.00)
Iter 4720 | Time 4.0530(4.3615) | Bit/dim 1.1325(1.1237) | Steps 514(529.21) | Grad Norm 6.4911(8.9983) | Total Time 10.00(10.00)
Iter 4730 | Time 4.7277(4.3679) | Bit/dim 1.1130(1.1218) | Steps 568(529.74) | Grad Norm 18.0868(9.0376) | Total Time 10.00(10.00)
Iter 4740 | Time 4.4115(4.3480) | Bit/dim 1.1119(1.1219) | Steps 526(529.48) | Grad Norm 3.6536(8.9164) | Total Time 10.00(10.00)
Iter 4750 | Time 4.2542(4.3599) | Bit/dim 1.1217(1.1242) | Steps 520(530.60) | Grad Norm 10.2051(9.0245) | Total Time 10.00(10.00)
Iter 4760 | Time 4.6213(4.3788) | Bit/dim 1.1517(1.1205) | Steps 538(531.88) | Grad Norm 8.7196(8.6243) | Total Time 10.00(10.00)
Iter 4770 | Time 4.3367(4.3906) | Bit/dim 1.1319(1.1228) | Steps 538(533.64) | Grad Norm 4.5996(8.7957) | Total Time 10.00(10.00)
Iter 4780 | Time 4.0445(4.3719) | Bit/dim 1.1761(1.1346) | Steps 514(532.89) | Grad Norm 13.3161(9.8585) | Total Time 10.00(10.00)
Iter 4790 | Time 4.2207(4.3619) | Bit/dim 1.2269(1.1490) | Steps 532(532.39) | Grad Norm 8.9464(10.6044) | Total Time 10.00(10.00)
validating...
Epoch 0016 | Time 59.0033, Bit/dim 1.1439
===> Using batch size 200. Total 300 iterations/epoch.
Iter 4800 | Time 4.3108(4.3830) | Bit/dim 1.1165(1.1523) | Steps 538(533.36) | Grad Norm 6.1465(9.8507) | Total Time 10.00(10.00)
Iter 4810 | Time 4.3961(4.3558) | Bit/dim 1.1265(1.1492) | Steps 526(530.88) | Grad Norm 5.1874(9.1665) | Total Time 10.00(10.00)
Iter 4820 | Time 4.3821(4.3450) | Bit/dim 1.1363(1.1422) | Steps 532(529.65) | Grad Norm 3.2812(7.7357) | Total Time 10.00(10.00)
Iter 4830 | Time 4.2491(4.3372) | Bit/dim 1.0955(1.1338) | Steps 532(530.43) | Grad Norm 1.8372(6.2651) | Total Time 10.00(10.00)
Iter 4840 | Time 4.2687(4.3285) | Bit/dim 1.1093(1.1276) | Steps 526(530.71) | Grad Norm 1.0259(4.9493) | Total Time 10.00(10.00)
Iter 4850 | Time 4.4838(4.3252) | Bit/dim 1.1421(1.1262) | Steps 544(532.33) | Grad Norm 4.0304(4.2669) | Total Time 10.00(10.00)
Iter 4860 | Time 4.3441(4.3254) | Bit/dim 1.0537(1.1203) | Steps 538(533.79) | Grad Norm 2.5328(3.6911) | Total Time 10.00(10.00)
Iter 4870 | Time 4.6861(4.3410) | Bit/dim 1.0959(1.1170) | Steps 556(535.59) | Grad Norm 6.5115(3.6447) | Total Time 10.00(10.00)
Iter 4880 | Time 4.3069(4.3546) | Bit/dim 1.1350(1.1146) | Steps 538(537.29) | Grad Norm 2.9348(4.1601) | Total Time 10.00(10.00)
Iter 4890 | Time 4.0897(4.3670) | Bit/dim 1.1048(1.1116) | Steps 526(538.06) | Grad Norm 7.4636(4.4366) | Total Time 10.00(10.00)
Iter 4900 | Time 4.7200(4.3836) | Bit/dim 1.1034(1.1117) | Steps 568(538.90) | Grad Norm 20.2983(5.6953) | Total Time 10.00(10.00)
Iter 4910 | Time 4.1129(4.3761) | Bit/dim 1.2867(1.1337) | Steps 514(537.47) | Grad Norm 10.9384(8.3375) | Total Time 10.00(10.00)
Iter 4920 | Time 4.4534(4.3846) | Bit/dim 1.1660(1.1504) | Steps 538(536.96) | Grad Norm 4.9307(8.9368) | Total Time 10.00(10.00)
Iter 4930 | Time 4.2027(4.3964) | Bit/dim 1.1421(1.1561) | Steps 520(536.18) | Grad Norm 7.2909(9.7120) | Total Time 10.00(10.00)
Iter 4940 | Time 4.2165(4.3693) | Bit/dim 1.1428(1.1543) | Steps 514(532.09) | Grad Norm 10.4544(9.9445) | Total Time 10.00(10.00)
Iter 4950 | Time 4.8361(4.3810) | Bit/dim 1.1093(1.1439) | Steps 550(530.89) | Grad Norm 9.7047(9.1743) | Total Time 10.00(10.00)
Iter 4960 | Time 4.2909(4.3439) | Bit/dim 1.1277(1.1383) | Steps 526(528.57) | Grad Norm 6.1859(8.2319) | Total Time 10.00(10.00)
Iter 4970 | Time 4.6022(4.3599) | Bit/dim 1.1180(1.1276) | Steps 544(529.56) | Grad Norm 6.8953(7.0904) | Total Time 10.00(10.00)
Iter 4980 | Time 4.2470(4.3673) | Bit/dim 1.1404(1.1247) | Steps 532(531.80) | Grad Norm 10.5537(7.2943) | Total Time 10.00(10.00)
Iter 4990 | Time 4.4548(4.3795) | Bit/dim 1.1347(1.1215) | Steps 544(534.50) | Grad Norm 3.5480(7.0605) | Total Time 10.00(10.00)
Iter 5000 | Time 4.3231(4.3779) | Bit/dim 1.1201(1.1194) | Steps 544(536.56) | Grad Norm 2.8140(6.0544) | Total Time 10.00(10.00)
Iter 5010 | Time 4.3079(4.3763) | Bit/dim 1.0903(1.1159) | Steps 538(537.60) | Grad Norm 3.2485(5.7486) | Total Time 10.00(10.00)
Iter 5020 | Time 4.4924(4.3729) | Bit/dim 1.1341(1.1138) | Steps 544(538.61) | Grad Norm 4.8954(5.5294) | Total Time 10.00(10.00)
Iter 5030 | Time 4.3392(4.3680) | Bit/dim 1.1010(1.1100) | Steps 538(538.73) | Grad Norm 2.7699(4.7303) | Total Time 10.00(10.00)
Iter 5040 | Time 4.4703(4.3954) | Bit/dim 1.1378(1.1107) | Steps 538(539.87) | Grad Norm 5.8102(4.6723) | Total Time 10.00(10.00)
Iter 5050 | Time 4.5471(4.4081) | Bit/dim 1.1145(1.1107) | Steps 550(540.51) | Grad Norm 10.6654(5.2314) | Total Time 10.00(10.00)
Iter 5060 | Time 4.1744(4.4077) | Bit/dim 1.2272(1.1154) | Steps 514(540.37) | Grad Norm 17.2075(6.4655) | Total Time 10.00(10.00)
Iter 5070 | Time 4.2757(4.4305) | Bit/dim 1.1988(1.1399) | Steps 520(541.00) | Grad Norm 11.3944(8.5163) | Total Time 10.00(10.00)
Iter 5080 | Time 4.2264(4.4120) | Bit/dim 1.1087(1.1467) | Steps 520(537.67) | Grad Norm 7.3024(9.3248) | Total Time 10.00(10.00)
Iter 5090 | Time 4.4159(4.3867) | Bit/dim 1.1220(1.1460) | Steps 538(532.99) | Grad Norm 3.8698(8.9113) | Total Time 10.00(10.00)
validating...
Epoch 0017 | Time 56.6222, Bit/dim 1.1152
===> Using batch size 200. Total 300 iterations/epoch.
Iter 5100 | Time 4.1315(4.3833) | Bit/dim 1.1025(1.1402) | Steps 514(532.51) | Grad Norm 6.4403(8.8282) | Total Time 10.00(10.00)
Iter 5110 | Time 4.1041(4.3709) | Bit/dim 1.1232(1.1347) | Steps 526(533.42) | Grad Norm 5.5308(8.3475) | Total Time 10.00(10.00)
Iter 5120 | Time 4.3522(4.3684) | Bit/dim 1.1420(1.1294) | Steps 544(535.29) | Grad Norm 2.2605(7.1390) | Total Time 10.00(10.00)
Iter 5130 | Time 4.4965(4.3836) | Bit/dim 1.0762(1.1282) | Steps 550(537.12) | Grad Norm 4.9765(7.7753) | Total Time 10.00(10.00)
Iter 5140 | Time 5.1994(4.4002) | Bit/dim 1.2034(1.1406) | Steps 592(538.08) | Grad Norm 33.7933(9.6956) | Total Time 10.00(10.00)
Iter 5150 | Time 4.7003(4.3818) | Bit/dim 1.1701(1.1499) | Steps 550(535.76) | Grad Norm 5.0351(9.7629) | Total Time 10.00(10.00)
Iter 5160 | Time 4.2966(4.3791) | Bit/dim 1.1216(1.1458) | Steps 532(534.71) | Grad Norm 3.6938(9.0935) | Total Time 10.00(10.00)
Iter 5170 | Time 4.6753(4.3614) | Bit/dim 1.1329(1.1375) | Steps 550(535.12) | Grad Norm 8.8277(8.1322) | Total Time 10.00(10.00)
Iter 5180 | Time 4.3049(4.3500) | Bit/dim 1.1195(1.1294) | Steps 538(535.90) | Grad Norm 6.6163(7.2696) | Total Time 10.00(10.00)
Iter 5190 | Time 4.3462(4.3667) | Bit/dim 1.1127(1.1217) | Steps 550(538.33) | Grad Norm 1.5331(5.9976) | Total Time 10.00(10.00)
Iter 5200 | Time 4.4292(4.3932) | Bit/dim 1.1099(1.1158) | Steps 544(540.16) | Grad Norm 1.8590(5.1500) | Total Time 10.00(10.00)
Iter 5210 | Time 4.3338(4.3861) | Bit/dim 1.1331(1.1117) | Steps 550(542.60) | Grad Norm 3.3177(4.6034) | Total Time 10.00(10.00)
Iter 5220 | Time 4.3292(4.3769) | Bit/dim 1.0886(1.1064) | Steps 550(544.68) | Grad Norm 2.0343(3.9748) | Total Time 10.00(10.00)
Iter 5230 | Time 4.4216(4.3855) | Bit/dim 1.0986(1.1039) | Steps 544(545.71) | Grad Norm 5.0972(4.2951) | Total Time 10.00(10.00)
Iter 5240 | Time 4.3348(4.4130) | Bit/dim 1.1098(1.1030) | Steps 550(547.61) | Grad Norm 1.4191(4.2543) | Total Time 10.00(10.00)
Iter 5250 | Time 4.2935(4.4142) | Bit/dim 1.1102(1.1015) | Steps 544(548.36) | Grad Norm 1.7613(3.9793) | Total Time 10.00(10.00)
Iter 5260 | Time 4.2483(4.3996) | Bit/dim 1.1050(1.1009) | Steps 538(548.29) | Grad Norm 12.8707(4.6582) | Total Time 10.00(10.00)
Iter 5270 | Time 4.2380(4.3968) | Bit/dim 1.1142(1.1026) | Steps 538(548.53) | Grad Norm 12.4675(5.4958) | Total Time 10.00(10.00)
Iter 5280 | Time 4.3361(4.3911) | Bit/dim 1.2366(1.1355) | Steps 550(547.91) | Grad Norm 5.7150(7.4486) | Total Time 10.00(10.00)
Iter 5290 | Time 4.3870(4.3852) | Bit/dim 1.1549(1.1483) | Steps 532(545.18) | Grad Norm 13.5785(7.5556) | Total Time 10.00(10.00)
Iter 5300 | Time 3.8793(4.3451) | Bit/dim 1.1960(1.1480) | Steps 496(541.77) | Grad Norm 14.6194(8.4552) | Total Time 10.00(10.00)
Iter 5310 | Time 4.4112(4.3335) | Bit/dim 1.1727(1.1514) | Steps 538(540.44) | Grad Norm 7.5881(9.6634) | Total Time 10.00(10.00)
Iter 5320 | Time 4.1553(4.3267) | Bit/dim 1.1278(1.1484) | Steps 538(539.44) | Grad Norm 2.5835(9.0109) | Total Time 10.00(10.00)
Iter 5330 | Time 4.2814(4.3348) | Bit/dim 1.1243(1.1429) | Steps 538(540.10) | Grad Norm 9.7602(9.4010) | Total Time 10.00(10.00)
Iter 5340 | Time 4.2831(4.3198) | Bit/dim 1.1499(1.1440) | Steps 544(540.73) | Grad Norm 9.3044(10.0893) | Total Time 10.00(10.00)
Iter 5350 | Time 4.4400(4.3357) | Bit/dim 1.1427(1.1444) | Steps 538(541.32) | Grad Norm 13.6738(10.2438) | Total Time 10.00(10.00)
Iter 5360 | Time 4.4600(4.3173) | Bit/dim 1.0748(1.1361) | Steps 544(540.44) | Grad Norm 2.3783(8.9311) | Total Time 10.00(10.00)
Iter 5370 | Time 4.3332(4.3358) | Bit/dim 1.1286(1.1297) | Steps 550(541.48) | Grad Norm 5.7633(7.9429) | Total Time 10.00(10.00)
Iter 5380 | Time 4.4979(4.3551) | Bit/dim 1.0730(1.1196) | Steps 556(544.34) | Grad Norm 2.8583(6.5183) | Total Time 10.00(10.00)
Iter 5390 | Time 4.3255(4.3563) | Bit/dim 1.0794(1.1125) | Steps 550(545.85) | Grad Norm 3.9891(5.5818) | Total Time 10.00(10.00)
validating...
Epoch 0018 | Time 59.6199, Bit/dim 1.0920
===> Using batch size 200. Total 300 iterations/epoch.
Iter 5400 | Time 4.3455(4.3590) | Bit/dim 1.0838(1.1088) | Steps 550(547.09) | Grad Norm 1.0382(4.8549) | Total Time 10.00(10.00)
Iter 5410 | Time 4.3279(4.3593) | Bit/dim 1.1013(1.1079) | Steps 550(548.00) | Grad Norm 2.2274(4.2897) | Total Time 10.00(10.00)
Iter 5420 | Time 4.3336(4.3696) | Bit/dim 1.1125(1.1059) | Steps 550(549.09) | Grad Norm 1.4768(4.0600) | Total Time 10.00(10.00)
Iter 5430 | Time 4.4372(4.3762) | Bit/dim 1.1291(1.1064) | Steps 550(549.41) | Grad Norm 4.4603(3.8193) | Total Time 10.00(10.00)
Iter 5440 | Time 4.4861(4.3799) | Bit/dim 1.0932(1.1026) | Steps 556(549.88) | Grad Norm 8.6231(3.7301) | Total Time 10.00(10.00)
Iter 5450 | Time 4.2443(4.3702) | Bit/dim 1.0949(1.1029) | Steps 538(548.87) | Grad Norm 11.0605(5.2714) | Total Time 10.00(10.00)
Iter 5460 | Time 4.3022(4.3702) | Bit/dim 1.1456(1.1230) | Steps 538(547.01) | Grad Norm 4.5420(7.3116) | Total Time 10.00(10.00)
Iter 5470 | Time 4.2761(4.3863) | Bit/dim 1.1302(1.1289) | Steps 532(545.18) | Grad Norm 7.6789(7.9503) | Total Time 10.00(10.00)
Iter 5480 | Time 4.1357(4.3521) | Bit/dim 1.1282(1.1337) | Steps 520(541.40) | Grad Norm 9.3212(9.2439) | Total Time 10.00(10.00)
Iter 5490 | Time 4.8799(4.3685) | Bit/dim 1.1253(1.1342) | Steps 568(541.25) | Grad Norm 17.3904(9.5106) | Total Time 10.00(10.00)
Iter 5500 | Time 4.1461(4.3625) | Bit/dim 1.1548(1.1349) | Steps 538(540.77) | Grad Norm 10.1794(9.5365) | Total Time 10.00(10.00)
Iter 5510 | Time 4.4750(4.3655) | Bit/dim 1.1306(1.1313) | Steps 550(541.52) | Grad Norm 13.8846(9.3029) | Total Time 10.00(10.00)
Iter 5520 | Time 3.9485(4.3318) | Bit/dim 1.1194(1.1259) | Steps 508(539.34) | Grad Norm 10.6183(9.4835) | Total Time 10.00(10.00)
Iter 5530 | Time 4.0024(4.3063) | Bit/dim 1.1468(1.1331) | Steps 514(538.03) | Grad Norm 7.8494(9.8753) | Total Time 10.00(10.00)
Iter 5540 | Time 4.6296(4.3339) | Bit/dim 1.1461(1.1293) | Steps 550(538.15) | Grad Norm 14.5839(9.4676) | Total Time 10.00(10.00)
Iter 5550 | Time 4.2686(4.3281) | Bit/dim 1.1281(1.1298) | Steps 538(538.25) | Grad Norm 9.8572(9.7051) | Total Time 10.00(10.00)
Iter 5560 | Time 4.4849(4.3249) | Bit/dim 1.1169(1.1261) | Steps 550(539.63) | Grad Norm 12.7845(9.3828) | Total Time 10.00(10.00)
Iter 5570 | Time 4.3089(4.3299) | Bit/dim 1.1125(1.1212) | Steps 544(541.33) | Grad Norm 1.3868(8.5555) | Total Time 10.00(10.00)
Iter 5580 | Time 4.4717(4.3399) | Bit/dim 1.0986(1.1154) | Steps 550(542.78) | Grad Norm 3.4520(8.0157) | Total Time 10.00(10.00)
Iter 5590 | Time 4.4668(4.3386) | Bit/dim 1.0995(1.1101) | Steps 550(543.32) | Grad Norm 12.5103(7.8995) | Total Time 10.00(10.00)
Iter 5600 | Time 4.3071(4.3346) | Bit/dim 1.1086(1.1072) | Steps 544(543.48) | Grad Norm 2.6154(7.9192) | Total Time 10.00(10.00)
Iter 5610 | Time 4.1215(4.3346) | Bit/dim 1.0825(1.1059) | Steps 538(543.82) | Grad Norm 6.7925(6.9546) | Total Time 10.00(10.00)
Iter 5620 | Time 4.1162(4.3554) | Bit/dim 1.1512(1.1137) | Steps 538(545.83) | Grad Norm 11.3394(8.5003) | Total Time 10.00(10.00)
Iter 5630 | Time 4.2591(4.3531) | Bit/dim 1.1317(1.1237) | Steps 532(544.67) | Grad Norm 8.1794(9.3645) | Total Time 10.00(10.00)
Iter 5640 | Time 4.2695(4.3463) | Bit/dim 1.0887(1.1275) | Steps 532(542.22) | Grad Norm 6.0797(9.5926) | Total Time 10.00(10.00)
Iter 5650 | Time 4.1382(4.3223) | Bit/dim 1.1206(1.1256) | Steps 538(541.02) | Grad Norm 4.1883(8.7707) | Total Time 10.00(10.00)
Iter 5660 | Time 4.1453(4.3051) | Bit/dim 1.1067(1.1207) | Steps 538(541.95) | Grad Norm 5.7624(7.7102) | Total Time 10.00(10.00)
Iter 5670 | Time 4.4584(4.3153) | Bit/dim 1.0742(1.1123) | Steps 550(544.14) | Grad Norm 3.2175(6.7923) | Total Time 10.00(10.00)
Iter 5680 | Time 4.6192(4.3242) | Bit/dim 1.0958(1.1114) | Steps 556(545.55) | Grad Norm 9.2980(7.4735) | Total Time 10.00(10.00)
Iter 5690 | Time 4.4644(4.3322) | Bit/dim 1.0974(1.1102) | Steps 550(545.98) | Grad Norm 8.8940(7.5899) | Total Time 10.00(10.00)
validating...
Epoch 0019 | Time 58.3096, Bit/dim 1.0913
===> Using batch size 200. Total 300 iterations/epoch.
Iter 5700 | Time 4.4795(4.3467) | Bit/dim 1.1091(1.1078) | Steps 550(546.70) | Grad Norm 4.9940(6.5320) | Total Time 10.00(10.00)
Iter 5710 | Time 4.2103(4.3431) | Bit/dim 1.0845(1.1054) | Steps 556(548.56) | Grad Norm 4.9451(6.3686) | Total Time 10.00(10.00)
Iter 5720 | Time 4.4762(4.3454) | Bit/dim 1.1290(1.1141) | Steps 550(547.99) | Grad Norm 2.7494(8.4288) | Total Time 10.00(10.00)
Iter 5730 | Time 4.2719(4.3471) | Bit/dim 1.1247(1.1153) | Steps 538(546.42) | Grad Norm 8.7449(8.4172) | Total Time 10.00(10.00)
Iter 5740 | Time 4.1212(4.3515) | Bit/dim 1.1015(1.1115) | Steps 520(545.72) | Grad Norm 11.1494(8.6191) | Total Time 10.00(10.00)
Iter 5750 | Time 4.3140(4.3413) | Bit/dim 1.1212(1.1106) | Steps 544(544.79) | Grad Norm 9.5214(8.7509) | Total Time 10.00(10.00)
Iter 5760 | Time 4.4742(4.3454) | Bit/dim 1.1084(1.1084) | Steps 550(545.08) | Grad Norm 15.5930(9.0232) | Total Time 10.00(10.00)
Iter 5770 | Time 4.4773(4.3456) | Bit/dim 1.1248(1.1075) | Steps 550(545.55) | Grad Norm 14.6612(8.6219) | Total Time 10.00(10.00)
Iter 5780 | Time 4.3768(4.3414) | Bit/dim 1.1104(1.1033) | Steps 562(545.97) | Grad Norm 7.9082(8.5119) | Total Time 10.00(10.00)
Iter 5790 | Time 4.1723(4.3337) | Bit/dim 1.1148(1.1050) | Steps 550(546.38) | Grad Norm 5.8035(8.6578) | Total Time 10.00(10.00)
Iter 5800 | Time 4.4751(4.3495) | Bit/dim 1.1049(1.1020) | Steps 550(547.79) | Grad Norm 14.2324(8.2857) | Total Time 10.00(10.00)
Iter 5810 | Time 4.3808(4.3464) | Bit/dim 1.1456(1.1107) | Steps 562(547.54) | Grad Norm 8.4019(9.5962) | Total Time 10.00(10.00)
Iter 5820 | Time 4.2946(4.3501) | Bit/dim 1.0960(1.1192) | Steps 538(546.87) | Grad Norm 4.0032(9.5389) | Total Time 10.00(10.00)
Iter 5830 | Time 4.2512(4.3539) | Bit/dim 1.1247(1.1211) | Steps 538(544.93) | Grad Norm 10.5217(9.7444) | Total Time 10.00(10.00)
Iter 5840 | Time 4.3040(4.3329) | Bit/dim 1.0823(1.1226) | Steps 544(544.04) | Grad Norm 5.2722(9.5082) | Total Time 10.00(10.00)
Iter 5850 | Time 4.1642(4.3359) | Bit/dim 1.0863(1.1162) | Steps 544(545.34) | Grad Norm 5.5912(8.5382) | Total Time 10.00(10.00)
Iter 5860 | Time 4.5257(4.3570) | Bit/dim 1.0830(1.1110) | Steps 562(547.69) | Grad Norm 2.4816(7.4761) | Total Time 10.00(10.00)
Iter 5870 | Time 4.3172(4.3760) | Bit/dim 1.0979(1.1074) | Steps 550(550.35) | Grad Norm 3.5439(6.4919) | Total Time 10.00(10.00)
Iter 5880 | Time 4.3585(4.3751) | Bit/dim 1.0810(1.1040) | Steps 556(552.00) | Grad Norm 0.8651(5.2965) | Total Time 10.00(10.00)
Iter 5890 | Time 4.3773(4.3768) | Bit/dim 1.0827(1.0997) | Steps 562(553.88) | Grad Norm 1.8155(4.6163) | Total Time 10.00(10.00)
Iter 5900 | Time 4.3682(4.3882) | Bit/dim 1.0903(1.0970) | Steps 562(555.75) | Grad Norm 2.1367(4.3121) | Total Time 10.00(10.00)
Iter 5910 | Time 4.3634(4.4016) | Bit/dim 1.0814(1.0955) | Steps 562(557.38) | Grad Norm 2.7924(4.2702) | Total Time 10.00(10.00)
Iter 5920 | Time 4.3689(4.4104) | Bit/dim 1.0971(1.0964) | Steps 562(558.87) | Grad Norm 2.8375(4.4086) | Total Time 10.00(10.00)
Iter 5930 | Time 4.6850(4.4022) | Bit/dim 1.0986(1.0959) | Steps 574(558.81) | Grad Norm 6.8711(4.7889) | Total Time 10.00(10.00)
Iter 5940 | Time 4.3733(4.3943) | Bit/dim 1.0836(1.0940) | Steps 562(559.46) | Grad Norm 1.5642(4.0019) | Total Time 10.00(10.00)
Iter 5950 | Time 4.5302(4.3959) | Bit/dim 1.0760(1.0935) | Steps 568(560.00) | Grad Norm 6.0253(4.3825) | Total Time 10.00(10.00)
Iter 5960 | Time 4.4979(4.4136) | Bit/dim 1.0978(1.1043) | Steps 562(559.77) | Grad Norm 3.1677(6.3421) | Total Time 10.00(10.00)
Iter 5970 | Time 3.8456(4.4079) | Bit/dim 1.1677(1.1133) | Steps 496(558.49) | Grad Norm 12.5751(7.6212) | Total Time 10.00(10.00)
Iter 5980 | Time 4.1409(4.3991) | Bit/dim 1.1367(1.1279) | Steps 526(555.24) | Grad Norm 8.0569(8.9369) | Total Time 10.00(10.00)
Iter 5990 | Time 4.1712(4.3905) | Bit/dim 1.1162(1.1312) | Steps 544(553.59) | Grad Norm 2.4723(9.2039) | Total Time 10.00(10.00)
validating...
Epoch 0020 | Time 57.3303, Bit/dim 1.1077
===> Using batch size 200. Total 300 iterations/epoch.
Iter 6000 | Time 4.2313(4.3828) | Bit/dim 1.0996(1.1281) | Steps 538(552.06) | Grad Norm 7.7535(8.8566) | Total Time 10.00(10.00)
Iter 6010 | Time 4.3654(4.3827) | Bit/dim 1.1050(1.1224) | Steps 556(551.83) | Grad Norm 6.3006(8.5250) | Total Time 10.00(10.00)
Iter 6020 | Time 4.2018(4.3847) | Bit/dim 1.1217(1.1173) | Steps 550(553.07) | Grad Norm 4.7469(7.7846) | Total Time 10.00(10.00)
Iter 6030 | Time 4.3120(4.3786) | Bit/dim 1.0785(1.1099) | Steps 550(554.45) | Grad Norm 8.1935(7.9661) | Total Time 10.00(10.00)
Iter 6040 | Time 4.3574(4.3931) | Bit/dim 1.1008(1.1042) | Steps 556(555.80) | Grad Norm 4.8948(7.3504) | Total Time 10.00(10.00)
Iter 6050 | Time 4.5169(4.4010) | Bit/dim 1.1122(1.0982) | Steps 562(556.97) | Grad Norm 2.5077(6.1627) | Total Time 10.00(10.00)
Iter 6060 | Time 4.5255(4.3963) | Bit/dim 1.1126(1.0949) | Steps 562(557.80) | Grad Norm 3.0025(5.1288) | Total Time 10.00(10.00)
Iter 6070 | Time 4.4765(4.3894) | Bit/dim 1.0865(1.0950) | Steps 556(557.48) | Grad Norm 3.3903(4.2576) | Total Time 10.00(10.00)
Iter 6080 | Time 4.3522(4.3959) | Bit/dim 1.1036(1.0941) | Steps 556(557.89) | Grad Norm 2.4572(4.0997) | Total Time 10.00(10.00)
Iter 6090 | Time 4.0990(4.3866) | Bit/dim 1.1548(1.0955) | Steps 538(557.30) | Grad Norm 13.9673(5.4578) | Total Time 10.00(10.00)
Iter 6100 | Time 4.3000(4.3661) | Bit/dim 1.1980(1.1249) | Steps 544(555.39) | Grad Norm 7.6455(6.9676) | Total Time 10.00(10.00)
Iter 6110 | Time 4.4961(4.3491) | Bit/dim 1.0731(1.1244) | Steps 556(552.65) | Grad Norm 12.1971(6.9507) | Total Time 10.00(10.00)
Iter 6120 | Time 5.2622(4.3384) | Bit/dim 1.1973(1.1280) | Steps 610(550.71) | Grad Norm 37.5090(8.4934) | Total Time 10.00(10.00)
Iter 6130 | Time 4.3024(4.2820) | Bit/dim 1.1022(1.1373) | Steps 544(544.85) | Grad Norm 9.4186(8.4122) | Total Time 10.00(10.00)
Iter 6140 | Time 3.9584(4.2727) | Bit/dim 1.1150(1.1328) | Steps 526(543.53) | Grad Norm 6.2785(7.7666) | Total Time 10.00(10.00)
Iter 6150 | Time 4.0236(4.2535) | Bit/dim 1.1114(1.1256) | Steps 526(541.65) | Grad Norm 10.5682(8.2978) | Total Time 10.00(10.00)
Iter 6160 | Time 4.5242(4.2695) | Bit/dim 1.0955(1.1212) | Steps 562(544.59) | Grad Norm 9.4032(7.9614) | Total Time 10.00(10.00)
Iter 6170 | Time 4.1998(4.2928) | Bit/dim 1.1055(1.1163) | Steps 550(547.59) | Grad Norm 4.4814(7.6978) | Total Time 10.00(10.00)
Iter 6180 | Time 4.1989(4.3084) | Bit/dim 1.1069(1.1121) | Steps 550(549.92) | Grad Norm 9.8205(8.0407) | Total Time 10.00(10.00)
Iter 6190 | Time 4.3289(4.3384) | Bit/dim 1.0659(1.1105) | Steps 556(551.97) | Grad Norm 9.3134(8.0156) | Total Time 10.00(10.00)
Iter 6200 | Time 4.3188(4.3551) | Bit/dim 1.0943(1.1042) | Steps 550(553.12) | Grad Norm 5.5734(7.6660) | Total Time 10.00(10.00)
Iter 6210 | Time 3.8940(4.3421) | Bit/dim 1.1490(1.1049) | Steps 508(551.36) | Grad Norm 12.4855(8.1197) | Total Time 10.00(10.00)
Iter 6220 | Time 3.9363(4.3340) | Bit/dim 1.1276(1.1129) | Steps 514(550.67) | Grad Norm 8.8473(9.0928) | Total Time 10.00(10.00)
Iter 6230 | Time 4.8309(4.3320) | Bit/dim 1.1758(1.1148) | Steps 586(549.51) | Grad Norm 29.2647(9.7868) | Total Time 10.00(10.00)
Iter 6240 | Time 4.7454(4.3537) | Bit/dim 1.1626(1.1237) | Steps 574(550.92) | Grad Norm 17.7834(10.1817) | Total Time 10.00(10.00)
Iter 6250 | Time 4.4477(4.3491) | Bit/dim 1.1233(1.1187) | Steps 544(548.71) | Grad Norm 2.4638(9.4114) | Total Time 10.00(10.00)
Iter 6260 | Time 4.2868(4.3244) | Bit/dim 1.0862(1.1107) | Steps 538(546.44) | Grad Norm 7.6963(8.3467) | Total Time 10.00(10.00)
Iter 6270 | Time 4.3556(4.3411) | Bit/dim 1.0970(1.1070) | Steps 556(548.95) | Grad Norm 6.8644(7.1802) | Total Time 10.00(10.00)
Iter 6280 | Time 4.3135(4.3524) | Bit/dim 1.0790(1.1032) | Steps 550(550.81) | Grad Norm 7.7523(7.0139) | Total Time 10.00(10.00)
Iter 6290 | Time 4.3507(4.3472) | Bit/dim 1.1204(1.1025) | Steps 556(551.38) | Grad Norm 7.9906(7.8718) | Total Time 10.00(10.00)
validating...
Epoch 0021 | Time 57.9342, Bit/dim 1.0880
===> Using batch size 200. Total 300 iterations/epoch.
Iter 6300 | Time 4.1914(4.3457) | Bit/dim 1.0757(1.0992) | Steps 550(551.96) | Grad Norm 6.0850(7.5192) | Total Time 10.00(10.00)
Iter 6310 | Time 4.5141(4.3512) | Bit/dim 1.0813(1.0986) | Steps 562(552.74) | Grad Norm 11.2772(8.0525) | Total Time 10.00(10.00)
Iter 6320 | Time 4.3667(4.3568) | Bit/dim 1.0993(1.0984) | Steps 556(553.03) | Grad Norm 7.9002(8.5619) | Total Time 10.00(10.00)
Iter 6330 | Time 4.3650(4.3620) | Bit/dim 1.1029(1.0955) | Steps 556(553.83) | Grad Norm 3.6992(7.4180) | Total Time 10.00(10.00)
Iter 6340 | Time 4.3774(4.3715) | Bit/dim 1.0768(1.0929) | Steps 562(555.06) | Grad Norm 1.7764(6.2477) | Total Time 10.00(10.00)
Iter 6350 | Time 4.3581(4.3808) | Bit/dim 1.1028(1.0917) | Steps 556(556.70) | Grad Norm 4.0706(5.4807) | Total Time 10.00(10.00)
Iter 6360 | Time 4.3742(4.3840) | Bit/dim 1.1193(1.0927) | Steps 562(556.99) | Grad Norm 1.4430(4.8270) | Total Time 10.00(10.00)
Iter 6370 | Time 4.7118(4.3982) | Bit/dim 1.0956(1.0914) | Steps 574(558.56) | Grad Norm 6.2881(4.7193) | Total Time 10.00(10.00)
Iter 6380 | Time 4.6878(4.4111) | Bit/dim 1.0847(1.0905) | Steps 568(559.45) | Grad Norm 5.7772(5.1988) | Total Time 10.00(10.00)
Iter 6390 | Time 3.8990(4.4061) | Bit/dim 1.2372(1.0953) | Steps 508(557.98) | Grad Norm 15.8273(6.9041) | Total Time 10.00(10.00)
Iter 6400 | Time 4.7812(4.3925) | Bit/dim 1.2137(1.1290) | Steps 586(556.31) | Grad Norm 7.6233(7.8764) | Total Time 10.00(10.00)
Iter 6410 | Time 4.5565(4.3704) | Bit/dim 1.1204(1.1309) | Steps 568(555.30) | Grad Norm 12.2822(7.2325) | Total Time 10.00(10.00)
Iter 6420 | Time 3.9610(4.3147) | Bit/dim 1.1395(1.1350) | Steps 520(549.30) | Grad Norm 9.2314(8.4660) | Total Time 10.00(10.00)
Iter 6430 | Time 4.5010(4.3167) | Bit/dim 1.1027(1.1307) | Steps 556(550.29) | Grad Norm 17.6995(8.9082) | Total Time 10.00(10.00)
Iter 6440 | Time 4.0745(4.3071) | Bit/dim 1.1782(1.1301) | Steps 538(550.25) | Grad Norm 11.5679(9.3480) | Total Time 10.00(10.00)
Iter 6450 | Time 4.4108(4.3110) | Bit/dim 1.0971(1.1248) | Steps 568(551.48) | Grad Norm 2.0914(8.9630) | Total Time 10.00(10.00)
Iter 6460 | Time 4.2195(4.3130) | Bit/dim 1.1054(1.1202) | Steps 556(552.36) | Grad Norm 3.6251(9.3223) | Total Time 10.00(10.00)
Iter 6470 | Time 4.0381(4.2926) | Bit/dim 1.0959(1.1169) | Steps 544(551.41) | Grad Norm 5.2793(9.1231) | Total Time 10.00(10.00)
Iter 6480 | Time 4.3463(4.3119) | Bit/dim 1.0733(1.1115) | Steps 556(553.09) | Grad Norm 7.4339(8.8316) | Total Time 10.00(10.00)
Iter 6490 | Time 4.3699(4.3190) | Bit/dim 1.0665(1.1063) | Steps 562(554.66) | Grad Norm 6.4743(8.6186) | Total Time 10.00(10.00)
Iter 6500 | Time 4.3556(4.3157) | Bit/dim 1.0712(1.1020) | Steps 556(555.17) | Grad Norm 8.0971(8.3824) | Total Time 10.00(10.00)
Iter 6510 | Time 4.2411(4.3027) | Bit/dim 1.1262(1.1066) | Steps 544(553.31) | Grad Norm 8.9167(9.1640) | Total Time 10.00(10.00)
Iter 6520 | Time 4.1931(4.3048) | Bit/dim 1.1467(1.1180) | Steps 532(552.62) | Grad Norm 8.6851(9.7090) | Total Time 10.00(10.00)
Iter 6530 | Time 3.9684(4.2995) | Bit/dim 1.0860(1.1159) | Steps 520(550.36) | Grad Norm 7.3723(9.2348) | Total Time 10.00(10.00)
Iter 6540 | Time 4.3859(4.3253) | Bit/dim 1.0691(1.1103) | Steps 562(552.98) | Grad Norm 10.1018(8.7704) | Total Time 10.00(10.00)
Iter 6550 | Time 4.5593(4.3324) | Bit/dim 1.1187(1.1081) | Steps 574(556.15) | Grad Norm 1.6467(7.9967) | Total Time 10.00(10.00)
Iter 6560 | Time 4.5361(4.3272) | Bit/dim 1.0864(1.1023) | Steps 568(556.31) | Grad Norm 2.1092(7.0858) | Total Time 10.00(10.00)
Iter 6570 | Time 4.5352(4.3412) | Bit/dim 1.0940(1.0981) | Steps 568(557.00) | Grad Norm 13.7916(7.4640) | Total Time 10.00(10.00)
Iter 6580 | Time 4.3789(4.3553) | Bit/dim 1.1136(1.0987) | Steps 562(558.28) | Grad Norm 1.9719(6.8998) | Total Time 10.00(10.00)
Iter 6590 | Time 4.3215(4.3645) | Bit/dim 1.0786(1.0941) | Steps 556(558.71) | Grad Norm 2.2381(6.1491) | Total Time 10.00(10.00)
validating...
Epoch 0022 | Time 45.4487, Bit/dim 1.2049
===> Using batch size 200. Total 300 iterations/epoch.
Iter 6600 | Time 3.9175(4.3496) | Bit/dim 1.2130(1.1023) | Steps 514(555.95) | Grad Norm 10.7016(8.2612) | Total Time 10.00(10.00)
Iter 6610 | Time 4.3372(4.3668) | Bit/dim 1.1713(1.1139) | Steps 550(555.45) | Grad Norm 6.9532(8.3849) | Total Time 10.00(10.00)
Iter 6620 | Time 4.4609(4.3454) | Bit/dim 1.0820(1.1137) | Steps 550(551.67) | Grad Norm 2.9570(8.4817) | Total Time 10.00(10.00)
Iter 6630 | Time 4.5551(4.3371) | Bit/dim 1.0762(1.1136) | Steps 574(552.08) | Grad Norm 2.1708(8.8820) | Total Time 10.00(10.00)
Iter 6640 | Time 4.3883(4.3357) | Bit/dim 1.0892(1.1068) | Steps 568(554.85) | Grad Norm 3.5017(7.8752) | Total Time 10.00(10.00)
Iter 6650 | Time 4.3910(4.3248) | Bit/dim 1.0759(1.1027) | Steps 568(556.43) | Grad Norm 5.1750(7.3175) | Total Time 10.00(10.00)
Iter 6660 | Time 4.3221(4.3440) | Bit/dim 1.0740(1.0974) | Steps 556(557.70) | Grad Norm 4.4063(6.1264) | Total Time 10.00(10.00)
Iter 6670 | Time 4.1722(4.3518) | Bit/dim 1.0883(1.0973) | Steps 556(558.64) | Grad Norm 7.0757(5.6187) | Total Time 10.00(10.00)
Iter 6680 | Time 4.0310(4.3286) | Bit/dim 1.1317(1.0980) | Steps 532(555.99) | Grad Norm 12.6978(7.1610) | Total Time 10.00(10.00)
Iter 6690 | Time 4.0798(4.3310) | Bit/dim 1.1292(1.1071) | Steps 538(556.39) | Grad Norm 9.8187(8.4060) | Total Time 10.00(10.00)
Iter 6700 | Time 4.0643(4.3204) | Bit/dim 1.1664(1.1123) | Steps 538(555.54) | Grad Norm 9.5962(8.8982) | Total Time 10.00(10.00)
Iter 6710 | Time 4.3423(4.3264) | Bit/dim 1.0639(1.1114) | Steps 556(556.56) | Grad Norm 5.5468(8.3938) | Total Time 10.00(10.00)
Iter 6720 | Time 4.5265(4.3149) | Bit/dim 1.0652(1.1061) | Steps 568(556.39) | Grad Norm 2.5830(7.6623) | Total Time 10.00(10.00)
Iter 6730 | Time 4.3863(4.3358) | Bit/dim 1.0660(1.1018) | Steps 562(559.04) | Grad Norm 0.9575(6.5150) | Total Time 10.00(10.00)
Iter 6740 | Time 4.3857(4.3543) | Bit/dim 1.0853(1.0981) | Steps 562(560.38) | Grad Norm 1.1207(5.1764) | Total Time 10.00(10.00)
Iter 6750 | Time 4.3695(4.3561) | Bit/dim 1.0756(1.0962) | Steps 562(560.49) | Grad Norm 0.7849(4.1959) | Total Time 10.00(10.00)
Iter 6760 | Time 4.3278(4.3551) | Bit/dim 1.0975(1.0913) | Steps 556(560.27) | Grad Norm 1.0292(3.4840) | Total Time 10.00(10.00)
Iter 6770 | Time 4.3689(4.3582) | Bit/dim 1.0793(1.0869) | Steps 562(560.72) | Grad Norm 0.7693(2.7924) | Total Time 10.00(10.00)
Iter 6780 | Time 4.5294(4.3617) | Bit/dim 1.1216(1.0883) | Steps 568(560.59) | Grad Norm 5.1594(2.6490) | Total Time 10.00(10.00)
Iter 6790 | Time 4.5306(4.3675) | Bit/dim 1.1009(1.0861) | Steps 568(560.07) | Grad Norm 23.4500(4.0969) | Total Time 10.00(10.00)
Iter 6800 | Time 4.1840(4.3409) | Bit/dim 1.0700(1.1050) | Steps 556(557.57) | Grad Norm 4.1963(6.6131) | Total Time 10.00(10.00)
Iter 6810 | Time 4.3479(4.3435) | Bit/dim 1.0934(1.1126) | Steps 550(555.92) | Grad Norm 5.2733(7.3771) | Total Time 10.00(10.00)
Iter 6820 | Time 4.2439(4.3394) | Bit/dim 1.1121(1.1101) | Steps 562(555.42) | Grad Norm 6.1533(7.3599) | Total Time 10.00(10.00)
Iter 6830 | Time 4.5602(4.3677) | Bit/dim 1.0963(1.1022) | Steps 568(557.16) | Grad Norm 3.3440(6.5968) | Total Time 10.00(10.00)
Iter 6840 | Time 4.5021(4.3861) | Bit/dim 1.0735(1.0962) | Steps 562(558.74) | Grad Norm 2.0331(5.6180) | Total Time 10.00(10.00)
Iter 6850 | Time 4.4918(4.3645) | Bit/dim 1.0723(1.0934) | Steps 562(558.27) | Grad Norm 12.5106(6.5797) | Total Time 10.00(10.00)
Iter 6860 | Time 4.9174(4.3469) | Bit/dim 1.1238(1.0952) | Steps 604(558.42) | Grad Norm 30.0829(8.0652) | Total Time 10.00(10.00)
Iter 6870 | Time 4.7785(4.3315) | Bit/dim 1.1595(1.1105) | Steps 586(557.27) | Grad Norm 15.2840(8.9653) | Total Time 10.00(10.00)
Iter 6880 | Time 4.3492(4.3270) | Bit/dim 1.0813(1.1105) | Steps 556(555.35) | Grad Norm 4.7243(8.7968) | Total Time 10.00(10.00)
Iter 6890 | Time 4.2204(4.3280) | Bit/dim 1.0751(1.1079) | Steps 556(555.66) | Grad Norm 5.5268(8.5847) | Total Time 10.00(10.00)
validating...
Epoch 0023 | Time 57.6286, Bit/dim 1.0796
===> Using batch size 200. Total 300 iterations/epoch.
Iter 6900 | Time 4.1852(4.3173) | Bit/dim 1.0914(1.1041) | Steps 550(555.24) | Grad Norm 2.1126(8.2331) | Total Time 10.00(10.00)
Iter 6910 | Time 4.2339(4.3248) | Bit/dim 1.0657(1.1039) | Steps 562(556.74) | Grad Norm 4.8488(8.4562) | Total Time 10.00(10.00)
Iter 6920 | Time 4.5502(4.3346) | Bit/dim 1.0629(1.1000) | Steps 574(558.37) | Grad Norm 11.8641(8.1669) | Total Time 10.00(10.00)
Iter 6930 | Time 4.3511(4.3194) | Bit/dim 1.0952(1.0947) | Steps 556(557.80) | Grad Norm 7.9166(7.6055) | Total Time 10.00(10.00)
Iter 6940 | Time 4.5372(4.3310) | Bit/dim 1.1036(1.0913) | Steps 568(558.60) | Grad Norm 8.0654(6.6523) | Total Time 10.00(10.00)
Iter 6950 | Time 4.5414(4.3276) | Bit/dim 1.0881(1.0903) | Steps 568(558.24) | Grad Norm 22.2445(7.7973) | Total Time 10.00(10.00)
Iter 6960 | Time 4.3413(4.3201) | Bit/dim 1.1935(1.1006) | Steps 562(557.58) | Grad Norm 9.2943(8.6564) | Total Time 10.00(10.00)
Iter 6970 | Time 3.9588(4.3344) | Bit/dim 1.1198(1.1070) | Steps 520(557.50) | Grad Norm 9.5282(9.1835) | Total Time 10.00(10.00)
Iter 6980 | Time 4.3956(4.3278) | Bit/dim 1.0865(1.1060) | Steps 568(557.86) | Grad Norm 2.9879(8.6854) | Total Time 10.00(10.00)
Iter 6990 | Time 4.0642(4.3334) | Bit/dim 1.1099(1.1055) | Steps 538(557.97) | Grad Norm 7.7430(8.6121) | Total Time 10.00(10.00)
Iter 7000 | Time 4.0803(4.3077) | Bit/dim 1.0832(1.1044) | Steps 544(556.68) | Grad Norm 8.8710(8.8565) | Total Time 10.00(10.00)
Iter 7010 | Time 4.5256(4.3185) | Bit/dim 1.1132(1.1003) | Steps 568(558.26) | Grad Norm 14.7363(8.8873) | Total Time 10.00(10.00)
Iter 7020 | Time 4.0536(4.3181) | Bit/dim 1.0504(1.0958) | Steps 538(558.05) | Grad Norm 8.3962(8.5628) | Total Time 10.00(10.00)
Iter 7030 | Time 4.5639(4.3261) | Bit/dim 1.0836(1.0954) | Steps 574(557.77) | Grad Norm 17.4059(9.0729) | Total Time 10.00(10.00)
Iter 7040 | Time 4.1842(4.3301) | Bit/dim 1.0745(1.0937) | Steps 556(557.92) | Grad Norm 5.3193(8.1802) | Total Time 10.00(10.00)
Iter 7050 | Time 4.1893(4.3264) | Bit/dim 1.0712(1.0896) | Steps 556(558.35) | Grad Norm 3.8460(7.0453) | Total Time 10.00(10.00)
Iter 7060 | Time 4.5063(4.3382) | Bit/dim 1.0681(1.0890) | Steps 568(558.56) | Grad Norm 3.4541(6.3282) | Total Time 10.00(10.00)
Iter 7070 | Time 4.3715(4.3434) | Bit/dim 1.1430(1.0912) | Steps 568(559.84) | Grad Norm 9.8954(7.2708) | Total Time 10.00(10.00)
Iter 7080 | Time 4.0886(4.3410) | Bit/dim 1.1295(1.1014) | Steps 544(558.86) | Grad Norm 8.0896(8.4772) | Total Time 10.00(10.00)
Iter 7090 | Time 4.3545(4.3619) | Bit/dim 1.0919(1.1052) | Steps 556(560.14) | Grad Norm 5.2438(8.3777) | Total Time 10.00(10.00)
Iter 7100 | Time 4.1733(4.3539) | Bit/dim 1.0638(1.1010) | Steps 550(559.80) | Grad Norm 5.5809(7.8727) | Total Time 10.00(10.00)
Iter 7110 | Time 4.4130(4.3515) | Bit/dim 1.1124(1.0985) | Steps 568(560.59) | Grad Norm 1.3493(7.0601) | Total Time 10.00(10.00)
Iter 7120 | Time 4.5033(4.3672) | Bit/dim 1.0946(1.0924) | Steps 568(561.98) | Grad Norm 3.5545(6.0810) | Total Time 10.00(10.00)
Iter 7130 | Time 4.3729(4.3656) | Bit/dim 1.0774(1.0896) | Steps 568(563.55) | Grad Norm 1.5660(5.6206) | Total Time 10.00(10.00)
Iter 7140 | Time 4.4804(4.3601) | Bit/dim 1.0553(1.0882) | Steps 568(563.28) | Grad Norm 4.4535(6.8100) | Total Time 10.00(10.00)
Iter 7150 | Time 4.3246(4.3504) | Bit/dim 1.0974(1.0896) | Steps 556(562.31) | Grad Norm 9.3313(7.6381) | Total Time 10.00(10.00)
Iter 7160 | Time 4.2116(4.3302) | Bit/dim 1.0615(1.0960) | Steps 562(560.22) | Grad Norm 2.4759(8.5194) | Total Time 10.00(10.00)
Iter 7170 | Time 4.5256(4.3496) | Bit/dim 1.0906(1.0970) | Steps 568(560.70) | Grad Norm 8.8158(8.7191) | Total Time 10.00(10.00)
Iter 7180 | Time 4.1825(4.3432) | Bit/dim 1.0853(1.0944) | Steps 550(559.93) | Grad Norm 4.7313(7.8375) | Total Time 10.00(10.00)
Iter 7190 | Time 4.5512(4.3463) | Bit/dim 1.0614(1.0924) | Steps 574(561.14) | Grad Norm 3.6932(6.7245) | Total Time 10.00(10.00)
validating...
Epoch 0024 | Time 57.2175, Bit/dim 1.0707
===> Using batch size 200. Total 300 iterations/epoch.
Iter 7200 | Time 4.3448(4.3590) | Bit/dim 1.0924(1.0874) | Steps 562(562.46) | Grad Norm 1.8312(5.5286) | Total Time 10.00(10.00)
Iter 7210 | Time 4.3437(4.3584) | Bit/dim 1.0939(1.0846) | Steps 562(562.95) | Grad Norm 1.9164(4.4348) | Total Time 10.00(10.00)
Iter 7220 | Time 4.4084(4.3577) | Bit/dim 1.0500(1.0805) | Steps 574(563.18) | Grad Norm 0.7643(3.6290) | Total Time 10.00(10.00)
Iter 7230 | Time 4.3380(4.3580) | Bit/dim 1.0574(1.0781) | Steps 556(563.18) | Grad Norm 1.5833(3.2518) | Total Time 10.00(10.00)
Iter 7240 | Time 3.9348(4.3525) | Bit/dim 1.0473(1.0762) | Steps 538(562.44) | Grad Norm 7.1363(3.6936) | Total Time 10.00(10.00)
Iter 7250 | Time 4.1865(4.3612) | Bit/dim 1.0556(1.0760) | Steps 556(563.07) | Grad Norm 6.4892(3.9088) | Total Time 10.00(10.00)
Iter 7260 | Time 4.3443(4.3775) | Bit/dim 1.1107(1.0779) | Steps 562(563.85) | Grad Norm 2.5463(3.8394) | Total Time 10.00(10.00)
Iter 7270 | Time 4.0487(4.3716) | Bit/dim 1.1169(1.0781) | Steps 538(563.59) | Grad Norm 14.4520(4.9273) | Total Time 10.00(10.00)
Iter 7280 | Time 4.2518(4.3734) | Bit/dim 1.2190(1.1009) | Steps 550(563.49) | Grad Norm 7.3519(7.3653) | Total Time 10.00(10.00)
Iter 7290 | Time 4.4070(4.3658) | Bit/dim 1.1317(1.1115) | Steps 562(562.39) | Grad Norm 9.0357(7.3861) | Total Time 10.00(10.00)
Iter 7300 | Time 4.2447(4.3317) | Bit/dim 1.1072(1.1084) | Steps 544(559.23) | Grad Norm 10.4173(6.8178) | Total Time 10.00(10.00)
Iter 7310 | Time 4.1211(4.3001) | Bit/dim 1.1373(1.1085) | Steps 550(556.41) | Grad Norm 7.2633(7.7453) | Total Time 10.00(10.00)
Iter 7320 | Time 4.0859(4.2939) | Bit/dim 1.0996(1.1137) | Steps 544(556.59) | Grad Norm 5.4947(8.1774) | Total Time 10.00(10.00)
Iter 7330 | Time 4.1226(4.2908) | Bit/dim 1.0386(1.1062) | Steps 544(556.02) | Grad Norm 5.0716(7.5623) | Total Time 10.00(10.00)
Iter 7340 | Time 4.5770(4.3129) | Bit/dim 1.0685(1.1011) | Steps 580(558.28) | Grad Norm 3.1208(6.3886) | Total Time 10.00(10.00)
Iter 7350 | Time 4.3852(4.3309) | Bit/dim 1.0154(1.0881) | Steps 562(559.72) | Grad Norm 1.1806(5.1238) | Total Time 10.00(10.00)
Iter 7360 | Time 4.5683(4.3592) | Bit/dim 1.0945(1.0839) | Steps 580(563.01) | Grad Norm 3.2542(4.4376) | Total Time 10.00(10.00)
Iter 7370 | Time 4.3843(4.3641) | Bit/dim 1.0524(1.0820) | Steps 568(564.32) | Grad Norm 1.7206(3.7581) | Total Time 10.00(10.00)
Iter 7380 | Time 4.4249(4.3695) | Bit/dim 1.0493(1.0817) | Steps 574(565.42) | Grad Norm 3.1601(3.2773) | Total Time 10.00(10.00)
Iter 7390 | Time 4.1605(4.3726) | Bit/dim 1.0529(1.0771) | Steps 556(566.52) | Grad Norm 3.2980(3.5598) | Total Time 10.00(10.00)
Iter 7400 | Time 4.6001(4.3833) | Bit/dim 1.1144(1.0805) | Steps 580(567.48) | Grad Norm 19.5343(4.5172) | Total Time 10.00(10.00)
Iter 7410 | Time 4.2685(4.3724) | Bit/dim 1.1876(1.1101) | Steps 562(566.39) | Grad Norm 4.5221(6.3497) | Total Time 10.00(10.00)
Iter 7420 | Time 3.9974(4.3652) | Bit/dim 1.1173(1.1194) | Steps 544(565.35) | Grad Norm 5.1535(6.7798) | Total Time 10.00(10.00)
Iter 7430 | Time 4.1273(4.3366) | Bit/dim 1.1104(1.1167) | Steps 544(561.61) | Grad Norm 7.1531(7.0450) | Total Time 10.00(10.00)
Iter 7440 | Time 4.1241(4.3233) | Bit/dim 1.1271(1.1100) | Steps 550(560.97) | Grad Norm 8.7337(7.3816) | Total Time 10.00(10.00)
Iter 7450 | Time 4.0624(4.3315) | Bit/dim 1.1049(1.1110) | Steps 538(561.88) | Grad Norm 7.9318(8.0385) | Total Time 10.00(10.00)
Iter 7460 | Time 4.3687(4.3126) | Bit/dim 1.0717(1.1039) | Steps 562(560.40) | Grad Norm 7.6278(7.4845) | Total Time 10.00(10.00)
Iter 7470 | Time 4.3393(4.2791) | Bit/dim 1.0662(1.0977) | Steps 556(557.76) | Grad Norm 3.6410(6.9655) | Total Time 10.00(10.00)
Iter 7480 | Time 4.1407(4.2706) | Bit/dim 1.0941(1.0908) | Steps 556(557.31) | Grad Norm 7.9192(6.8580) | Total Time 10.00(10.00)
Iter 7490 | Time 4.8738(4.2857) | Bit/dim 1.1225(1.0953) | Steps 604(559.23) | Grad Norm 17.2625(8.1348) | Total Time 10.00(10.00)
validating...
Epoch 0025 | Time 52.2902, Bit/dim 1.0900
===> Using batch size 200. Total 300 iterations/epoch.
Iter 7500 | Time 4.1197(4.2872) | Bit/dim 1.1035(1.0951) | Steps 550(560.59) | Grad Norm 8.3906(8.3723) | Total Time 10.00(10.00)
Iter 7510 | Time 4.6434(4.3055) | Bit/dim 1.1241(1.0993) | Steps 592(562.42) | Grad Norm 17.5225(9.0779) | Total Time 10.00(10.00)
Iter 7520 | Time 4.1762(4.3093) | Bit/dim 1.0622(1.0936) | Steps 556(562.62) | Grad Norm 4.3410(7.9438) | Total Time 10.00(10.00)
Iter 7530 | Time 4.4992(4.3194) | Bit/dim 1.0597(1.0900) | Steps 568(563.61) | Grad Norm 3.5027(7.1478) | Total Time 10.00(10.00)
Iter 7540 | Time 4.3818(4.3529) | Bit/dim 1.0479(1.0870) | Steps 568(565.86) | Grad Norm 2.0849(5.8452) | Total Time 10.00(10.00)
Iter 7550 | Time 4.4603(4.3696) | Bit/dim 1.0764(1.0853) | Steps 562(566.68) | Grad Norm 4.0828(4.8882) | Total Time 10.00(10.00)
Iter 7560 | Time 4.1346(4.3700) | Bit/dim 1.0402(1.0793) | Steps 550(567.09) | Grad Norm 3.5413(4.3245) | Total Time 10.00(10.00)
Iter 7570 | Time 4.3160(4.3784) | Bit/dim 1.0612(1.0763) | Steps 562(567.51) | Grad Norm 1.5160(3.8874) | Total Time 10.00(10.00)
Iter 7580 | Time 4.3554(4.3725) | Bit/dim 1.0902(1.0739) | Steps 562(566.84) | Grad Norm 1.0047(3.7963) | Total Time 10.00(10.00)
Iter 7590 | Time 4.1806(4.3620) | Bit/dim 1.2363(1.0847) | Steps 544(566.02) | Grad Norm 9.5441(5.8652) | Total Time 10.00(10.00)
Iter 7600 | Time 4.7276(4.3851) | Bit/dim 1.1506(1.1097) | Steps 604(569.25) | Grad Norm 5.6801(6.4349) | Total Time 10.00(10.00)
Iter 7610 | Time 4.1400(4.3695) | Bit/dim 1.0675(1.1055) | Steps 550(567.62) | Grad Norm 1.7709(5.3879) | Total Time 10.00(10.00)
Iter 7620 | Time 4.1507(4.3062) | Bit/dim 1.0883(1.1015) | Steps 556(563.01) | Grad Norm 1.0411(4.4429) | Total Time 10.00(10.00)
Iter 7630 | Time 4.2000(4.2741) | Bit/dim 1.0984(1.0947) | Steps 562(561.42) | Grad Norm 0.8167(3.5598) | Total Time 10.00(10.00)
Iter 7640 | Time 4.1429(4.2537) | Bit/dim 1.0582(1.0858) | Steps 556(560.28) | Grad Norm 2.4555(3.0013) | Total Time 10.00(10.00)
Iter 7650 | Time 4.1618(4.2599) | Bit/dim 1.0948(1.0810) | Steps 556(560.68) | Grad Norm 1.5428(2.6962) | Total Time 10.00(10.00)
Iter 7660 | Time 3.9505(4.2663) | Bit/dim 1.0770(1.0763) | Steps 544(560.40) | Grad Norm 8.5519(3.1306) | Total Time 10.00(10.00)
Iter 7670 | Time 4.4156(4.2914) | Bit/dim 1.0692(1.0758) | Steps 574(562.21) | Grad Norm 1.5491(3.5800) | Total Time 10.00(10.00)
Iter 7680 | Time 5.1623(4.3294) | Bit/dim 1.1990(1.0827) | Steps 628(564.44) | Grad Norm 43.2370(5.9108) | Total Time 10.00(10.00)
Iter 7690 | Time 4.3946(4.3261) | Bit/dim 1.1697(1.1215) | Steps 568(563.87) | Grad Norm 10.0527(6.3162) | Total Time 10.00(10.00)
Iter 7700 | Time 4.6150(4.3722) | Bit/dim 1.0862(1.1223) | Steps 586(568.09) | Grad Norm 2.7499(5.5355) | Total Time 10.00(10.00)
Iter 7710 | Time 4.3422(4.3424) | Bit/dim 1.0532(1.1151) | Steps 556(565.09) | Grad Norm 2.8359(4.8078) | Total Time 10.00(10.00)
Iter 7720 | Time 4.1906(4.2966) | Bit/dim 1.0980(1.1046) | Steps 556(562.02) | Grad Norm 1.4073(3.9315) | Total Time 10.00(10.00)
Iter 7730 | Time 4.3185(4.2754) | Bit/dim 1.0494(1.0959) | Steps 562(561.26) | Grad Norm 4.4211(3.5452) | Total Time 10.00(10.00)
Iter 7740 | Time 4.3211(4.2621) | Bit/dim 1.0967(1.0890) | Steps 562(560.57) | Grad Norm 4.3364(3.3866) | Total Time 10.00(10.00)
Iter 7750 | Time 4.1603(4.2498) | Bit/dim 1.1001(1.0847) | Steps 556(560.12) | Grad Norm 3.0611(3.3582) | Total Time 10.00(10.00)
Iter 7760 | Time 4.5700(4.2728) | Bit/dim 1.0705(1.0809) | Steps 580(561.51) | Grad Norm 20.9516(4.2940) | Total Time 10.00(10.00)
Iter 7770 | Time 5.0488(4.2931) | Bit/dim 1.1762(1.1061) | Steps 628(563.77) | Grad Norm 26.4393(6.8501) | Total Time 10.00(10.00)
Iter 7780 | Time 4.8487(4.3281) | Bit/dim 1.1179(1.1101) | Steps 598(567.47) | Grad Norm 12.9028(7.2421) | Total Time 10.00(10.00)
Iter 7790 | Time 4.1224(4.3263) | Bit/dim 1.0788(1.1053) | Steps 550(566.65) | Grad Norm 6.6447(6.8980) | Total Time 10.00(10.00)
validating...
Epoch 0026 | Time 56.2026, Bit/dim 1.0666
===> Using batch size 200. Total 300 iterations/epoch.
Iter 7800 | Time 4.2455(4.3297) | Bit/dim 1.0685(1.0973) | Steps 550(566.05) | Grad Norm 1.2905(5.9589) | Total Time 10.00(10.00)
Iter 7810 | Time 4.6350(4.3100) | Bit/dim 1.1108(1.0910) | Steps 586(564.38) | Grad Norm 21.5373(6.2836) | Total Time 10.00(10.00)
Iter 7820 | Time 4.9966(4.3283) | Bit/dim 1.1742(1.1038) | Steps 622(567.28) | Grad Norm 22.8515(7.9225) | Total Time 10.00(10.00)
Iter 7830 | Time 4.4134(4.3382) | Bit/dim 1.0720(1.1046) | Steps 574(567.79) | Grad Norm 9.0162(7.5839) | Total Time 10.00(10.00)
Iter 7840 | Time 4.4337(4.3350) | Bit/dim 1.0739(1.1020) | Steps 574(567.07) | Grad Norm 5.9881(7.4063) | Total Time 10.00(10.00)
Iter 7850 | Time 4.0804(4.3119) | Bit/dim 1.0934(1.0982) | Steps 544(564.74) | Grad Norm 8.0420(7.5061) | Total Time 10.00(10.00)
Iter 7860 | Time 4.0849(4.2989) | Bit/dim 1.0969(1.0925) | Steps 544(563.53) | Grad Norm 9.2710(7.7196) | Total Time 10.00(10.00)
Iter 7870 | Time 4.2495(4.2924) | Bit/dim 1.0754(1.0871) | Steps 550(561.84) | Grad Norm 7.9561(7.8030) | Total Time 10.00(10.00)
Iter 7880 | Time 4.1164(4.2839) | Bit/dim 1.0967(1.0895) | Steps 550(560.70) | Grad Norm 6.9992(8.3795) | Total Time 10.00(10.00)
Iter 7890 | Time 4.3684(4.3091) | Bit/dim 1.0675(1.0937) | Steps 580(563.46) | Grad Norm 3.4878(8.9301) | Total Time 10.00(10.00)
Iter 7900 | Time 4.6938(4.3668) | Bit/dim 1.1113(1.0945) | Steps 592(567.02) | Grad Norm 15.8603(8.8969) | Total Time 10.00(10.00)
Iter 7910 | Time 4.3476(4.3607) | Bit/dim 1.0671(1.0882) | Steps 562(565.95) | Grad Norm 7.5652(8.5360) | Total Time 10.00(10.00)
Iter 7920 | Time 4.6018(4.3695) | Bit/dim 1.1193(1.0858) | Steps 580(565.69) | Grad Norm 15.9335(8.3450) | Total Time 10.00(10.00)
Iter 7930 | Time 4.0918(4.3563) | Bit/dim 1.0842(1.0861) | Steps 544(564.26) | Grad Norm 8.2388(8.5896) | Total Time 10.00(10.00)
Iter 7940 | Time 4.4457(4.3565) | Bit/dim 1.0505(1.0834) | Steps 574(564.95) | Grad Norm 8.5865(7.9692) | Total Time 10.00(10.00)
Iter 7950 | Time 4.3851(4.3299) | Bit/dim 1.1034(1.0816) | Steps 568(563.36) | Grad Norm 10.1270(7.4239) | Total Time 10.00(10.00)
Iter 7960 | Time 4.0942(4.3038) | Bit/dim 1.0811(1.0790) | Steps 544(560.96) | Grad Norm 5.0769(6.8351) | Total Time 10.00(10.00)
Iter 7970 | Time 3.9576(4.3121) | Bit/dim 1.1000(1.0796) | Steps 544(561.03) | Grad Norm 6.4466(7.0193) | Total Time 10.00(10.00)
Iter 7980 | Time 3.9636(4.3056) | Bit/dim 1.0761(1.0772) | Steps 544(560.01) | Grad Norm 6.8353(7.1708) | Total Time 10.00(10.00)
Iter 7990 | Time 4.0916(4.3268) | Bit/dim 1.1143(1.0820) | Steps 544(562.32) | Grad Norm 11.1317(7.9943) | Total Time 10.00(10.00)
Iter 8000 | Time 4.2577(4.3395) | Bit/dim 1.0959(1.0910) | Steps 568(565.27) | Grad Norm 5.8987(8.2903) | Total Time 10.00(10.00)
Iter 8010 | Time 4.3868(4.3488) | Bit/dim 1.1004(1.0883) | Steps 568(568.01) | Grad Norm 7.9172(7.5987) | Total Time 10.00(10.00)
Iter 8020 | Time 4.3339(4.3475) | Bit/dim 1.0385(1.0839) | Steps 562(566.67) | Grad Norm 1.0244(6.2654) | Total Time 10.00(10.00)
Iter 8030 | Time 4.3875(4.3435) | Bit/dim 1.0629(1.0812) | Steps 568(564.84) | Grad Norm 1.2808(5.1274) | Total Time 10.00(10.00)
Iter 8040 | Time 4.1126(4.3251) | Bit/dim 1.0374(1.0756) | Steps 550(562.68) | Grad Norm 2.2407(4.3355) | Total Time 10.00(10.00)
Iter 8050 | Time 4.3436(4.3261) | Bit/dim 1.0561(1.0739) | Steps 562(562.04) | Grad Norm 1.2932(3.6641) | Total Time 10.00(10.00)
Iter 8060 | Time 5.2485(4.3589) | Bit/dim 1.2001(1.0792) | Steps 634(565.95) | Grad Norm 40.2091(5.8719) | Total Time 10.00(10.00)
Iter 8070 | Time 4.1322(4.3435) | Bit/dim 1.1444(1.1040) | Steps 556(567.08) | Grad Norm 7.9761(6.5654) | Total Time 10.00(10.00)
Iter 8080 | Time 4.6463(4.3924) | Bit/dim 1.1219(1.1071) | Steps 586(571.73) | Grad Norm 1.8659(6.5035) | Total Time 10.00(10.00)
Iter 8090 | Time 4.5760(4.3864) | Bit/dim 1.0669(1.1061) | Steps 580(570.38) | Grad Norm 1.3986(7.4403) | Total Time 10.00(10.00)
validating...
Epoch 0027 | Time 53.1636, Bit/dim 1.0724
===> Using batch size 200. Total 300 iterations/epoch.
Iter 8100 | Time 3.9809(4.3551) | Bit/dim 1.0798(1.0983) | Steps 544(568.29) | Grad Norm 5.8500(7.0279) | Total Time 10.00(10.00)
Iter 8110 | Time 4.0909(4.3372) | Bit/dim 1.0749(1.0920) | Steps 544(566.42) | Grad Norm 7.0074(6.8152) | Total Time 10.00(10.00)
Iter 8120 | Time 4.4371(4.3413) | Bit/dim 1.0709(1.0849) | Steps 574(566.34) | Grad Norm 1.6548(6.0327) | Total Time 10.00(10.00)
Iter 8130 | Time 4.0267(4.3281) | Bit/dim 1.0909(1.0819) | Steps 550(565.47) | Grad Norm 7.0529(6.7065) | Total Time 10.00(10.00)
Iter 8140 | Time 4.2963(4.3409) | Bit/dim 1.0603(1.0828) | Steps 556(566.73) | Grad Norm 8.2273(7.5770) | Total Time 10.00(10.00)
Iter 8150 | Time 4.4634(4.3600) | Bit/dim 1.1130(1.0885) | Steps 562(568.06) | Grad Norm 8.2514(8.4013) | Total Time 10.00(10.00)
Iter 8160 | Time 4.3017(4.3854) | Bit/dim 1.0861(1.0908) | Steps 574(570.70) | Grad Norm 4.1676(8.5469) | Total Time 10.00(10.00)
Iter 8170 | Time 4.4722(4.4046) | Bit/dim 1.0798(1.0895) | Steps 580(573.07) | Grad Norm 1.9775(7.9655) | Total Time 10.00(10.00)
Iter 8180 | Time 4.4994(4.4050) | Bit/dim 1.0848(1.0850) | Steps 580(572.47) | Grad Norm 7.4217(6.9927) | Total Time 10.00(10.00)
Iter 8190 | Time 4.4997(4.3799) | Bit/dim 1.0914(1.0790) | Steps 562(569.32) | Grad Norm 3.1756(7.0277) | Total Time 10.00(10.00)
Iter 8200 | Time 4.3563(4.3898) | Bit/dim 1.0641(1.0743) | Steps 562(569.25) | Grad Norm 1.9542(5.7281) | Total Time 10.00(10.00)
Iter 8210 | Time 4.4045(4.3819) | Bit/dim 1.0792(1.0722) | Steps 568(568.27) | Grad Norm 0.9240(4.9644) | Total Time 10.00(10.00)
Iter 8220 | Time 4.1962(4.3685) | Bit/dim 1.0431(1.0725) | Steps 556(566.68) | Grad Norm 2.7488(4.3508) | Total Time 10.00(10.00)
Iter 8230 | Time 4.3400(4.3821) | Bit/dim 1.0761(1.0702) | Steps 562(566.96) | Grad Norm 4.9560(4.0468) | Total Time 10.00(10.00)
Iter 8240 | Time 4.1734(4.3875) | Bit/dim 1.1088(1.0708) | Steps 556(567.54) | Grad Norm 12.7723(4.7373) | Total Time 10.00(10.00)
Iter 8250 | Time 4.1960(4.3766) | Bit/dim 1.1340(1.0865) | Steps 562(568.85) | Grad Norm 8.0068(6.9698) | Total Time 10.00(10.00)
Iter 8260 | Time 4.4344(4.4087) | Bit/dim 1.0937(1.0931) | Steps 580(573.45) | Grad Norm 6.6412(7.3432) | Total Time 10.00(10.00)
Iter 8270 | Time 4.4418(4.4223) | Bit/dim 1.0939(1.0907) | Steps 574(575.19) | Grad Norm 4.6303(7.1974) | Total Time 10.00(10.00)
Iter 8280 | Time 4.4906(4.4031) | Bit/dim 1.0853(1.0853) | Steps 580(573.50) | Grad Norm 8.4720(6.5443) | Total Time 10.00(10.00)
Iter 8290 | Time 4.4868(4.3948) | Bit/dim 1.0511(1.0780) | Steps 568(572.01) | Grad Norm 3.2457(5.7374) | Total Time 10.00(10.00)
Iter 8300 | Time 4.1519(4.3804) | Bit/dim 1.0523(1.0756) | Steps 550(570.70) | Grad Norm 6.5106(5.6463) | Total Time 10.00(10.00)
Iter 8310 | Time 4.3067(4.3872) | Bit/dim 1.0977(1.0827) | Steps 556(571.50) | Grad Norm 9.1048(7.2717) | Total Time 10.00(10.00)
Iter 8320 | Time 4.1469(4.4204) | Bit/dim 1.0791(1.0866) | Steps 550(574.81) | Grad Norm 7.2015(8.0351) | Total Time 10.00(10.00)
Iter 8330 | Time 4.2061(4.4084) | Bit/dim 1.1094(1.0865) | Steps 562(573.60) | Grad Norm 7.4989(8.1658) | Total Time 10.00(10.00)
Iter 8340 | Time 4.7037(4.4411) | Bit/dim 1.0855(1.0848) | Steps 592(575.46) | Grad Norm 14.5043(7.9014) | Total Time 10.00(10.00)
Iter 8350 | Time 4.1579(4.4379) | Bit/dim 1.1087(1.0845) | Steps 550(573.97) | Grad Norm 9.0574(8.0472) | Total Time 10.00(10.00)
Iter 8360 | Time 4.5935(4.4449) | Bit/dim 1.0976(1.0822) | Steps 592(575.06) | Grad Norm 11.7190(8.0655) | Total Time 10.00(10.00)
Iter 8370 | Time 4.2942(4.4274) | Bit/dim 1.0917(1.0788) | Steps 568(575.01) | Grad Norm 2.8855(7.2077) | Total Time 10.00(10.00)
Iter 8380 | Time 4.4285(4.4176) | Bit/dim 1.0931(1.0777) | Steps 574(574.36) | Grad Norm 1.9993(6.3465) | Total Time 10.00(10.00)
Iter 8390 | Time 4.0131(4.3901) | Bit/dim 1.0739(1.0747) | Steps 550(572.49) | Grad Norm 5.7497(6.9445) | Total Time 10.00(10.00)
validating...
Epoch 0028 | Time 53.8370, Bit/dim 1.0675
===> Using batch size 200. Total 300 iterations/epoch.
Iter 8400 | Time 4.2349(4.4144) | Bit/dim 1.0768(1.0780) | Steps 562(574.70) | Grad Norm 5.4112(8.1195) | Total Time 10.00(10.00)
Iter 8410 | Time 4.2698(4.3960) | Bit/dim 1.0586(1.0769) | Steps 568(574.19) | Grad Norm 4.9282(8.0230) | Total Time 10.00(10.00)
Iter 8420 | Time 4.0127(4.3956) | Bit/dim 1.0449(1.0766) | Steps 550(574.64) | Grad Norm 6.6427(7.6337) | Total Time 10.00(10.00)
Iter 8430 | Time 4.0056(4.3617) | Bit/dim 1.0605(1.0751) | Steps 550(572.20) | Grad Norm 7.5073(7.6364) | Total Time 10.00(10.00)
Iter 8440 | Time 4.4843(4.3506) | Bit/dim 1.0441(1.0743) | Steps 580(570.85) | Grad Norm 5.5729(7.4701) | Total Time 10.00(10.00)
Iter 8450 | Time 4.5174(4.3562) | Bit/dim 1.0344(1.0720) | Steps 586(570.69) | Grad Norm 2.8871(7.4938) | Total Time 10.00(10.00)
Iter 8460 | Time 4.4857(4.3611) | Bit/dim 1.0783(1.0728) | Steps 586(571.21) | Grad Norm 10.4578(7.8382) | Total Time 10.00(10.00)
Iter 8470 | Time 4.1814(4.3624) | Bit/dim 1.1022(1.0764) | Steps 562(571.34) | Grad Norm 10.4789(8.2278) | Total Time 10.00(10.00)
Iter 8480 | Time 4.4600(4.4000) | Bit/dim 1.0949(1.0828) | Steps 562(573.94) | Grad Norm 7.7244(8.6370) | Total Time 10.00(10.00)
Iter 8490 | Time 4.3235(4.4233) | Bit/dim 1.0534(1.0812) | Steps 580(577.45) | Grad Norm 2.6097(7.8525) | Total Time 10.00(10.00)
Iter 8500 | Time 4.1775(4.4058) | Bit/dim 1.0442(1.0793) | Steps 562(576.91) | Grad Norm 4.1107(6.9868) | Total Time 10.00(10.00)
Iter 8510 | Time 4.4500(4.4148) | Bit/dim 1.0804(1.0725) | Steps 580(576.85) | Grad Norm 1.3211(5.8368) | Total Time 10.00(10.00)
Iter 8520 | Time 4.7213(4.4177) | Bit/dim 1.0742(1.0707) | Steps 598(576.06) | Grad Norm 10.4749(5.3431) | Total Time 10.00(10.00)
Iter 8530 | Time 4.2571(4.4059) | Bit/dim 1.1694(1.0823) | Steps 574(576.59) | Grad Norm 10.2098(7.1916) | Total Time 10.00(10.00)
Iter 8540 | Time 4.3973(4.4161) | Bit/dim 1.1026(1.0879) | Steps 592(578.27) | Grad Norm 6.8473(7.3692) | Total Time 10.00(10.00)
Iter 8550 | Time 4.4385(4.4143) | Bit/dim 1.0621(1.0862) | Steps 580(580.09) | Grad Norm 5.1559(6.5446) | Total Time 10.00(10.00)
Iter 8560 | Time 4.5024(4.4060) | Bit/dim 1.0745(1.0840) | Steps 586(578.70) | Grad Norm 3.1643(5.8502) | Total Time 10.00(10.00)
Iter 8570 | Time 3.9997(4.3857) | Bit/dim 1.0787(1.0786) | Steps 550(576.85) | Grad Norm 9.8220(6.3786) | Total Time 10.00(10.00)
Iter 8580 | Time 4.1548(4.4069) | Bit/dim 1.1086(1.0855) | Steps 556(578.17) | Grad Norm 8.4170(7.5165) | Total Time 10.00(10.00)
Iter 8590 | Time 4.5572(4.4212) | Bit/dim 1.0606(1.0841) | Steps 592(580.29) | Grad Norm 2.0007(7.0973) | Total Time 10.00(10.00)
Iter 8600 | Time 4.0622(4.4196) | Bit/dim 1.0535(1.0808) | Steps 562(581.56) | Grad Norm 4.6014(6.3892) | Total Time 10.00(10.00)
Iter 8610 | Time 4.3732(4.4051) | Bit/dim 1.0400(1.0767) | Steps 568(580.07) | Grad Norm 1.5795(5.6430) | Total Time 10.00(10.00)
Iter 8620 | Time 4.5095(4.3989) | Bit/dim 1.0643(1.0756) | Steps 586(578.68) | Grad Norm 4.6948(6.2755) | Total Time 10.00(10.00)
Iter 8630 | Time 4.5134(4.4017) | Bit/dim 1.0641(1.0712) | Steps 586(578.38) | Grad Norm 3.6718(5.5708) | Total Time 10.00(10.00)
Iter 8640 | Time 4.1909(4.3904) | Bit/dim 1.1124(1.0704) | Steps 562(576.96) | Grad Norm 11.9063(6.1626) | Total Time 10.00(10.00)
Iter 8650 | Time 4.4207(4.4081) | Bit/dim 1.1331(1.0787) | Steps 580(578.31) | Grad Norm 8.3001(7.5231) | Total Time 10.00(10.00)
Iter 8660 | Time 4.2229(4.4212) | Bit/dim 1.0850(1.0814) | Steps 562(578.97) | Grad Norm 5.3834(7.4919) | Total Time 10.00(10.00)
Iter 8670 | Time 4.3651(4.4303) | Bit/dim 1.0671(1.0776) | Steps 586(581.31) | Grad Norm 3.9002(6.8166) | Total Time 10.00(10.00)
Iter 8680 | Time 4.5378(4.4458) | Bit/dim 1.0546(1.0738) | Steps 574(582.82) | Grad Norm 3.3123(5.8399) | Total Time 10.00(10.00)
Iter 8690 | Time 4.5147(4.4442) | Bit/dim 1.0806(1.0701) | Steps 586(581.89) | Grad Norm 2.2526(4.9649) | Total Time 10.00(10.00)
validating...
Epoch 0029 | Time 58.8526, Bit/dim 1.0659
===> Using batch size 200. Total 300 iterations/epoch.
Iter 8700 | Time 4.6401(4.4277) | Bit/dim 1.0538(1.0673) | Steps 586(579.90) | Grad Norm 16.6444(5.1472) | Total Time 10.00(10.00)
Iter 8710 | Time 4.0262(4.4176) | Bit/dim 1.1305(1.0768) | Steps 556(579.81) | Grad Norm 8.8354(6.9348) | Total Time 10.00(10.00)
Iter 8720 | Time 4.2124(4.4404) | Bit/dim 1.0818(1.0795) | Steps 562(581.42) | Grad Norm 8.4390(7.4795) | Total Time 10.00(10.00)
Iter 8730 | Time 4.5349(4.4494) | Bit/dim 1.0483(1.0763) | Steps 586(583.31) | Grad Norm 5.7751(7.0312) | Total Time 10.00(10.00)
Iter 8740 | Time 4.4849(4.4217) | Bit/dim 1.0685(1.0768) | Steps 580(580.17) | Grad Norm 6.6513(6.3832) | Total Time 10.00(10.00)
Iter 8750 | Time 4.2158(4.4161) | Bit/dim 1.0741(1.0714) | Steps 562(578.74) | Grad Norm 7.9413(6.2180) | Total Time 10.00(10.00)
Iter 8760 | Time 4.0509(4.3961) | Bit/dim 1.0827(1.0713) | Steps 556(576.99) | Grad Norm 6.2320(6.7447) | Total Time 10.00(10.00)
Iter 8770 | Time 4.1873(4.3902) | Bit/dim 1.0940(1.0708) | Steps 562(576.27) | Grad Norm 9.8110(7.2404) | Total Time 10.00(10.00)
Iter 8780 | Time 4.3409(4.3952) | Bit/dim 1.1043(1.0801) | Steps 562(576.43) | Grad Norm 5.8288(8.0686) | Total Time 10.00(10.00)
Iter 8790 | Time 4.7099(4.4272) | Bit/dim 1.0856(1.0810) | Steps 592(579.25) | Grad Norm 12.5665(7.8948) | Total Time 10.00(10.00)
Iter 8800 | Time 4.1223(4.4128) | Bit/dim 1.0683(1.0773) | Steps 568(579.78) | Grad Norm 5.5323(7.4509) | Total Time 10.00(10.00)
Iter 8810 | Time 4.3159(4.4164) | Bit/dim 1.0524(1.0721) | Steps 574(579.64) | Grad Norm 3.5392(6.7714) | Total Time 10.00(10.00)
Iter 8820 | Time 4.2824(4.4143) | Bit/dim 1.0524(1.0676) | Steps 574(579.05) | Grad Norm 4.8470(6.1063) | Total Time 10.00(10.00)
Iter 8830 | Time 4.2153(4.3985) | Bit/dim 1.0790(1.0691) | Steps 568(578.42) | Grad Norm 7.6616(7.0058) | Total Time 10.00(10.00)
Iter 8840 | Time 4.2177(4.3950) | Bit/dim 1.0959(1.0701) | Steps 562(577.62) | Grad Norm 9.2110(7.2627) | Total Time 10.00(10.00)
Iter 8850 | Time 4.7703(4.4109) | Bit/dim 1.0805(1.0744) | Steps 604(579.19) | Grad Norm 14.0355(7.9346) | Total Time 10.00(10.00)
Iter 8860 | Time 4.5098(4.4156) | Bit/dim 1.0786(1.0717) | Steps 586(580.09) | Grad Norm 2.4571(7.3592) | Total Time 10.00(10.00)
Iter 8870 | Time 4.4699(4.4205) | Bit/dim 1.0761(1.0700) | Steps 580(580.37) | Grad Norm 1.7695(6.5984) | Total Time 10.00(10.00)
Iter 8880 | Time 4.5512(4.4117) | Bit/dim 1.0773(1.0704) | Steps 592(580.45) | Grad Norm 5.7301(7.1403) | Total Time 10.00(10.00)
Iter 8890 | Time 4.3191(4.4024) | Bit/dim 1.0882(1.0711) | Steps 562(579.65) | Grad Norm 9.2471(7.7388) | Total Time 10.00(10.00)
Iter 8900 | Time 4.3992(4.4065) | Bit/dim 1.0526(1.0754) | Steps 586(579.80) | Grad Norm 3.5273(8.3844) | Total Time 10.00(10.00)
Iter 8910 | Time 4.5508(4.4345) | Bit/dim 1.0545(1.0768) | Steps 592(582.57) | Grad Norm 5.2817(8.0788) | Total Time 10.00(10.00)
Iter 8920 | Time 4.2460(4.4548) | Bit/dim 1.0692(1.0725) | Steps 574(586.20) | Grad Norm 5.6682(7.6676) | Total Time 10.00(10.00)
Iter 8930 | Time 4.4435(4.4435) | Bit/dim 1.0582(1.0724) | Steps 592(584.32) | Grad Norm 2.8373(7.4876) | Total Time 10.00(10.00)
Iter 8940 | Time 4.3509(4.4608) | Bit/dim 1.0764(1.0702) | Steps 580(584.58) | Grad Norm 2.3219(7.5286) | Total Time 10.00(10.00)
Iter 8950 | Time 4.5413(4.4498) | Bit/dim 1.0697(1.0701) | Steps 586(583.39) | Grad Norm 9.1063(7.3020) | Total Time 10.00(10.00)
Iter 8960 | Time 4.6088(4.4440) | Bit/dim 1.0398(1.0635) | Steps 598(583.53) | Grad Norm 6.7401(6.7304) | Total Time 10.00(10.00)
Iter 8970 | Time 4.5348(4.4644) | Bit/dim 1.0756(1.0639) | Steps 592(585.45) | Grad Norm 1.6524(5.5811) | Total Time 10.00(10.00)
Iter 8980 | Time 4.5208(4.4815) | Bit/dim 1.0166(1.0609) | Steps 586(586.76) | Grad Norm 2.4429(4.5771) | Total Time 10.00(10.00)
Iter 8990 | Time 4.6706(4.4781) | Bit/dim 1.0516(1.0606) | Steps 592(585.29) | Grad Norm 7.5679(4.5037) | Total Time 10.00(10.00)
validating...
Epoch 0030 | Time 70.8902, Bit/dim 1.1416
===> Using batch size 200. Total 300 iterations/epoch.
Iter 9000 | Time 5.2880(4.4898) | Bit/dim 1.1383(1.0674) | Steps 646(586.83) | Grad Norm 33.9399(6.2979) | Total Time 10.00(10.00)
Iter 9010 | Time 4.5598(4.4878) | Bit/dim 1.1068(1.0843) | Steps 598(587.56) | Grad Norm 3.9005(6.7643) | Total Time 10.00(10.00)
Iter 9020 | Time 4.7394(4.5072) | Bit/dim 1.0578(1.0828) | Steps 604(588.95) | Grad Norm 12.2170(6.6568) | Total Time 10.00(10.00)
Iter 9030 | Time 4.2271(4.4688) | Bit/dim 1.1077(1.0835) | Steps 562(586.20) | Grad Norm 8.3521(6.9656) | Total Time 10.00(10.00)
Iter 9040 | Time 4.5367(4.4655) | Bit/dim 1.0867(1.0804) | Steps 586(584.81) | Grad Norm 9.6770(7.2163) | Total Time 10.00(10.00)
Iter 9050 | Time 4.5130(4.4587) | Bit/dim 1.0625(1.0742) | Steps 586(584.46) | Grad Norm 2.0287(6.3882) | Total Time 10.00(10.00)
Iter 9060 | Time 4.5414(4.4446) | Bit/dim 1.0403(1.0697) | Steps 586(583.06) | Grad Norm 7.7553(6.3807) | Total Time 10.00(10.00)
Iter 9070 | Time 4.0177(4.4323) | Bit/dim 1.0604(1.0681) | Steps 556(581.04) | Grad Norm 9.8230(6.5350) | Total Time 10.00(10.00)
Iter 9080 | Time 4.0425(4.4088) | Bit/dim 1.0924(1.0750) | Steps 556(580.15) | Grad Norm 5.3662(7.4650) | Total Time 10.00(10.00)
Iter 9090 | Time 4.3591(4.4375) | Bit/dim 1.0726(1.0715) | Steps 580(582.26) | Grad Norm 3.2018(7.0134) | Total Time 10.00(10.00)
Iter 9100 | Time 4.3189(4.4419) | Bit/dim 1.0537(1.0663) | Steps 562(581.12) | Grad Norm 3.0056(5.9342) | Total Time 10.00(10.00)
Iter 9110 | Time 4.4866(4.4511) | Bit/dim 1.0574(1.0645) | Steps 580(581.45) | Grad Norm 1.1633(4.7612) | Total Time 10.00(10.00)
Iter 9120 | Time 4.5134(4.4578) | Bit/dim 1.0682(1.0617) | Steps 592(582.45) | Grad Norm 2.1491(4.1412) | Total Time 10.00(10.00)
Iter 9130 | Time 4.5283(4.4549) | Bit/dim 1.0580(1.0600) | Steps 592(583.01) | Grad Norm 1.1511(3.9835) | Total Time 10.00(10.00)
Iter 9140 | Time 4.7317(4.4688) | Bit/dim 1.0616(1.0582) | Steps 604(584.34) | Grad Norm 15.6809(4.6083) | Total Time 10.00(10.00)
Iter 9150 | Time 4.2678(4.4450) | Bit/dim 1.1087(1.0683) | Steps 586(584.85) | Grad Norm 7.5286(6.6644) | Total Time 10.00(10.00)
Iter 9160 | Time 4.9219(4.4628) | Bit/dim 1.0906(1.0722) | Steps 604(585.44) | Grad Norm 17.8625(7.3962) | Total Time 10.00(10.00)
Iter 9170 | Time 4.2928(4.4542) | Bit/dim 1.0893(1.0721) | Steps 574(584.54) | Grad Norm 6.1635(7.3445) | Total Time 10.00(10.00)
Iter 9180 | Time 4.3053(4.4375) | Bit/dim 1.0854(1.0687) | Steps 580(582.85) | Grad Norm 4.4619(6.9041) | Total Time 10.00(10.00)
Iter 9190 | Time 4.4724(4.4410) | Bit/dim 1.0436(1.0644) | Steps 580(582.60) | Grad Norm 1.7455(5.9729) | Total Time 10.00(10.00)
Iter 9200 | Time 4.6680(4.4501) | Bit/dim 1.0704(1.0602) | Steps 592(582.16) | Grad Norm 15.4727(5.8709) | Total Time 10.00(10.00)
Iter 9210 | Time 4.1848(4.4293) | Bit/dim 1.1516(1.0733) | Steps 568(582.04) | Grad Norm 9.5919(7.4080) | Total Time 10.00(10.00)
Iter 9220 | Time 4.3127(4.4660) | Bit/dim 1.0852(1.0784) | Steps 580(586.77) | Grad Norm 5.5897(7.4715) | Total Time 10.00(10.00)
Iter 9230 | Time 4.6100(4.4631) | Bit/dim 1.0504(1.0739) | Steps 604(587.13) | Grad Norm 6.2407(7.0592) | Total Time 10.00(10.00)
Iter 9240 | Time 4.4661(4.4436) | Bit/dim 1.0349(1.0688) | Steps 586(584.95) | Grad Norm 1.5789(6.4637) | Total Time 10.00(10.00)
Iter 9250 | Time 4.7026(4.4352) | Bit/dim 1.0917(1.0683) | Steps 598(583.04) | Grad Norm 14.7853(7.0364) | Total Time 10.00(10.00)
Iter 9260 | Time 4.6212(4.4184) | Bit/dim 1.0637(1.0656) | Steps 580(580.16) | Grad Norm 17.9248(7.1530) | Total Time 10.00(10.00)
Iter 9270 | Time 3.9993(4.4085) | Bit/dim 1.1402(1.0673) | Steps 556(580.42) | Grad Norm 9.9301(8.2123) | Total Time 10.00(10.00)
Iter 9280 | Time 4.4322(4.4622) | Bit/dim 1.0490(1.0718) | Steps 598(585.64) | Grad Norm 3.3347(8.0935) | Total Time 10.00(10.00)
Iter 9290 | Time 4.3886(4.4714) | Bit/dim 1.0643(1.0694) | Steps 604(588.64) | Grad Norm 6.3604(7.5580) | Total Time 10.00(10.00)
validating...
Epoch 0031 | Time 60.9250, Bit/dim 1.0520
===> Using batch size 200. Total 300 iterations/epoch.
Iter 9300 | Time 4.5210(4.4629) | Bit/dim 1.0265(1.0667) | Steps 592(588.16) | Grad Norm 5.2096(7.0395) | Total Time 10.00(10.00)
Iter 9310 | Time 4.5940(4.4548) | Bit/dim 1.0599(1.0657) | Steps 580(586.77) | Grad Norm 9.0874(6.3869) | Total Time 10.00(10.00)
Iter 9320 | Time 4.4977(4.4286) | Bit/dim 1.0497(1.0652) | Steps 586(584.84) | Grad Norm 5.0223(6.9573) | Total Time 10.00(10.00)
Iter 9330 | Time 4.8677(4.4532) | Bit/dim 1.0565(1.0680) | Steps 604(585.84) | Grad Norm 10.0447(7.7498) | Total Time 10.00(10.00)
Iter 9340 | Time 4.3374(4.4499) | Bit/dim 1.0505(1.0671) | Steps 592(585.91) | Grad Norm 5.8891(7.3705) | Total Time 10.00(10.00)
Iter 9350 | Time 4.5279(4.4688) | Bit/dim 1.0452(1.0638) | Steps 598(588.86) | Grad Norm 1.1699(6.3094) | Total Time 10.00(10.00)
Iter 9360 | Time 4.2389(4.4677) | Bit/dim 1.0568(1.0602) | Steps 574(589.39) | Grad Norm 7.0375(5.7714) | Total Time 10.00(10.00)
Iter 9370 | Time 4.9780(4.4720) | Bit/dim 1.0884(1.0673) | Steps 622(590.33) | Grad Norm 12.8601(6.9745) | Total Time 10.00(10.00)
Iter 9380 | Time 4.3857(4.4540) | Bit/dim 1.0628(1.0656) | Steps 598(590.25) | Grad Norm 5.9328(6.8344) | Total Time 10.00(10.00)
Iter 9390 | Time 4.3496(4.4511) | Bit/dim 1.0556(1.0647) | Steps 586(589.97) | Grad Norm 4.4335(6.6485) | Total Time 10.00(10.00)
Iter 9400 | Time 4.5626(4.4709) | Bit/dim 1.0674(1.0620) | Steps 598(590.09) | Grad Norm 5.0000(5.9200) | Total Time 10.00(10.00)
Iter 9410 | Time 5.2975(4.5036) | Bit/dim 1.0741(1.0606) | Steps 652(592.05) | Grad Norm 26.0681(6.8959) | Total Time 10.00(10.00)
Iter 9420 | Time 5.0101(4.5047) | Bit/dim 1.1065(1.0710) | Steps 622(592.43) | Grad Norm 11.0060(7.3524) | Total Time 10.00(10.00)
Iter 9430 | Time 4.3684(4.4972) | Bit/dim 1.0440(1.0686) | Steps 592(593.34) | Grad Norm 5.5184(7.0174) | Total Time 10.00(10.00)
Iter 9440 | Time 4.4300(4.4985) | Bit/dim 1.0764(1.0660) | Steps 598(594.81) | Grad Norm 3.6668(6.1329) | Total Time 10.00(10.00)
Iter 9450 | Time 4.7094(4.5103) | Bit/dim 1.0494(1.0642) | Steps 604(594.68) | Grad Norm 4.0912(5.7529) | Total Time 10.00(10.00)
Iter 9460 | Time 4.7072(4.4970) | Bit/dim 1.0596(1.0636) | Steps 598(593.27) | Grad Norm 4.4027(6.0170) | Total Time 10.00(10.00)
Iter 9470 | Time 4.3380(4.4813) | Bit/dim 1.0790(1.0632) | Steps 580(591.28) | Grad Norm 4.8563(6.3181) | Total Time 10.00(10.00)
Iter 9480 | Time 4.4309(4.4661) | Bit/dim 1.0639(1.0609) | Steps 604(589.72) | Grad Norm 6.8110(7.0081) | Total Time 10.00(10.00)
Iter 9490 | Time 4.3640(4.4518) | Bit/dim 1.0432(1.0618) | Steps 586(587.03) | Grad Norm 6.1340(7.0963) | Total Time 10.00(10.00)
Iter 9500 | Time 4.2595(4.4446) | Bit/dim 1.0617(1.0624) | Steps 574(585.79) | Grad Norm 7.1546(7.1897) | Total Time 10.00(10.00)
Iter 9510 | Time 4.6005(4.4659) | Bit/dim 1.0354(1.0579) | Steps 598(587.31) | Grad Norm 5.4599(6.5647) | Total Time 10.00(10.00)
Iter 9520 | Time 4.1073(4.4633) | Bit/dim 1.0729(1.0573) | Steps 574(587.60) | Grad Norm 10.8170(6.5543) | Total Time 10.00(10.00)
Iter 9530 | Time 4.2583(4.4477) | Bit/dim 1.0385(1.0590) | Steps 574(588.32) | Grad Norm 4.8097(7.4684) | Total Time 10.00(10.00)
Iter 9540 | Time 4.5235(4.4784) | Bit/dim 1.0530(1.0575) | Steps 592(591.85) | Grad Norm 1.3841(7.1827) | Total Time 10.00(10.00)
Iter 9550 | Time 4.3701(4.4853) | Bit/dim 1.0705(1.0576) | Steps 592(592.74) | Grad Norm 4.4370(6.6552) | Total Time 10.00(10.00)
Iter 9560 | Time 4.2867(4.4976) | Bit/dim 1.0752(1.0556) | Steps 586(593.61) | Grad Norm 7.8999(6.9960) | Total Time 10.00(10.00)
Iter 9570 | Time 4.3028(4.4921) | Bit/dim 1.0556(1.0545) | Steps 586(594.10) | Grad Norm 6.8252(6.9151) | Total Time 10.00(10.00)
Iter 9580 | Time 4.5505(4.5073) | Bit/dim 1.0497(1.0559) | Steps 598(595.68) | Grad Norm 7.0123(6.8546) | Total Time 10.00(10.00)
Iter 9590 | Time 4.6602(4.5321) | Bit/dim 1.0457(1.0563) | Steps 622(597.40) | Grad Norm 1.9079(7.0008) | Total Time 10.00(10.00)
validating...
Epoch 0032 | Time 59.5813, Bit/dim 1.0441
===> Using batch size 200. Total 300 iterations/epoch.
Iter 9600 | Time 4.5369(4.5219) | Bit/dim 1.0407(1.0573) | Steps 598(596.15) | Grad Norm 3.0377(6.8893) | Total Time 10.00(10.00)
Iter 9610 | Time 4.2681(4.5095) | Bit/dim 1.0648(1.0567) | Steps 574(594.58) | Grad Norm 8.4060(6.8456) | Total Time 10.00(10.00)
Iter 9620 | Time 4.4088(4.5102) | Bit/dim 1.0509(1.0567) | Steps 592(594.71) | Grad Norm 4.6493(6.5335) | Total Time 10.00(10.00)
Iter 9630 | Time 4.3942(4.5043) | Bit/dim 1.0922(1.0707) | Steps 598(595.41) | Grad Norm 4.7284(7.6759) | Total Time 10.00(10.00)
Iter 9640 | Time 4.3769(4.5287) | Bit/dim 1.0643(1.0760) | Steps 586(596.90) | Grad Norm 5.2250(7.3946) | Total Time 10.00(10.00)
Iter 9650 | Time 4.3614(4.5351) | Bit/dim 1.0622(1.0742) | Steps 580(597.42) | Grad Norm 1.6783(6.2262) | Total Time 10.00(10.00)
Iter 9660 | Time 4.4868(4.5121) | Bit/dim 1.0750(1.0679) | Steps 592(595.15) | Grad Norm 1.0159(4.9634) | Total Time 10.00(10.00)
Iter 9670 | Time 4.4977(4.5053) | Bit/dim 1.0484(1.0631) | Steps 592(594.63) | Grad Norm 3.6871(4.4638) | Total Time 10.00(10.00)
Iter 9680 | Time 4.5544(4.5051) | Bit/dim 1.0541(1.0597) | Steps 604(595.35) | Grad Norm 1.2480(3.9265) | Total Time 10.00(10.00)
Iter 9690 | Time 4.7302(4.5149) | Bit/dim 1.0769(1.0570) | Steps 610(594.63) | Grad Norm 6.5687(4.0590) | Total Time 10.00(10.00)
Iter 9700 | Time 4.2611(4.5125) | Bit/dim 1.1801(1.0627) | Steps 580(594.20) | Grad Norm 9.5189(5.5988) | Total Time 10.00(10.00)
Iter 9710 | Time 4.5851(4.5065) | Bit/dim 1.0634(1.0705) | Steps 604(595.51) | Grad Norm 2.1194(5.9479) | Total Time 10.00(10.00)
Iter 9720 | Time 4.7220(4.5134) | Bit/dim 1.0312(1.0683) | Steps 604(596.89) | Grad Norm 9.4395(6.4970) | Total Time 10.00(10.00)
Iter 9730 | Time 4.2267(4.4957) | Bit/dim 1.0838(1.0663) | Steps 568(595.19) | Grad Norm 8.8222(6.5362) | Total Time 10.00(10.00)
Iter 9740 | Time 4.5086(4.4916) | Bit/dim 1.0638(1.0641) | Steps 592(593.94) | Grad Norm 2.2845(5.8651) | Total Time 10.00(10.00)
Iter 9750 | Time 4.4820(4.4925) | Bit/dim 1.0549(1.0580) | Steps 586(592.86) | Grad Norm 1.0640(4.7796) | Total Time 10.00(10.00)
Iter 9760 | Time 4.1009(4.4852) | Bit/dim 1.0989(1.0595) | Steps 574(592.86) | Grad Norm 11.8038(5.1885) | Total Time 10.00(10.00)
Iter 9770 | Time 4.0957(4.4624) | Bit/dim 1.0908(1.0597) | Steps 574(592.19) | Grad Norm 10.8016(6.9057) | Total Time 10.00(10.00)
Iter 9780 | Time 4.5577(4.4510) | Bit/dim 1.0916(1.0658) | Steps 598(592.06) | Grad Norm 2.4891(7.0350) | Total Time 10.00(10.00)
Iter 9790 | Time 4.5410(4.4489) | Bit/dim 1.0392(1.0651) | Steps 604(592.69) | Grad Norm 3.2983(7.1421) | Total Time 10.00(10.00)
Iter 9800 | Time 5.0059(4.4691) | Bit/dim 1.0906(1.0676) | Steps 616(593.36) | Grad Norm 20.1963(8.2422) | Total Time 10.00(10.00)
Iter 9810 | Time 4.3295(4.4677) | Bit/dim 1.0370(1.0682) | Steps 586(593.88) | Grad Norm 2.8256(7.7922) | Total Time 10.00(10.00)
Iter 9820 | Time 4.5789(4.4725) | Bit/dim 1.0574(1.0651) | Steps 604(595.02) | Grad Norm 7.3009(7.1741) | Total Time 10.00(10.00)
Iter 9830 | Time 4.6995(4.4778) | Bit/dim 1.0521(1.0604) | Steps 598(594.69) | Grad Norm 12.1472(6.8364) | Total Time 10.00(10.00)
Iter 9840 | Time 4.2840(4.4633) | Bit/dim 1.0391(1.0548) | Steps 586(593.09) | Grad Norm 8.2957(6.9896) | Total Time 10.00(10.00)
Iter 9850 | Time 4.6254(4.4695) | Bit/dim 1.0302(1.0558) | Steps 598(593.34) | Grad Norm 3.1799(6.8505) | Total Time 10.00(10.00)
Iter 9860 | Time 4.5781(4.4686) | Bit/dim 1.0595(1.0550) | Steps 610(593.32) | Grad Norm 2.9549(7.1100) | Total Time 10.00(10.00)
Iter 9870 | Time 4.3870(4.4668) | Bit/dim 1.0746(1.0633) | Steps 598(593.13) | Grad Norm 4.1668(7.9742) | Total Time 10.00(10.00)
Iter 9880 | Time 4.6960(4.5044) | Bit/dim 1.0475(1.0645) | Steps 604(595.63) | Grad Norm 6.7694(7.3281) | Total Time 10.00(10.00)
Iter 9890 | Time 4.5448(4.5012) | Bit/dim 1.0752(1.0612) | Steps 604(597.04) | Grad Norm 2.0269(6.3449) | Total Time 10.00(10.00)
validating...
Epoch 0033 | Time 61.9949, Bit/dim 1.0492
===> Using batch size 200. Total 300 iterations/epoch.
Iter 9900 | Time 4.6822(4.4984) | Bit/dim 1.0664(1.0583) | Steps 598(597.24) | Grad Norm 9.7219(6.6168) | Total Time 10.00(10.00)
Iter 9910 | Time 4.2260(4.4966) | Bit/dim 1.0582(1.0577) | Steps 574(595.92) | Grad Norm 8.7714(6.7611) | Total Time 10.00(10.00)
Iter 9920 | Time 4.2272(4.4809) | Bit/dim 1.0520(1.0562) | Steps 574(594.20) | Grad Norm 6.7896(6.8239) | Total Time 10.00(10.00)
Iter 9930 | Time 4.3713(4.4730) | Bit/dim 1.0424(1.0563) | Steps 598(594.04) | Grad Norm 8.3977(7.0174) | Total Time 10.00(10.00)
Iter 9940 | Time 4.5433(4.4891) | Bit/dim 1.0268(1.0528) | Steps 604(595.14) | Grad Norm 2.4536(6.5023) | Total Time 10.00(10.00)
Iter 9950 | Time 4.2649(4.4944) | Bit/dim 1.1344(1.0571) | Steps 586(595.90) | Grad Norm 10.2071(7.6983) | Total Time 10.00(10.00)
Iter 9960 | Time 4.3167(4.4899) | Bit/dim 1.0716(1.0623) | Steps 586(595.95) | Grad Norm 5.5282(7.5815) | Total Time 10.00(10.00)
Iter 9970 | Time 4.7311(4.5017) | Bit/dim 1.0320(1.0603) | Steps 610(596.92) | Grad Norm 9.0441(7.0349) | Total Time 10.00(10.00)
Iter 9980 | Time 4.7064(4.4963) | Bit/dim 1.0686(1.0586) | Steps 604(597.08) | Grad Norm 8.4273(6.3980) | Total Time 10.00(10.00)
Iter 9990 | Time 4.5255(4.4888) | Bit/dim 1.0866(1.0580) | Steps 592(595.84) | Grad Norm 8.1736(6.7733) | Total Time 10.00(10.00)
Iter 10000 | Time 4.8508(4.4790) | Bit/dim 1.0482(1.0546) | Steps 628(595.21) | Grad Norm 10.6288(6.7914) | Total Time 10.00(10.00)
Iter 10010 | Time 4.6014(4.4746) | Bit/dim 1.0516(1.0522) | Steps 610(594.94) | Grad Norm 2.3712(6.9861) | Total Time 10.00(10.00)
Iter 10020 | Time 4.5522(4.4968) | Bit/dim 1.0378(1.0488) | Steps 604(596.52) | Grad Norm 1.5873(5.7050) | Total Time 10.00(10.00)
Iter 10030 | Time 4.5448(4.5042) | Bit/dim 1.0369(1.0460) | Steps 604(597.19) | Grad Norm 2.9994(5.0037) | Total Time 10.00(10.00)
Iter 10040 | Time 4.3000(4.5056) | Bit/dim 1.0377(1.0439) | Steps 586(596.91) | Grad Norm 9.5026(5.1044) | Total Time 10.00(10.00)
Iter 10050 | Time 4.5701(4.4978) | Bit/dim 1.0644(1.0428) | Steps 604(596.13) | Grad Norm 2.2883(4.8908) | Total Time 10.00(10.00)
Iter 10060 | Time 4.0687(4.5023) | Bit/dim 1.0966(1.0462) | Steps 568(596.60) | Grad Norm 12.5545(5.3413) | Total Time 10.00(10.00)
Iter 10070 | Time 4.1439(4.5137) | Bit/dim 1.1007(1.0645) | Steps 580(598.26) | Grad Norm 7.2570(6.8926) | Total Time 10.00(10.00)
Iter 10080 | Time 4.4053(4.5291) | Bit/dim 1.0615(1.0691) | Steps 598(599.06) | Grad Norm 5.6688(6.7577) | Total Time 10.00(10.00)
Iter 10090 | Time 4.3013(4.5308) | Bit/dim 1.0681(1.0690) | Steps 586(599.14) | Grad Norm 6.8001(6.4832) | Total Time 10.00(10.00)
Iter 10100 | Time 4.5495(4.5317) | Bit/dim 1.0650(1.0650) | Steps 604(600.13) | Grad Norm 1.6162(5.9654) | Total Time 10.00(10.00)
Iter 10110 | Time 4.8622(4.5383) | Bit/dim 1.0468(1.0635) | Steps 628(601.19) | Grad Norm 10.8739(5.6802) | Total Time 10.00(10.00)
Iter 10120 | Time 4.1423(4.5009) | Bit/dim 1.0918(1.0692) | Steps 580(598.36) | Grad Norm 6.9891(7.2334) | Total Time 10.00(10.00)
Iter 10130 | Time 4.5931(4.5255) | Bit/dim 1.0250(1.0645) | Steps 604(600.67) | Grad Norm 5.2260(7.3227) | Total Time 10.00(10.00)
Iter 10140 | Time 4.4050(4.5330) | Bit/dim 1.0676(1.0600) | Steps 604(602.67) | Grad Norm 5.2889(6.9001) | Total Time 10.00(10.00)
Iter 10150 | Time 4.6836(4.5443) | Bit/dim 1.0468(1.0578) | Steps 610(604.34) | Grad Norm 3.6198(6.5431) | Total Time 10.00(10.00)
Iter 10160 | Time 4.5873(4.5440) | Bit/dim 1.0356(1.0544) | Steps 604(604.67) | Grad Norm 5.3854(6.8818) | Total Time 10.00(10.00)
Iter 10170 | Time 4.9012(4.5350) | Bit/dim 1.0772(1.0615) | Steps 634(604.54) | Grad Norm 9.4204(7.8964) | Total Time 10.00(10.00)
Iter 10180 | Time 4.8347(4.5741) | Bit/dim 1.0866(1.0644) | Steps 622(607.73) | Grad Norm 6.0116(7.6333) | Total Time 10.00(10.00)
Iter 10190 | Time 4.4366(4.5834) | Bit/dim 1.0161(1.0619) | Steps 604(609.96) | Grad Norm 4.2364(6.7500) | Total Time 10.00(10.00)
validating...
Epoch 0034 | Time 60.2576, Bit/dim 1.0386
===> Using batch size 200. Total 300 iterations/epoch.
Iter 10200 | Time 4.5634(4.5809) | Bit/dim 1.0505(1.0588) | Steps 610(609.54) | Grad Norm 2.8592(5.6873) | Total Time 10.00(10.00)
Iter 10210 | Time 4.4942(4.5676) | Bit/dim 1.0155(1.0533) | Steps 598(608.31) | Grad Norm 0.9219(4.6088) | Total Time 10.00(10.00)
Iter 10220 | Time 4.5383(4.5594) | Bit/dim 1.0343(1.0481) | Steps 604(606.87) | Grad Norm 1.7771(3.7534) | Total Time 10.00(10.00)
Iter 10230 | Time 4.5389(4.5527) | Bit/dim 1.0352(1.0435) | Steps 604(604.96) | Grad Norm 4.0132(3.8529) | Total Time 10.00(10.00)
Iter 10240 | Time 4.5889(4.5618) | Bit/dim 1.0244(1.0430) | Steps 610(604.44) | Grad Norm 3.0069(4.0598) | Total Time 10.00(10.00)
Iter 10250 | Time 5.2076(4.5781) | Bit/dim 1.1098(1.0473) | Steps 634(604.77) | Grad Norm 36.2485(6.1612) | Total Time 10.00(10.00)
Iter 10260 | Time 4.5878(4.5762) | Bit/dim 1.1447(1.0724) | Steps 616(608.79) | Grad Norm 6.3888(6.5299) | Total Time 10.00(10.00)
Iter 10270 | Time 4.5617(4.5943) | Bit/dim 1.0717(1.0743) | Steps 610(609.13) | Grad Norm 6.3604(6.0512) | Total Time 10.00(10.00)
Iter 10280 | Time 4.7275(4.5690) | Bit/dim 1.0439(1.0674) | Steps 604(607.72) | Grad Norm 8.0824(5.8811) | Total Time 10.00(10.00)
Iter 10290 | Time 4.9179(4.5694) | Bit/dim 1.0418(1.0640) | Steps 640(607.89) | Grad Norm 9.4565(5.3761) | Total Time 10.00(10.00)
Iter 10300 | Time 4.3003(4.5680) | Bit/dim 1.1266(1.0719) | Steps 592(608.08) | Grad Norm 9.2682(7.0789) | Total Time 10.00(10.00)
Iter 10310 | Time 4.4924(4.5890) | Bit/dim 1.0422(1.0729) | Steps 622(611.11) | Grad Norm 2.1811(6.9377) | Total Time 10.00(10.00)
Iter 10320 | Time 4.3796(4.5624) | Bit/dim 1.0241(1.0684) | Steps 598(609.49) | Grad Norm 4.9167(6.6430) | Total Time 10.00(10.00)
Iter 10330 | Time 4.3514(4.5447) | Bit/dim 1.0367(1.0618) | Steps 598(608.38) | Grad Norm 3.4958(5.8918) | Total Time 10.00(10.00)
Iter 10340 | Time 4.2733(4.5255) | Bit/dim 1.0656(1.0571) | Steps 586(606.78) | Grad Norm 8.5541(6.2825) | Total Time 10.00(10.00)
Iter 10350 | Time 5.0660(4.5372) | Bit/dim 1.0728(1.0563) | Steps 640(607.15) | Grad Norm 17.7472(7.0063) | Total Time 10.00(10.00)
Iter 10360 | Time 4.4037(4.5296) | Bit/dim 1.0521(1.0550) | Steps 604(607.44) | Grad Norm 4.7652(6.9704) | Total Time 10.00(10.00)
Iter 10370 | Time 4.7119(4.5258) | Bit/dim 1.0319(1.0516) | Steps 616(607.08) | Grad Norm 2.1915(6.3835) | Total Time 10.00(10.00)
Iter 10380 | Time 4.3548(4.5366) | Bit/dim 1.0675(1.0499) | Steps 598(607.30) | Grad Norm 4.8540(5.5335) | Total Time 10.00(10.00)
Iter 10390 | Time 4.2415(4.5293) | Bit/dim 1.0387(1.0480) | Steps 580(605.94) | Grad Norm 6.1818(5.9233) | Total Time 10.00(10.00)
Iter 10400 | Time 4.8312(4.5331) | Bit/dim 1.0673(1.0489) | Steps 622(605.62) | Grad Norm 10.1847(6.4966) | Total Time 10.00(10.00)
Iter 10410 | Time 4.5474(4.5438) | Bit/dim 1.0660(1.0489) | Steps 610(606.65) | Grad Norm 1.8331(5.6389) | Total Time 10.00(10.00)
Iter 10420 | Time 4.5527(4.5447) | Bit/dim 1.0525(1.0473) | Steps 610(607.25) | Grad Norm 1.9958(4.7183) | Total Time 10.00(10.00)
Iter 10430 | Time 4.7607(4.5515) | Bit/dim 1.0502(1.0444) | Steps 622(607.88) | Grad Norm 5.5223(4.4000) | Total Time 10.00(10.00)
Iter 10440 | Time 4.6251(4.5586) | Bit/dim 1.0736(1.0430) | Steps 604(607.87) | Grad Norm 6.4930(4.5676) | Total Time 10.00(10.00)
Iter 10450 | Time 4.7118(4.5611) | Bit/dim 1.0225(1.0426) | Steps 610(607.60) | Grad Norm 16.7786(5.2528) | Total Time 10.00(10.00)
Iter 10460 | Time 4.6929(4.5527) | Bit/dim 1.1030(1.0652) | Steps 634(609.76) | Grad Norm 3.6229(6.4885) | Total Time 10.00(10.00)
Iter 10470 | Time 4.6286(4.5875) | Bit/dim 1.0589(1.0726) | Steps 610(611.54) | Grad Norm 3.2022(6.5231) | Total Time 10.00(10.00)
Iter 10480 | Time 4.5383(4.6000) | Bit/dim 1.0580(1.0706) | Steps 610(610.84) | Grad Norm 1.7249(5.3683) | Total Time 10.00(10.00)
Iter 10490 | Time 4.5319(4.5771) | Bit/dim 1.0367(1.0621) | Steps 604(608.27) | Grad Norm 1.3938(4.3959) | Total Time 10.00(10.00)
validating...
Epoch 0035 | Time 60.3909, Bit/dim 1.0331
===> Using batch size 200. Total 300 iterations/epoch.
Iter 10500 | Time 4.5340(4.5653) | Bit/dim 1.0476(1.0562) | Steps 604(606.06) | Grad Norm 0.9807(4.1363) | Total Time 10.00(10.00)
Iter 10510 | Time 4.6783(4.5649) | Bit/dim 1.0643(1.0528) | Steps 610(605.70) | Grad Norm 8.5954(4.2232) | Total Time 10.00(10.00)
Iter 10520 | Time 4.2241(4.5347) | Bit/dim 1.1992(1.0701) | Steps 610(605.32) | Grad Norm 6.7221(6.3919) | Total Time 10.00(10.00)
Iter 10530 | Time 4.9653(4.5652) | Bit/dim 1.0537(1.0810) | Steps 628(609.10) | Grad Norm 3.2472(6.2443) | Total Time 10.00(10.00)
Iter 10540 | Time 4.2943(4.5683) | Bit/dim 1.0354(1.0778) | Steps 592(610.42) | Grad Norm 3.9801(5.7232) | Total Time 10.00(10.00)
Iter 10550 | Time 4.5676(4.5678) | Bit/dim 1.0342(1.0694) | Steps 610(611.34) | Grad Norm 2.4234(4.9687) | Total Time 10.00(10.00)
Iter 10560 | Time 5.1266(4.5605) | Bit/dim 1.0980(1.0657) | Steps 628(608.95) | Grad Norm 26.1362(5.6308) | Total Time 10.00(10.00)
Iter 10570 | Time 4.5208(4.5306) | Bit/dim 1.1283(1.0818) | Steps 628(608.82) | Grad Norm 5.6338(6.8760) | Total Time 10.00(10.00)
Iter 10580 | Time 4.8178(4.6086) | Bit/dim 1.0799(1.0858) | Steps 634(614.94) | Grad Norm 2.3398(6.5577) | Total Time 10.00(10.00)
Iter 10590 | Time 4.3920(4.5827) | Bit/dim 1.0521(1.0788) | Steps 604(613.44) | Grad Norm 3.8630(5.8140) | Total Time 10.00(10.00)
Iter 10600 | Time 4.5700(4.5731) | Bit/dim 1.0601(1.0714) | Steps 610(611.31) | Grad Norm 2.5376(5.2035) | Total Time 10.00(10.00)
Iter 10610 | Time 4.6072(4.5724) | Bit/dim 1.0347(1.0614) | Steps 616(611.16) | Grad Norm 2.6535(4.3128) | Total Time 10.00(10.00)
Iter 10620 | Time 4.5669(4.5656) | Bit/dim 1.0177(1.0529) | Steps 610(610.13) | Grad Norm 0.9971(3.7069) | Total Time 10.00(10.00)
Iter 10630 | Time 4.5881(4.5492) | Bit/dim 1.0490(1.0490) | Steps 610(607.68) | Grad Norm 2.2875(3.5113) | Total Time 10.00(10.00)
Iter 10640 | Time 4.5502(4.5453) | Bit/dim 1.0453(1.0479) | Steps 604(605.96) | Grad Norm 2.4310(3.1408) | Total Time 10.00(10.00)
Iter 10650 | Time 4.6925(4.5595) | Bit/dim 1.0609(1.0483) | Steps 604(605.40) | Grad Norm 16.9091(4.8031) | Total Time 10.00(10.00)
Iter 10660 | Time 4.6748(4.5501) | Bit/dim 1.0526(1.0599) | Steps 622(606.86) | Grad Norm 7.3236(6.4220) | Total Time 10.00(10.00)
Iter 10670 | Time 4.3660(4.5615) | Bit/dim 1.0453(1.0584) | Steps 598(607.88) | Grad Norm 6.2092(6.3863) | Total Time 10.00(10.00)
Iter 10680 | Time 4.7569(4.5736) | Bit/dim 1.0403(1.0567) | Steps 616(608.44) | Grad Norm 3.3715(5.9264) | Total Time 10.00(10.00)
Iter 10690 | Time 4.5866(4.5698) | Bit/dim 1.0282(1.0500) | Steps 616(609.06) | Grad Norm 2.8440(5.2247) | Total Time 10.00(10.00)
Iter 10700 | Time 4.7209(4.5586) | Bit/dim 1.0355(1.0469) | Steps 616(608.38) | Grad Norm 14.6477(5.1748) | Total Time 10.00(10.00)
Iter 10710 | Time 4.4531(4.5255) | Bit/dim 1.0397(1.0509) | Steps 610(607.80) | Grad Norm 6.6202(6.3506) | Total Time 10.00(10.00)
Iter 10720 | Time 4.4085(4.5488) | Bit/dim 1.0547(1.0519) | Steps 604(610.35) | Grad Norm 5.7327(7.1726) | Total Time 10.00(10.00)
Iter 10730 | Time 4.7553(4.5881) | Bit/dim 1.0043(1.0494) | Steps 622(612.77) | Grad Norm 1.1626(7.0201) | Total Time 10.00(10.00)
Iter 10740 | Time 4.7592(4.6042) | Bit/dim 1.0562(1.0475) | Steps 622(615.24) | Grad Norm 2.0416(7.0274) | Total Time 10.00(10.00)
Iter 10750 | Time 4.7644(4.5959) | Bit/dim 1.0442(1.0444) | Steps 622(614.64) | Grad Norm 8.2136(6.4119) | Total Time 10.00(10.00)
Iter 10760 | Time 4.5934(4.5915) | Bit/dim 1.0668(1.0430) | Steps 616(613.97) | Grad Norm 1.5348(5.4110) | Total Time 10.00(10.00)
Iter 10770 | Time 4.5637(4.5887) | Bit/dim 1.0393(1.0405) | Steps 610(613.80) | Grad Norm 0.9429(4.3773) | Total Time 10.00(10.00)
Iter 10780 | Time 4.3163(4.5781) | Bit/dim 1.0420(1.0384) | Steps 592(612.13) | Grad Norm 7.3710(3.9868) | Total Time 10.00(10.00)
Iter 10790 | Time 4.5196(4.5746) | Bit/dim 1.0493(1.0394) | Steps 604(611.37) | Grad Norm 2.1264(4.2089) | Total Time 10.00(10.00)
validating...
Epoch 0036 | Time 64.1904, Bit/dim 1.0974
===> Using batch size 200. Total 300 iterations/epoch.
Iter 10800 | Time 4.7196(4.5910) | Bit/dim 1.1176(1.0569) | Steps 634(613.92) | Grad Norm 4.7717(5.6207) | Total Time 10.00(10.00)
Iter 10810 | Time 4.6665(4.6020) | Bit/dim 1.0553(1.0610) | Steps 628(616.50) | Grad Norm 2.9683(5.8802) | Total Time 10.00(10.00)
Iter 10820 | Time 4.6638(4.6252) | Bit/dim 1.0624(1.0592) | Steps 634(618.61) | Grad Norm 1.9969(5.5511) | Total Time 10.00(10.00)
Iter 10830 | Time 4.6007(4.6280) | Bit/dim 1.0565(1.0539) | Steps 616(619.67) | Grad Norm 1.6486(5.7482) | Total Time 10.00(10.00)
Iter 10840 | Time 4.2331(4.6099) | Bit/dim 1.1179(1.0575) | Steps 604(618.32) | Grad Norm 9.8851(6.9444) | Total Time 10.00(10.00)
Iter 10850 | Time 4.4745(4.6078) | Bit/dim 1.0421(1.0614) | Steps 616(620.22) | Grad Norm 4.8374(6.8928) | Total Time 10.00(10.00)
Iter 10860 | Time 4.5727(4.5970) | Bit/dim 1.0190(1.0583) | Steps 610(617.81) | Grad Norm 8.7017(6.4302) | Total Time 10.00(10.00)
Iter 10870 | Time 4.5999(4.5963) | Bit/dim 1.0355(1.0555) | Steps 616(616.92) | Grad Norm 5.3205(5.8004) | Total Time 10.00(10.00)
Iter 10880 | Time 4.3411(4.6009) | Bit/dim 1.0795(1.0531) | Steps 598(617.15) | Grad Norm 9.2455(5.8347) | Total Time 10.00(10.00)
Iter 10890 | Time 4.3757(4.5941) | Bit/dim 1.1331(1.0638) | Steps 604(616.87) | Grad Norm 9.5702(7.2958) | Total Time 10.00(10.00)
Iter 10900 | Time 5.3125(4.6321) | Bit/dim 1.0931(1.0693) | Steps 658(619.01) | Grad Norm 14.4727(7.3656) | Total Time 10.00(10.00)
Iter 10910 | Time 4.7050(4.6284) | Bit/dim 1.0576(1.0627) | Steps 610(617.82) | Grad Norm 6.6986(6.7499) | Total Time 10.00(10.00)
Iter 10920 | Time 4.4372(4.6092) | Bit/dim 1.0371(1.0585) | Steps 610(616.24) | Grad Norm 3.6146(5.8590) | Total Time 10.00(10.00)
Iter 10930 | Time 4.5941(4.6103) | Bit/dim 1.0359(1.0541) | Steps 616(615.97) | Grad Norm 1.4489(5.0600) | Total Time 10.00(10.00)
Iter 10940 | Time 4.5605(4.5980) | Bit/dim 1.0168(1.0500) | Steps 610(614.40) | Grad Norm 2.3758(4.2037) | Total Time 10.00(10.00)
Iter 10950 | Time 4.5618(4.5781) | Bit/dim 1.0266(1.0468) | Steps 610(612.71) | Grad Norm 6.3106(4.2775) | Total Time 10.00(10.00)
Iter 10960 | Time 4.5808(4.5830) | Bit/dim 1.0231(1.0414) | Steps 610(611.58) | Grad Norm 3.0646(4.1354) | Total Time 10.00(10.00)
Iter 10970 | Time 4.5707(4.5967) | Bit/dim 1.0027(1.0378) | Steps 610(611.16) | Grad Norm 3.2794(4.1658) | Total Time 10.00(10.00)
Iter 10980 | Time 4.5714(4.5876) | Bit/dim 1.0257(1.0376) | Steps 610(610.50) | Grad Norm 4.6339(4.5775) | Total Time 10.00(10.00)
Iter 10990 | Time 5.1537(4.5951) | Bit/dim 1.0626(1.0407) | Steps 634(610.48) | Grad Norm 24.0772(5.2683) | Total Time 10.00(10.00)
Iter 11000 | Time 4.5141(4.5864) | Bit/dim 1.1155(1.0627) | Steps 628(614.05) | Grad Norm 5.0087(6.6218) | Total Time 10.00(10.00)
Iter 11010 | Time 4.6328(4.6434) | Bit/dim 1.0939(1.0648) | Steps 622(618.81) | Grad Norm 6.1342(6.6820) | Total Time 10.00(10.00)
Iter 11020 | Time 4.7690(4.6551) | Bit/dim 1.0395(1.0596) | Steps 628(619.87) | Grad Norm 3.4026(5.9329) | Total Time 10.00(10.00)
Iter 11030 | Time 4.6029(4.6488) | Bit/dim 1.0241(1.0539) | Steps 616(619.35) | Grad Norm 1.6951(5.1276) | Total Time 10.00(10.00)
Iter 11040 | Time 5.0060(4.6287) | Bit/dim 1.0922(1.0486) | Steps 634(617.43) | Grad Norm 21.5355(5.7004) | Total Time 10.00(10.00)
Iter 11050 | Time 4.7437(4.6299) | Bit/dim 1.0605(1.0545) | Steps 622(617.46) | Grad Norm 1.8094(6.7768) | Total Time 10.00(10.00)
Iter 11060 | Time 4.4108(4.6395) | Bit/dim 1.0546(1.0544) | Steps 604(619.28) | Grad Norm 4.4300(6.5921) | Total Time 10.00(10.00)
Iter 11070 | Time 4.7553(4.6301) | Bit/dim 1.0395(1.0518) | Steps 622(618.00) | Grad Norm 1.5086(5.9910) | Total Time 10.00(10.00)
Iter 11080 | Time 4.6117(4.6443) | Bit/dim 1.0059(1.0472) | Steps 622(619.40) | Grad Norm 3.0837(5.3522) | Total Time 10.00(10.00)
Iter 11090 | Time 4.9060(4.6358) | Bit/dim 1.0530(1.0487) | Steps 646(619.83) | Grad Norm 9.0814(5.9036) | Total Time 10.00(10.00)
validating...
Epoch 0037 | Time 61.1219, Bit/dim 1.0314
===> Using batch size 200. Total 300 iterations/epoch.
Iter 11100 | Time 4.5897(4.6265) | Bit/dim 1.0322(1.0461) | Steps 616(618.94) | Grad Norm 1.9813(5.2087) | Total Time 10.00(10.00)
Iter 11110 | Time 4.5679(4.6145) | Bit/dim 1.0287(1.0414) | Steps 610(618.13) | Grad Norm 3.5671(4.3808) | Total Time 10.00(10.00)
Iter 11120 | Time 4.5908(4.6058) | Bit/dim 1.0077(1.0387) | Steps 616(617.07) | Grad Norm 3.4505(4.2538) | Total Time 10.00(10.00)
Iter 11130 | Time 4.7538(4.6190) | Bit/dim 1.0569(1.0494) | Steps 640(619.68) | Grad Norm 3.7587(5.7800) | Total Time 10.00(10.00)
Iter 11140 | Time 4.4806(4.6294) | Bit/dim 1.0544(1.0589) | Steps 616(621.84) | Grad Norm 4.9656(6.3122) | Total Time 10.00(10.00)
Iter 11150 | Time 4.7253(4.6330) | Bit/dim 1.0378(1.0564) | Steps 616(620.60) | Grad Norm 2.9380(6.1619) | Total Time 10.00(10.00)
Iter 11160 | Time 4.6651(4.6146) | Bit/dim 1.0305(1.0532) | Steps 628(619.64) | Grad Norm 2.6157(5.4967) | Total Time 10.00(10.00)
Iter 11170 | Time 4.5988(4.6297) | Bit/dim 1.0466(1.0480) | Steps 616(620.25) | Grad Norm 7.9426(5.1710) | Total Time 10.00(10.00)
Iter 11180 | Time 4.5111(4.6346) | Bit/dim 1.0410(1.0527) | Steps 634(622.15) | Grad Norm 5.3964(6.3339) | Total Time 10.00(10.00)
Iter 11190 | Time 4.4660(4.6474) | Bit/dim 1.0411(1.0527) | Steps 616(623.35) | Grad Norm 5.3892(6.2060) | Total Time 10.00(10.00)
Iter 11200 | Time 4.7789(4.6619) | Bit/dim 1.0257(1.0488) | Steps 628(623.49) | Grad Norm 2.8273(5.5430) | Total Time 10.00(10.00)
Iter 11210 | Time 4.6201(4.6457) | Bit/dim 1.0283(1.0452) | Steps 622(622.72) | Grad Norm 4.6365(4.9194) | Total Time 10.00(10.00)
Iter 11220 | Time 4.6124(4.6375) | Bit/dim 1.0458(1.0419) | Steps 622(621.73) | Grad Norm 2.1436(4.3581) | Total Time 10.00(10.00)
Iter 11230 | Time 4.7465(4.6283) | Bit/dim 1.0559(1.0399) | Steps 622(620.25) | Grad Norm 6.5476(4.4074) | Total Time 10.00(10.00)
Iter 11240 | Time 4.7905(4.6462) | Bit/dim 1.0999(1.0600) | Steps 634(622.79) | Grad Norm 3.4570(5.9356) | Total Time 10.00(10.00)
Iter 11250 | Time 4.7713(4.6597) | Bit/dim 1.0381(1.0663) | Steps 628(625.04) | Grad Norm 2.6980(5.8041) | Total Time 10.00(10.00)
Iter 11260 | Time 4.5111(4.6477) | Bit/dim 1.0375(1.0604) | Steps 628(623.85) | Grad Norm 3.7436(5.4161) | Total Time 10.00(10.00)
Iter 11270 | Time 4.6415(4.6353) | Bit/dim 1.0373(1.0552) | Steps 622(624.12) | Grad Norm 6.5713(5.5779) | Total Time 10.00(10.00)
Iter 11280 | Time 4.6462(4.6490) | Bit/dim 1.0230(1.0456) | Steps 628(623.93) | Grad Norm 7.6478(5.1906) | Total Time 10.00(10.00)
Iter 11290 | Time 4.5174(4.6377) | Bit/dim 1.1564(1.0636) | Steps 640(624.71) | Grad Norm 6.0377(6.6899) | Total Time 10.00(10.00)
Iter 11300 | Time 4.7517(4.6720) | Bit/dim 1.0730(1.0722) | Steps 646(628.27) | Grad Norm 2.5141(6.2825) | Total Time 10.00(10.00)
Iter 11310 | Time 4.4332(4.6781) | Bit/dim 1.0614(1.0695) | Steps 616(627.87) | Grad Norm 2.5086(5.5556) | Total Time 10.00(10.00)
Iter 11320 | Time 4.7885(4.6941) | Bit/dim 1.0397(1.0626) | Steps 634(627.63) | Grad Norm 1.2783(4.5792) | Total Time 10.00(10.00)
Iter 11330 | Time 4.9161(4.6907) | Bit/dim 1.0232(1.0610) | Steps 652(627.93) | Grad Norm 3.8214(5.2012) | Total Time 10.00(10.00)
Iter 11340 | Time 4.5576(4.6755) | Bit/dim 1.0687(1.0695) | Steps 622(627.57) | Grad Norm 6.0056(6.4826) | Total Time 10.00(10.00)
Iter 11350 | Time 4.4731(4.6842) | Bit/dim 1.0462(1.0661) | Steps 622(629.67) | Grad Norm 5.1084(6.0068) | Total Time 10.00(10.00)
Iter 11360 | Time 4.8631(4.6953) | Bit/dim 1.0329(1.0611) | Steps 646(629.90) | Grad Norm 2.2009(5.3674) | Total Time 10.00(10.00)
Iter 11370 | Time 4.6132(4.6929) | Bit/dim 1.0133(1.0535) | Steps 628(628.73) | Grad Norm 0.8174(4.3882) | Total Time 10.00(10.00)
Iter 11380 | Time 4.6047(4.6650) | Bit/dim 1.0339(1.0476) | Steps 622(625.53) | Grad Norm 1.1190(3.7237) | Total Time 10.00(10.00)
Iter 11390 | Time 4.5441(4.6409) | Bit/dim 1.0201(1.0435) | Steps 610(622.57) | Grad Norm 3.8754(3.6909) | Total Time 10.00(10.00)
validating...
Epoch 0038 | Time 61.3478, Bit/dim 1.0275
===> Using batch size 200. Total 300 iterations/epoch.
Iter 11400 | Time 4.5840(4.6390) | Bit/dim 1.0351(1.0405) | Steps 616(621.00) | Grad Norm 2.8585(3.8796) | Total Time 10.00(10.00)
Iter 11410 | Time 4.4343(4.6380) | Bit/dim 1.0735(1.0548) | Steps 622(623.65) | Grad Norm 6.3596(5.6643) | Total Time 10.00(10.00)
Iter 11420 | Time 4.3122(4.6383) | Bit/dim 1.0510(1.0593) | Steps 598(623.97) | Grad Norm 7.2932(6.4692) | Total Time 10.00(10.00)
Iter 11430 | Time 4.6283(4.6691) | Bit/dim 1.0949(1.0592) | Steps 628(625.77) | Grad Norm 5.6060(6.4942) | Total Time 10.00(10.00)
Iter 11440 | Time 4.8298(4.6646) | Bit/dim 1.0375(1.0545) | Steps 634(624.67) | Grad Norm 3.1953(5.9826) | Total Time 10.00(10.00)
Iter 11450 | Time 4.6134(4.6447) | Bit/dim 1.0367(1.0503) | Steps 622(623.27) | Grad Norm 4.4219(5.4033) | Total Time 10.00(10.00)
Iter 11460 | Time 4.4586(4.6454) | Bit/dim 1.0343(1.0468) | Steps 616(623.60) | Grad Norm 5.4504(5.4887) | Total Time 10.00(10.00)
Iter 11470 | Time 4.4607(4.6578) | Bit/dim 1.0537(1.0462) | Steps 616(625.58) | Grad Norm 7.4437(5.9960) | Total Time 10.00(10.00)
Iter 11480 | Time 4.4066(4.6660) | Bit/dim 1.0519(1.0437) | Steps 610(626.86) | Grad Norm 8.8925(6.4458) | Total Time 10.00(10.00)
Iter 11490 | Time 4.2906(4.6759) | Bit/dim 1.0847(1.0488) | Steps 616(628.58) | Grad Norm 7.9832(7.4080) | Total Time 10.00(10.00)
Iter 11500 | Time 5.1600(4.7103) | Bit/dim 1.0702(1.0532) | Steps 658(631.14) | Grad Norm 11.3945(7.2887) | Total Time 10.00(10.00)
Iter 11510 | Time 4.9997(4.6998) | Bit/dim 1.0436(1.0530) | Steps 646(629.74) | Grad Norm 4.7766(6.3814) | Total Time 10.00(10.00)
Iter 11520 | Time 4.7686(4.7139) | Bit/dim 1.0293(1.0477) | Steps 628(630.73) | Grad Norm 1.2153(5.3510) | Total Time 10.00(10.00)
Iter 11530 | Time 4.8018(4.7057) | Bit/dim 1.0469(1.0440) | Steps 628(629.30) | Grad Norm 1.1173(4.6010) | Total Time 10.00(10.00)
Iter 11540 | Time 4.1974(4.6818) | Bit/dim 1.1409(1.0510) | Steps 604(627.06) | Grad Norm 9.5138(6.3134) | Total Time 10.00(10.00)
Iter 11550 | Time 5.1317(4.7031) | Bit/dim 1.0320(1.0590) | Steps 658(630.71) | Grad Norm 9.3032(6.2877) | Total Time 10.00(10.00)
Iter 11560 | Time 4.4650(4.6991) | Bit/dim 1.0582(1.0551) | Steps 622(630.14) | Grad Norm 3.3443(5.7534) | Total Time 10.00(10.00)
Iter 11570 | Time 4.5639(4.6922) | Bit/dim 1.0644(1.0505) | Steps 610(627.69) | Grad Norm 2.1393(5.4226) | Total Time 10.00(10.00)
Iter 11580 | Time 4.5565(4.6635) | Bit/dim 1.0422(1.0478) | Steps 610(624.44) | Grad Norm 2.5290(5.0639) | Total Time 10.00(10.00)
Iter 11590 | Time 4.8643(4.6538) | Bit/dim 1.0623(1.0468) | Steps 634(623.83) | Grad Norm 6.1692(5.7407) | Total Time 10.00(10.00)
Iter 11600 | Time 4.5777(4.6565) | Bit/dim 1.0213(1.0428) | Steps 616(622.37) | Grad Norm 4.7863(5.6550) | Total Time 10.00(10.00)
Iter 11610 | Time 4.3967(4.6509) | Bit/dim 1.0343(1.0413) | Steps 604(620.94) | Grad Norm 6.1321(6.0558) | Total Time 10.00(10.00)
Iter 11620 | Time 4.6090(4.6496) | Bit/dim 1.0506(1.0387) | Steps 616(620.80) | Grad Norm 3.1643(5.4229) | Total Time 10.00(10.00)
Iter 11630 | Time 4.6968(4.6427) | Bit/dim 1.0076(1.0344) | Steps 634(620.02) | Grad Norm 2.4371(4.4995) | Total Time 10.00(10.00)
Iter 11640 | Time 4.7097(4.6405) | Bit/dim 1.0220(1.0325) | Steps 628(619.50) | Grad Norm 2.1646(4.0517) | Total Time 10.00(10.00)
Iter 11650 | Time 4.5693(4.6643) | Bit/dim 1.1181(1.0335) | Steps 622(622.17) | Grad Norm 11.3891(5.0635) | Total Time 10.00(10.00)
Iter 11660 | Time 4.5944(4.6708) | Bit/dim 1.0512(1.0437) | Steps 640(625.84) | Grad Norm 4.4178(5.7689) | Total Time 10.00(10.00)
Iter 11670 | Time 4.8756(4.6921) | Bit/dim 1.0409(1.0448) | Steps 646(627.38) | Grad Norm 1.7068(5.5161) | Total Time 10.00(10.00)
Iter 11680 | Time 4.8265(4.7137) | Bit/dim 1.0179(1.0410) | Steps 640(629.64) | Grad Norm 1.3993(4.4789) | Total Time 10.00(10.00)
Iter 11690 | Time 4.9082(4.7272) | Bit/dim 1.0411(1.0381) | Steps 628(630.49) | Grad Norm 7.9703(4.3438) | Total Time 10.00(10.00)
validating...
Epoch 0039 | Time 61.8478, Bit/dim 1.0792
===> Using batch size 200. Total 300 iterations/epoch.
Iter 11700 | Time 4.9128(4.7359) | Bit/dim 1.0709(1.0532) | Steps 640(631.95) | Grad Norm 10.4199(5.9657) | Total Time 10.00(10.00)
Iter 11710 | Time 5.0942(4.7231) | Bit/dim 1.0471(1.0551) | Steps 634(632.67) | Grad Norm 2.8200(5.5866) | Total Time 10.00(10.00)
Iter 11720 | Time 4.6105(4.7206) | Bit/dim 1.0438(1.0518) | Steps 622(631.26) | Grad Norm 3.4355(5.2653) | Total Time 10.00(10.00)
Iter 11730 | Time 4.5343(4.7001) | Bit/dim 1.0163(1.0468) | Steps 622(629.23) | Grad Norm 5.9837(5.1457) | Total Time 10.00(10.00)
Iter 11740 | Time 4.4833(4.6906) | Bit/dim 1.1167(1.0579) | Steps 634(628.89) | Grad Norm 6.4215(6.5567) | Total Time 10.00(10.00)
Iter 11750 | Time 4.9555(4.7394) | Bit/dim 1.0729(1.0614) | Steps 640(633.28) | Grad Norm 2.8664(6.5022) | Total Time 10.00(10.00)
Iter 11760 | Time 4.8253(4.7515) | Bit/dim 1.0216(1.0579) | Steps 640(633.51) | Grad Norm 4.2777(5.8927) | Total Time 10.00(10.00)
Iter 11770 | Time 4.8363(4.7366) | Bit/dim 1.0350(1.0501) | Steps 646(631.46) | Grad Norm 2.7684(5.1159) | Total Time 10.00(10.00)
Iter 11780 | Time 4.6657(4.7275) | Bit/dim 1.0167(1.0474) | Steps 628(629.54) | Grad Norm 1.8384(4.7114) | Total Time 10.00(10.00)
Iter 11790 | Time 4.3238(4.7426) | Bit/dim 1.1530(1.0514) | Steps 622(630.75) | Grad Norm 8.4118(6.4266) | Total Time 10.00(10.00)
Iter 11800 | Time 5.0610(4.7432) | Bit/dim 1.0558(1.0597) | Steps 652(633.26) | Grad Norm 2.6815(6.2356) | Total Time 10.00(10.00)
Iter 11810 | Time 4.9085(4.7708) | Bit/dim 1.0025(1.0537) | Steps 628(633.66) | Grad Norm 3.9900(5.7096) | Total Time 10.00(10.00)
Iter 11820 | Time 4.7925(4.7385) | Bit/dim 1.0196(1.0476) | Steps 634(631.40) | Grad Norm 1.0875(5.1534) | Total Time 10.00(10.00)
Iter 11830 | Time 4.5376(4.7029) | Bit/dim 1.0249(1.0470) | Steps 604(626.79) | Grad Norm 5.8023(5.1870) | Total Time 10.00(10.00)
Iter 11840 | Time 4.9690(4.7116) | Bit/dim 1.0510(1.0425) | Steps 640(625.32) | Grad Norm 8.7918(5.1744) | Total Time 10.00(10.00)
Iter 11850 | Time 4.5059(4.7015) | Bit/dim 1.0671(1.0490) | Steps 634(624.91) | Grad Norm 5.8276(6.3933) | Total Time 10.00(10.00)
Iter 11860 | Time 4.7295(4.7417) | Bit/dim 1.0323(1.0521) | Steps 646(630.11) | Grad Norm 5.1279(6.6203) | Total Time 10.00(10.00)
Iter 11870 | Time 4.7283(4.7457) | Bit/dim 1.0472(1.0505) | Steps 616(631.90) | Grad Norm 3.6595(5.8876) | Total Time 10.00(10.00)
Iter 11880 | Time 4.8505(4.7331) | Bit/dim 1.0132(1.0454) | Steps 646(632.10) | Grad Norm 1.4692(5.0745) | Total Time 10.00(10.00)
Iter 11890 | Time 4.6801(4.7294) | Bit/dim 1.0271(1.0396) | Steps 628(630.59) | Grad Norm 1.6324(4.2465) | Total Time 10.00(10.00)
Iter 11900 | Time 4.5926(4.7157) | Bit/dim 1.0671(1.0375) | Steps 616(628.60) | Grad Norm 2.1903(3.6170) | Total Time 10.00(10.00)
Iter 11910 | Time 4.9545(4.7110) | Bit/dim 1.0246(1.0338) | Steps 640(628.97) | Grad Norm 5.6872(3.3310) | Total Time 10.00(10.00)
Iter 11920 | Time 5.2649(4.7276) | Bit/dim 1.0425(1.0313) | Steps 658(630.73) | Grad Norm 14.1435(3.7727) | Total Time 10.00(10.00)
Iter 11930 | Time 4.5929(4.7525) | Bit/dim 1.1257(1.0470) | Steps 628(633.55) | Grad Norm 9.6448(6.1494) | Total Time 10.00(10.00)
Iter 11940 | Time 4.5269(4.7455) | Bit/dim 1.0812(1.0658) | Steps 628(636.20) | Grad Norm 3.6554(5.9863) | Total Time 10.00(10.00)
Iter 11950 | Time 4.8223(4.7901) | Bit/dim 1.0424(1.0627) | Steps 640(638.48) | Grad Norm 2.5422(5.4560) | Total Time 10.00(10.00)
Iter 11960 | Time 4.8736(4.8076) | Bit/dim 1.0419(1.0550) | Steps 646(639.87) | Grad Norm 1.3946(4.5736) | Total Time 10.00(10.00)
Iter 11970 | Time 4.4984(4.7978) | Bit/dim 1.0230(1.0509) | Steps 622(638.58) | Grad Norm 5.5656(4.5161) | Total Time 10.00(10.00)
Iter 11980 | Time 4.6396(4.7617) | Bit/dim 1.0187(1.0458) | Steps 628(636.35) | Grad Norm 2.3783(5.2380) | Total Time 10.00(10.00)
Iter 11990 | Time 4.7874(4.7843) | Bit/dim 1.0386(1.0427) | Steps 634(637.39) | Grad Norm 3.5554(5.5697) | Total Time 10.00(10.00)
validating...
Epoch 0040 | Time 61.6421, Bit/dim 1.0273
===> Using batch size 200. Total 300 iterations/epoch.
Iter 12000 | Time 4.6620(4.7903) | Bit/dim 1.0177(1.0376) | Steps 634(638.67) | Grad Norm 5.9528(5.6229) | Total Time 10.00(10.00)
Iter 12010 | Time 4.5946(4.7949) | Bit/dim 1.0313(1.0390) | Steps 628(638.67) | Grad Norm 7.3558(6.9034) | Total Time 10.00(10.00)
Iter 12020 | Time 4.9945(4.7909) | Bit/dim 1.0573(1.0444) | Steps 658(640.91) | Grad Norm 7.0377(6.9486) | Total Time 10.00(10.00)
Iter 12030 | Time 4.5073(4.7695) | Bit/dim 1.0593(1.0434) | Steps 628(639.62) | Grad Norm 4.0568(6.4983) | Total Time 10.00(10.00)
Iter 12040 | Time 4.8248(4.7615) | Bit/dim 1.0445(1.0413) | Steps 646(638.69) | Grad Norm 2.0206(5.9365) | Total Time 10.00(10.00)
Iter 12050 | Time 4.7807(4.7669) | Bit/dim 1.0038(1.0345) | Steps 652(639.58) | Grad Norm 3.2143(5.4753) | Total Time 10.00(10.00)
Iter 12060 | Time 4.7050(4.7874) | Bit/dim 1.0567(1.0347) | Steps 634(638.98) | Grad Norm 4.7179(5.5015) | Total Time 10.00(10.00)
Iter 12070 | Time 4.7358(4.7852) | Bit/dim 0.9871(1.0305) | Steps 646(639.52) | Grad Norm 5.1705(4.8307) | Total Time 10.00(10.00)
Iter 12080 | Time 5.2651(4.8148) | Bit/dim 1.0709(1.0362) | Steps 658(641.79) | Grad Norm 24.5519(6.2466) | Total Time 10.00(10.00)
Iter 12090 | Time 4.5686(4.8102) | Bit/dim 1.0783(1.0495) | Steps 640(644.36) | Grad Norm 4.9503(6.5872) | Total Time 10.00(10.00)
Iter 12100 | Time 4.7215(4.8306) | Bit/dim 1.0193(1.0510) | Steps 640(644.59) | Grad Norm 4.8164(6.2109) | Total Time 10.00(10.00)
Iter 12110 | Time 4.7991(4.8352) | Bit/dim 1.0395(1.0464) | Steps 634(643.05) | Grad Norm 2.0511(5.2454) | Total Time 10.00(10.00)
Iter 12120 | Time 4.8072(4.8215) | Bit/dim 1.0471(1.0428) | Steps 640(641.55) | Grad Norm 1.0429(4.4104) | Total Time 10.00(10.00)
Iter 12130 | Time 4.9265(4.8132) | Bit/dim 1.0708(1.0397) | Steps 676(641.86) | Grad Norm 2.8534(3.7618) | Total Time 10.00(10.00)
Iter 12140 | Time 4.7620(4.7821) | Bit/dim 1.0361(1.0335) | Steps 646(639.35) | Grad Norm 2.9170(3.2785) | Total Time 10.00(10.00)
Iter 12150 | Time 4.7401(4.7885) | Bit/dim 1.0179(1.0323) | Steps 640(640.38) | Grad Norm 3.5125(3.6383) | Total Time 10.00(10.00)
Iter 12160 | Time 4.6415(4.7612) | Bit/dim 1.0177(1.0317) | Steps 628(637.73) | Grad Norm 1.4003(3.8002) | Total Time 10.00(10.00)
Iter 12170 | Time 4.7608(4.7495) | Bit/dim 1.1428(1.0508) | Steps 664(638.75) | Grad Norm 6.0624(5.8149) | Total Time 10.00(10.00)
Iter 12180 | Time 5.1200(4.8011) | Bit/dim 1.0576(1.0616) | Steps 676(643.93) | Grad Norm 4.3542(5.5813) | Total Time 10.00(10.00)
Iter 12190 | Time 4.9440(4.8513) | Bit/dim 1.0409(1.0576) | Steps 664(646.65) | Grad Norm 2.2239(4.9988) | Total Time 10.00(10.00)
Iter 12200 | Time 4.8267(4.8437) | Bit/dim 1.0329(1.0520) | Steps 640(644.46) | Grad Norm 0.9881(4.1461) | Total Time 10.00(10.00)
Iter 12210 | Time 4.6522(4.8156) | Bit/dim 1.0348(1.0456) | Steps 634(642.83) | Grad Norm 2.7483(3.6781) | Total Time 10.00(10.00)
Iter 12220 | Time 5.3910(4.8267) | Bit/dim 1.0286(1.0409) | Steps 664(641.31) | Grad Norm 10.7193(4.5004) | Total Time 10.00(10.00)
Iter 12230 | Time 4.2697(4.7998) | Bit/dim 1.1794(1.0487) | Steps 616(639.01) | Grad Norm 9.1691(6.0246) | Total Time 10.00(10.00)
Iter 12240 | Time 4.8224(4.8046) | Bit/dim 1.0585(1.0577) | Steps 640(641.34) | Grad Norm 2.5759(5.8869) | Total Time 10.00(10.00)
Iter 12250 | Time 5.2441(4.8589) | Bit/dim 1.0263(1.0549) | Steps 664(642.70) | Grad Norm 2.8813(5.4199) | Total Time 10.00(10.00)
Iter 12260 | Time 4.7982(4.8419) | Bit/dim 1.0551(1.0502) | Steps 646(641.17) | Grad Norm 1.2359(4.6886) | Total Time 10.00(10.00)
Iter 12270 | Time 4.7182(4.8330) | Bit/dim 1.0030(1.0442) | Steps 646(640.94) | Grad Norm 2.0631(3.9040) | Total Time 10.00(10.00)
Iter 12280 | Time 4.7366(4.8000) | Bit/dim 1.0340(1.0398) | Steps 646(640.69) | Grad Norm 1.6256(3.5809) | Total Time 10.00(10.00)
Iter 12290 | Time 4.5001(4.7701) | Bit/dim 1.0509(1.0361) | Steps 634(640.12) | Grad Norm 7.2301(3.6998) | Total Time 10.00(10.00)
validating...
Epoch 0041 | Time 56.7840, Bit/dim 1.1195
===> Using batch size 200. Total 300 iterations/epoch.
Iter 12300 | Time 4.5734(4.7801) | Bit/dim 1.1462(1.0419) | Steps 628(641.39) | Grad Norm 9.1211(5.2814) | Total Time 10.00(10.00)
Iter 12310 | Time 4.4810(4.7830) | Bit/dim 1.0512(1.0482) | Steps 628(642.94) | Grad Norm 5.3060(6.0958) | Total Time 10.00(10.00)
Iter 12320 | Time 4.4818(4.7745) | Bit/dim 1.0488(1.0471) | Steps 628(642.73) | Grad Norm 5.1432(6.1897) | Total Time 10.00(10.00)
Iter 12330 | Time 5.1318(4.7979) | Bit/dim 1.0106(1.0461) | Steps 652(643.18) | Grad Norm 3.1741(5.5356) | Total Time 10.00(10.00)
Iter 12340 | Time 4.8107(4.7894) | Bit/dim 1.0404(1.0417) | Steps 640(642.07) | Grad Norm 1.0548(4.8039) | Total Time 10.00(10.00)
Iter 12350 | Time 4.8359(4.7785) | Bit/dim 1.0388(1.0405) | Steps 646(641.93) | Grad Norm 4.3640(5.2033) | Total Time 10.00(10.00)
Iter 12360 | Time 4.9460(4.8020) | Bit/dim 1.1184(1.0638) | Steps 652(645.02) | Grad Norm 5.6847(6.2477) | Total Time 10.00(10.00)
Iter 12370 | Time 5.3797(4.8182) | Bit/dim 1.0638(1.0643) | Steps 670(645.33) | Grad Norm 2.4008(5.6367) | Total Time 10.00(10.00)
Iter 12380 | Time 5.0503(4.8295) | Bit/dim 1.0409(1.0573) | Steps 664(644.11) | Grad Norm 1.8732(4.6188) | Total Time 10.00(10.00)
Iter 12390 | Time 4.8310(4.8136) | Bit/dim 1.0331(1.0497) | Steps 640(643.44) | Grad Norm 3.5144(4.2326) | Total Time 10.00(10.00)
Iter 12400 | Time 4.2599(4.7897) | Bit/dim 1.1334(1.0508) | Steps 616(641.69) | Grad Norm 8.8355(6.0536) | Total Time 10.00(10.00)
Iter 12410 | Time 5.0923(4.8082) | Bit/dim 1.0544(1.0544) | Steps 664(646.36) | Grad Norm 9.0496(6.1147) | Total Time 10.00(10.00)
Iter 12420 | Time 4.9391(4.8380) | Bit/dim 1.0312(1.0523) | Steps 664(647.22) | Grad Norm 1.4225(5.2783) | Total Time 10.00(10.00)
Iter 12430 | Time 4.7274(4.8219) | Bit/dim 1.0199(1.0462) | Steps 640(645.48) | Grad Norm 3.8811(4.5833) | Total Time 10.00(10.00)
Iter 12440 | Time 4.7257(4.8097) | Bit/dim 1.0059(1.0410) | Steps 634(643.66) | Grad Norm 6.2517(4.0827) | Total Time 10.00(10.00)
Iter 12450 | Time 4.8972(4.8217) | Bit/dim 1.1443(1.0470) | Steps 640(644.08) | Grad Norm 6.9867(5.7146) | Total Time 10.00(10.00)
Iter 12460 | Time 4.5544(4.8420) | Bit/dim 1.0926(1.0524) | Steps 628(647.02) | Grad Norm 3.1185(5.8918) | Total Time 10.00(10.00)
Iter 12470 | Time 4.4821(4.8284) | Bit/dim 1.0104(1.0473) | Steps 628(646.47) | Grad Norm 4.0435(5.5531) | Total Time 10.00(10.00)
Iter 12480 | Time 4.7754(4.8082) | Bit/dim 1.0259(1.0445) | Steps 628(644.11) | Grad Norm 1.7518(5.0062) | Total Time 10.00(10.00)
Iter 12490 | Time 4.6401(4.7834) | Bit/dim 1.0206(1.0395) | Steps 634(642.97) | Grad Norm 2.5565(4.5412) | Total Time 10.00(10.00)
Iter 12500 | Time 4.8226(4.8120) | Bit/dim 1.0405(1.0378) | Steps 646(645.95) | Grad Norm 2.5888(4.9072) | Total Time 10.00(10.00)
Iter 12510 | Time 4.9063(4.8192) | Bit/dim 1.0899(1.0364) | Steps 658(648.26) | Grad Norm 8.2102(5.6683) | Total Time 10.00(10.00)
Iter 12520 | Time 5.0856(4.8374) | Bit/dim 1.0329(1.0417) | Steps 670(649.39) | Grad Norm 5.9311(6.4588) | Total Time 10.00(10.00)
Iter 12530 | Time 4.5627(4.8237) | Bit/dim 1.0277(1.0412) | Steps 640(649.67) | Grad Norm 4.4769(6.2432) | Total Time 10.00(10.00)
Iter 12540 | Time 4.9937(4.8299) | Bit/dim 1.0317(1.0382) | Steps 652(648.77) | Grad Norm 4.6759(5.5842) | Total Time 10.00(10.00)
Iter 12550 | Time 4.8165(4.8284) | Bit/dim 0.9889(1.0316) | Steps 640(647.28) | Grad Norm 2.4436(4.7954) | Total Time 10.00(10.00)
Iter 12560 | Time 4.6788(4.8266) | Bit/dim 1.0073(1.0268) | Steps 640(647.21) | Grad Norm 3.9686(4.0229) | Total Time 10.00(10.00)
Iter 12570 | Time 4.6393(4.8041) | Bit/dim 1.0896(1.0263) | Steps 634(646.01) | Grad Norm 9.7697(4.8744) | Total Time 10.00(10.00)
Iter 12580 | Time 5.5549(4.8288) | Bit/dim 1.1017(1.0408) | Steps 694(647.57) | Grad Norm 20.7929(6.2316) | Total Time 10.00(10.00)
Iter 12590 | Time 4.5917(4.8471) | Bit/dim 1.0202(1.0460) | Steps 646(648.51) | Grad Norm 3.7822(6.1548) | Total Time 10.00(10.00)
validating...
Epoch 0042 | Time 61.4788, Bit/dim 1.0274
===> Using batch size 200. Total 300 iterations/epoch.
Iter 12600 | Time 4.8433(4.8719) | Bit/dim 1.0264(1.0461) | Steps 646(649.34) | Grad Norm 1.1086(5.3629) | Total Time 10.00(10.00)
Iter 12610 | Time 4.7960(4.8506) | Bit/dim 1.0105(1.0428) | Steps 640(647.85) | Grad Norm 1.0500(4.5032) | Total Time 10.00(10.00)
Iter 12620 | Time 4.9019(4.8519) | Bit/dim 0.9856(1.0350) | Steps 652(648.22) | Grad Norm 1.9195(3.6377) | Total Time 10.00(10.00)
Iter 12630 | Time 4.8888(4.8388) | Bit/dim 1.0252(1.0325) | Steps 658(647.40) | Grad Norm 1.2335(3.1358) | Total Time 10.00(10.00)
Iter 12640 | Time 4.7797(4.8204) | Bit/dim 1.0008(1.0287) | Steps 634(646.36) | Grad Norm 0.8303(2.9456) | Total Time 10.00(10.00)
Iter 12650 | Time 4.7678(4.8228) | Bit/dim 1.1340(1.0416) | Steps 658(646.30) | Grad Norm 6.1265(4.6678) | Total Time 10.00(10.00)
Iter 12660 | Time 5.0361(4.8362) | Bit/dim 1.0560(1.0518) | Steps 670(649.43) | Grad Norm 9.0897(5.0463) | Total Time 10.00(10.00)
Iter 12670 | Time 4.6116(4.8221) | Bit/dim 1.0459(1.0487) | Steps 646(647.08) | Grad Norm 4.4499(4.7962) | Total Time 10.00(10.00)
Iter 12680 | Time 4.9929(4.8464) | Bit/dim 1.0038(1.0428) | Steps 652(646.56) | Grad Norm 2.6079(4.3551) | Total Time 10.00(10.00)
Iter 12690 | Time 4.8327(4.8492) | Bit/dim 1.0075(1.0364) | Steps 646(646.46) | Grad Norm 1.6140(3.7383) | Total Time 10.00(10.00)
Iter 12700 | Time 4.6306(4.8249) | Bit/dim 1.0013(1.0297) | Steps 628(644.27) | Grad Norm 4.6233(3.5033) | Total Time 10.00(10.00)
Iter 12710 | Time 4.8912(4.8376) | Bit/dim 1.0389(1.0340) | Steps 676(645.72) | Grad Norm 5.7106(5.0051) | Total Time 10.00(10.00)
Iter 12720 | Time 4.6745(4.8423) | Bit/dim 1.0775(1.0480) | Steps 646(647.82) | Grad Norm 6.0798(6.0694) | Total Time 10.00(10.00)
Iter 12730 | Time 4.8052(4.8341) | Bit/dim 1.0497(1.0460) | Steps 634(647.27) | Grad Norm 5.1255(5.8598) | Total Time 10.00(10.00)
Iter 12740 | Time 4.9292(4.8345) | Bit/dim 1.0376(1.0425) | Steps 646(645.85) | Grad Norm 4.7843(5.2223) | Total Time 10.00(10.00)
Iter 12750 | Time 4.7815(4.8244) | Bit/dim 1.0397(1.0390) | Steps 640(644.46) | Grad Norm 2.1476(4.4558) | Total Time 10.00(10.00)
Iter 12760 | Time 4.7559(4.8170) | Bit/dim 1.0548(1.0359) | Steps 652(644.74) | Grad Norm 6.9143(4.1549) | Total Time 10.00(10.00)
Iter 12770 | Time 5.2634(4.8427) | Bit/dim 1.1491(1.0504) | Steps 694(649.10) | Grad Norm 5.8026(5.8485) | Total Time 10.00(10.00)
Iter 12780 | Time 5.0542(4.8583) | Bit/dim 1.0392(1.0599) | Steps 658(652.63) | Grad Norm 1.9421(5.5158) | Total Time 10.00(10.00)
Iter 12790 | Time 4.8529(4.8903) | Bit/dim 1.0254(1.0585) | Steps 640(651.58) | Grad Norm 1.3769(4.8765) | Total Time 10.00(10.00)
Iter 12800 | Time 4.8139(4.8561) | Bit/dim 1.0040(1.0505) | Steps 646(649.63) | Grad Norm 1.6706(4.1872) | Total Time 10.00(10.00)
Iter 12810 | Time 5.3935(4.8520) | Bit/dim 1.0488(1.0478) | Steps 664(649.82) | Grad Norm 13.1814(4.3446) | Total Time 10.00(10.00)
Iter 12820 | Time 4.5739(4.8241) | Bit/dim 1.1132(1.0494) | Steps 628(647.05) | Grad Norm 8.6405(5.8833) | Total Time 10.00(10.00)
Iter 12830 | Time 5.0079(4.8187) | Bit/dim 1.0334(1.0503) | Steps 652(647.64) | Grad Norm 7.9546(6.2873) | Total Time 10.00(10.00)
Iter 12840 | Time 5.1381(4.8308) | Bit/dim 1.0282(1.0472) | Steps 646(648.18) | Grad Norm 3.4801(5.7537) | Total Time 10.00(10.00)
Iter 12850 | Time 4.7739(4.8253) | Bit/dim 1.0569(1.0428) | Steps 634(647.47) | Grad Norm 2.7940(5.0445) | Total Time 10.00(10.00)
Iter 12860 | Time 4.6594(4.8109) | Bit/dim 1.0275(1.0380) | Steps 646(648.54) | Grad Norm 3.3932(4.4496) | Total Time 10.00(10.00)
Iter 12870 | Time 4.8429(4.8129) | Bit/dim 1.0238(1.0321) | Steps 646(648.71) | Grad Norm 2.7576(4.1717) | Total Time 10.00(10.00)
Iter 12880 | Time 4.4772(4.8305) | Bit/dim 1.0212(1.0351) | Steps 634(649.21) | Grad Norm 3.8038(5.3044) | Total Time 10.00(10.00)
Iter 12890 | Time 5.4932(4.8552) | Bit/dim 1.0345(1.0358) | Steps 694(652.19) | Grad Norm 9.8434(5.5420) | Total Time 10.00(10.00)
validating...
Epoch 0043 | Time 62.2330, Bit/dim 1.0173
===> Using batch size 200. Total 300 iterations/epoch.
Iter 12900 | Time 4.7237(4.8413) | Bit/dim 1.0102(1.0309) | Steps 652(653.02) | Grad Norm 3.8169(5.3454) | Total Time 10.00(10.00)
Iter 12910 | Time 5.4209(4.8768) | Bit/dim 1.0242(1.0272) | Steps 670(654.53) | Grad Norm 10.5694(5.1993) | Total Time 10.00(10.00)
Iter 12920 | Time 5.1105(4.8664) | Bit/dim 1.0373(1.0321) | Steps 664(653.79) | Grad Norm 6.7577(6.2188) | Total Time 10.00(10.00)
Iter 12930 | Time 4.7435(4.8833) | Bit/dim 1.0845(1.0379) | Steps 652(655.88) | Grad Norm 6.1707(6.7191) | Total Time 10.00(10.00)
Iter 12940 | Time 4.8622(4.8777) | Bit/dim 1.0314(1.0348) | Steps 658(655.59) | Grad Norm 3.4353(6.0828) | Total Time 10.00(10.00)
Iter 12950 | Time 4.8597(4.9032) | Bit/dim 1.0211(1.0299) | Steps 652(655.75) | Grad Norm 1.0073(5.1666) | Total Time 10.00(10.00)
Iter 12960 | Time 4.5846(4.9046) | Bit/dim 1.0166(1.0276) | Steps 646(656.50) | Grad Norm 5.9242(4.7232) | Total Time 10.00(10.00)
Iter 12970 | Time 4.6413(4.9194) | Bit/dim 1.0371(1.0266) | Steps 658(658.61) | Grad Norm 5.6526(5.2371) | Total Time 10.00(10.00)
Iter 12980 | Time 4.8439(4.9340) | Bit/dim 1.0239(1.0255) | Steps 652(659.79) | Grad Norm 3.0871(5.3412) | Total Time 10.00(10.00)
Iter 12990 | Time 5.4210(4.9455) | Bit/dim 1.0313(1.0219) | Steps 706(661.85) | Grad Norm 13.2501(5.6527) | Total Time 10.00(10.00)
Iter 13000 | Time 4.7556(4.9410) | Bit/dim 1.0551(1.0242) | Steps 658(662.17) | Grad Norm 3.1189(5.3398) | Total Time 10.00(10.00)
Iter 13010 | Time 4.5277(4.9253) | Bit/dim 1.0331(1.0234) | Steps 634(661.72) | Grad Norm 8.0683(5.1935) | Total Time 10.00(10.00)
Iter 13020 | Time 4.9575(4.9408) | Bit/dim 1.0393(1.0299) | Steps 670(661.55) | Grad Norm 7.2377(6.2898) | Total Time 10.00(10.00)
Iter 13030 | Time 4.6121(4.9183) | Bit/dim 1.0388(1.0317) | Steps 652(659.63) | Grad Norm 7.4929(6.5123) | Total Time 10.00(10.00)
Iter 13040 | Time 4.6690(4.9001) | Bit/dim 1.0509(1.0358) | Steps 640(658.02) | Grad Norm 6.9286(6.9558) | Total Time 10.00(10.00)
Iter 13050 | Time 4.8731(4.8904) | Bit/dim 1.0352(1.0371) | Steps 652(657.08) | Grad Norm 3.0985(6.3508) | Total Time 10.00(10.00)
Iter 13060 | Time 5.1425(4.9452) | Bit/dim 1.0195(1.0344) | Steps 664(658.92) | Grad Norm 3.3790(5.4535) | Total Time 10.00(10.00)
Iter 13070 | Time 4.7777(4.9271) | Bit/dim 1.0014(1.0334) | Steps 634(656.98) | Grad Norm 2.3308(4.5935) | Total Time 10.00(10.00)
Iter 13080 | Time 4.8124(4.9049) | Bit/dim 1.0073(1.0314) | Steps 664(655.42) | Grad Norm 3.8686(4.0213) | Total Time 10.00(10.00)
Iter 13090 | Time 5.2369(4.9345) | Bit/dim 1.0777(1.0351) | Steps 694(658.24) | Grad Norm 5.8306(5.1237) | Total Time 10.00(10.00)
Iter 13100 | Time 4.5891(4.9515) | Bit/dim 1.0773(1.0468) | Steps 634(659.73) | Grad Norm 6.2838(6.2704) | Total Time 10.00(10.00)
Iter 13110 | Time 5.1322(4.9590) | Bit/dim 1.0530(1.0478) | Steps 664(659.60) | Grad Norm 5.3653(5.8689) | Total Time 10.00(10.00)
Iter 13120 | Time 5.1340(5.0144) | Bit/dim 1.0388(1.0429) | Steps 658(658.91) | Grad Norm 1.8407(4.8674) | Total Time 10.00(10.00)
Iter 13130 | Time 4.9159(4.9852) | Bit/dim 1.0135(1.0380) | Steps 658(658.24) | Grad Norm 0.9101(4.0370) | Total Time 10.00(10.00)
Iter 13140 | Time 4.9225(4.9592) | Bit/dim 0.9924(1.0327) | Steps 658(658.51) | Grad Norm 1.2061(3.3746) | Total Time 10.00(10.00)
Iter 13150 | Time 4.9861(4.9452) | Bit/dim 1.0745(1.0327) | Steps 652(659.36) | Grad Norm 10.8209(4.1829) | Total Time 10.00(10.00)
Iter 13160 | Time 5.3030(4.9383) | Bit/dim 1.0813(1.0453) | Steps 682(659.34) | Grad Norm 13.6898(5.5294) | Total Time 10.00(10.00)
Iter 13170 | Time 4.9936(4.9185) | Bit/dim 1.0465(1.0462) | Steps 676(658.05) | Grad Norm 8.1227(5.4305) | Total Time 10.00(10.00)
Iter 13180 | Time 4.9195(4.9382) | Bit/dim 1.0276(1.0430) | Steps 670(661.19) | Grad Norm 2.0683(4.7324) | Total Time 10.00(10.00)
Iter 13190 | Time 4.8180(4.9273) | Bit/dim 1.0113(1.0372) | Steps 652(660.57) | Grad Norm 1.3969(3.9087) | Total Time 10.00(10.00)
validating...
Epoch 0044 | Time 62.0464, Bit/dim 1.0146
===> Using batch size 200. Total 300 iterations/epoch.
Iter 13200 | Time 4.9138(4.9119) | Bit/dim 1.0198(1.0310) | Steps 652(658.38) | Grad Norm 1.0292(3.4803) | Total Time 10.00(10.00)
Iter 13210 | Time 4.6868(4.8897) | Bit/dim 1.0067(1.0264) | Steps 646(655.35) | Grad Norm 4.6811(3.1182) | Total Time 10.00(10.00)
Iter 13220 | Time 5.3290(4.8759) | Bit/dim 1.0068(1.0239) | Steps 664(653.11) | Grad Norm 9.9317(3.4630) | Total Time 10.00(10.00)
Iter 13230 | Time 4.6789(4.9102) | Bit/dim 1.0666(1.0394) | Steps 640(653.57) | Grad Norm 6.4419(5.3230) | Total Time 10.00(10.00)
Iter 13240 | Time 5.3621(4.9011) | Bit/dim 1.0371(1.0438) | Steps 688(655.10) | Grad Norm 10.6425(5.7073) | Total Time 10.00(10.00)
Iter 13250 | Time 4.7006(4.8769) | Bit/dim 1.0504(1.0413) | Steps 652(653.88) | Grad Norm 3.5025(5.3638) | Total Time 10.00(10.00)
Iter 13260 | Time 5.0927(4.9021) | Bit/dim 1.0077(1.0344) | Steps 652(653.28) | Grad Norm 4.1826(4.8656) | Total Time 10.00(10.00)
Iter 13270 | Time 4.8934(4.8916) | Bit/dim 1.0249(1.0309) | Steps 658(653.13) | Grad Norm 1.0266(3.9629) | Total Time 10.00(10.00)
Iter 13280 | Time 5.3974(4.9308) | Bit/dim 1.0481(1.0280) | Steps 670(655.49) | Grad Norm 21.7468(4.8447) | Total Time 10.00(10.00)
Iter 13290 | Time 4.9588(4.9330) | Bit/dim 1.0336(1.0310) | Steps 670(656.59) | Grad Norm 8.4875(5.6783) | Total Time 10.00(10.00)
Iter 13300 | Time 4.5682(4.8953) | Bit/dim 1.0496(1.0313) | Steps 646(655.39) | Grad Norm 5.9275(5.9717) | Total Time 10.00(10.00)
Iter 13310 | Time 5.1805(4.8797) | Bit/dim 1.0382(1.0323) | Steps 664(654.19) | Grad Norm 2.9547(5.5222) | Total Time 10.00(10.00)
Iter 13320 | Time 4.8628(4.8931) | Bit/dim 1.0324(1.0290) | Steps 646(654.65) | Grad Norm 1.4764(4.8900) | Total Time 10.00(10.00)
Iter 13330 | Time 5.0846(4.9102) | Bit/dim 1.0236(1.0255) | Steps 670(656.46) | Grad Norm 6.3663(5.5151) | Total Time 10.00(10.00)
Iter 13340 | Time 4.9121(4.9189) | Bit/dim 1.0155(1.0243) | Steps 658(657.86) | Grad Norm 4.0466(5.3412) | Total Time 10.00(10.00)
Iter 13350 | Time 4.7053(4.9428) | Bit/dim 1.0150(1.0241) | Steps 646(660.59) | Grad Norm 7.2337(6.0445) | Total Time 10.00(10.00)
Iter 13360 | Time 5.2045(4.9775) | Bit/dim 1.0349(1.0247) | Steps 670(661.76) | Grad Norm 3.4365(6.1888) | Total Time 10.00(10.00)
Iter 13370 | Time 4.8710(4.9688) | Bit/dim 1.0236(1.0238) | Steps 652(661.30) | Grad Norm 5.4840(5.9486) | Total Time 10.00(10.00)
Iter 13380 | Time 4.8839(4.9687) | Bit/dim 0.9998(1.0237) | Steps 658(660.69) | Grad Norm 4.8076(5.4339) | Total Time 10.00(10.00)
Iter 13390 | Time 4.7606(4.9801) | Bit/dim 1.0156(1.0238) | Steps 652(661.50) | Grad Norm 3.6735(5.4633) | Total Time 10.00(10.00)
Iter 13400 | Time 5.5541(5.0035) | Bit/dim 1.0247(1.0237) | Steps 706(663.66) | Grad Norm 10.7149(5.6258) | Total Time 10.00(10.00)
Iter 13410 | Time 5.2897(4.9639) | Bit/dim 1.0463(1.0251) | Steps 682(661.93) | Grad Norm 11.4172(6.1667) | Total Time 10.00(10.00)
Iter 13420 | Time 5.3306(4.9950) | Bit/dim 1.0087(1.0249) | Steps 682(663.33) | Grad Norm 12.6763(5.8995) | Total Time 10.00(10.00)
Iter 13430 | Time 5.1070(4.9798) | Bit/dim 1.0132(1.0242) | Steps 676(662.73) | Grad Norm 5.2660(5.8133) | Total Time 10.00(10.00)
Iter 13440 | Time 4.7327(4.9797) | Bit/dim 1.0162(1.0229) | Steps 652(662.36) | Grad Norm 9.8398(5.7170) | Total Time 10.00(10.00)
Iter 13450 | Time 4.9355(4.9743) | Bit/dim 1.0060(1.0233) | Steps 658(663.14) | Grad Norm 3.1457(6.0759) | Total Time 10.00(10.00)
Iter 13460 | Time 4.6294(4.9763) | Bit/dim 1.0560(1.0336) | Steps 652(664.59) | Grad Norm 4.1693(6.6520) | Total Time 10.00(10.00)
Iter 13470 | Time 4.9394(4.9639) | Bit/dim 1.0507(1.0342) | Steps 664(663.02) | Grad Norm 6.8336(6.1300) | Total Time 10.00(10.00)
Iter 13480 | Time 4.5715(4.9699) | Bit/dim 1.0133(1.0325) | Steps 646(662.48) | Grad Norm 3.2117(5.2471) | Total Time 10.00(10.00)
Iter 13490 | Time 4.6908(4.9790) | Bit/dim 1.0028(1.0312) | Steps 640(663.64) | Grad Norm 6.1212(5.0365) | Total Time 10.00(10.00)
validating...
Epoch 0045 | Time 70.1803, Bit/dim 1.0608
===> Using batch size 200. Total 300 iterations/epoch.
Iter 13500 | Time 5.5959(4.9951) | Bit/dim 1.0868(1.0385) | Steps 706(665.83) | Grad Norm 15.7043(6.4394) | Total Time 10.00(10.00)
Iter 13510 | Time 5.1914(4.9912) | Bit/dim 1.0109(1.0363) | Steps 676(666.69) | Grad Norm 7.1761(6.0117) | Total Time 10.00(10.00)
Iter 13520 | Time 5.0520(4.9807) | Bit/dim 1.0081(1.0354) | Steps 676(665.88) | Grad Norm 3.2485(5.5370) | Total Time 10.00(10.00)
Iter 13530 | Time 4.8997(4.9947) | Bit/dim 1.0318(1.0325) | Steps 658(666.81) | Grad Norm 1.9614(5.0149) | Total Time 10.00(10.00)
Iter 13540 | Time 5.2215(5.0154) | Bit/dim 1.0556(1.0321) | Steps 676(669.46) | Grad Norm 17.8908(5.2047) | Total Time 10.00(10.00)
Iter 13550 | Time 5.5122(5.0299) | Bit/dim 1.0514(1.0415) | Steps 700(670.42) | Grad Norm 12.2868(6.3515) | Total Time 10.00(10.00)
Iter 13560 | Time 5.2973(5.0444) | Bit/dim 1.0479(1.0426) | Steps 694(670.81) | Grad Norm 3.8202(5.8617) | Total Time 10.00(10.00)
Iter 13570 | Time 4.9512(5.0799) | Bit/dim 1.0229(1.0368) | Steps 658(671.51) | Grad Norm 2.2394(4.9725) | Total Time 10.00(10.00)
Iter 13580 | Time 4.8494(5.0722) | Bit/dim 1.0467(1.0334) | Steps 652(671.88) | Grad Norm 2.8303(4.2234) | Total Time 10.00(10.00)
Iter 13590 | Time 5.0112(5.0774) | Bit/dim 1.0405(1.0342) | Steps 676(672.25) | Grad Norm 7.4160(5.3839) | Total Time 10.00(10.00)
Iter 13600 | Time 4.5665(5.0307) | Bit/dim 1.0321(1.0338) | Steps 646(669.63) | Grad Norm 5.2191(5.3112) | Total Time 10.00(10.00)
Iter 13610 | Time 4.8791(5.0287) | Bit/dim 1.0081(1.0312) | Steps 658(669.71) | Grad Norm 3.1243(4.9688) | Total Time 10.00(10.00)
Iter 13620 | Time 4.8513(5.0398) | Bit/dim 1.0254(1.0300) | Steps 646(668.35) | Grad Norm 6.5844(4.5863) | Total Time 10.00(10.00)
Iter 13630 | Time 4.7700(5.0345) | Bit/dim 1.0115(1.0282) | Steps 652(668.36) | Grad Norm 6.2110(5.1660) | Total Time 10.00(10.00)
Iter 13640 | Time 4.9396(5.0482) | Bit/dim 1.0056(1.0239) | Steps 664(669.16) | Grad Norm 3.7984(4.8432) | Total Time 10.00(10.00)
Iter 13650 | Time 4.5952(5.0312) | Bit/dim 1.0631(1.0242) | Steps 646(668.74) | Grad Norm 9.0886(5.5408) | Total Time 10.00(10.00)
Iter 13660 | Time 4.5926(5.0231) | Bit/dim 1.0380(1.0259) | Steps 646(670.32) | Grad Norm 4.0633(6.0845) | Total Time 10.00(10.00)
Iter 13670 | Time 5.8785(5.0518) | Bit/dim 1.0288(1.0266) | Steps 730(672.68) | Grad Norm 12.2226(6.3043) | Total Time 10.00(10.00)
Iter 13680 | Time 4.5730(5.0484) | Bit/dim 1.0107(1.0255) | Steps 652(673.47) | Grad Norm 3.1337(6.0529) | Total Time 10.00(10.00)
Iter 13690 | Time 5.2029(5.0504) | Bit/dim 0.9985(1.0219) | Steps 682(672.42) | Grad Norm 5.5431(5.5061) | Total Time 10.00(10.00)
Iter 13700 | Time 4.9235(5.0557) | Bit/dim 1.0060(1.0213) | Steps 658(671.99) | Grad Norm 2.1837(4.9722) | Total Time 10.00(10.00)
Iter 13710 | Time 4.7776(5.0697) | Bit/dim 1.0133(1.0219) | Steps 658(673.62) | Grad Norm 6.8958(5.6396) | Total Time 10.00(10.00)
Iter 13720 | Time 5.8169(5.0709) | Bit/dim 1.0485(1.0259) | Steps 724(675.25) | Grad Norm 10.9877(6.4076) | Total Time 10.00(10.00)
Iter 13730 | Time 5.2720(5.0484) | Bit/dim 1.0002(1.0240) | Steps 682(673.41) | Grad Norm 2.6941(5.7996) | Total Time 10.00(10.00)
Iter 13740 | Time 5.2874(5.0203) | Bit/dim 1.0238(1.0226) | Steps 670(671.01) | Grad Norm 1.5763(5.4281) | Total Time 10.00(10.00)
Iter 13750 | Time 4.9362(5.0320) | Bit/dim 1.0099(1.0231) | Steps 664(671.11) | Grad Norm 7.9407(5.3549) | Total Time 10.00(10.00)
Iter 13760 | Time 5.1284(5.0564) | Bit/dim 1.0344(1.0220) | Steps 688(673.97) | Grad Norm 9.1916(5.9374) | Total Time 10.00(10.00)
Iter 13770 | Time 4.9670(5.0603) | Bit/dim 1.0422(1.0221) | Steps 670(674.15) | Grad Norm 8.0708(6.3565) | Total Time 10.00(10.00)
Iter 13780 | Time 4.6511(5.0981) | Bit/dim 1.0270(1.0274) | Steps 652(676.29) | Grad Norm 5.5500(6.6920) | Total Time 10.00(10.00)
Iter 13790 | Time 4.6183(5.1087) | Bit/dim 1.0116(1.0279) | Steps 652(677.24) | Grad Norm 5.1975(6.1390) | Total Time 10.00(10.00)
validating...
Epoch 0046 | Time 66.9816, Bit/dim 1.0222
===> Using batch size 200. Total 300 iterations/epoch.
Iter 13800 | Time 5.0930(5.0851) | Bit/dim 1.0456(1.0288) | Steps 688(675.51) | Grad Norm 8.1432(5.7015) | Total Time 10.00(10.00)
Iter 13810 | Time 4.9784(5.0709) | Bit/dim 1.0254(1.0269) | Steps 670(675.43) | Grad Norm 2.2237(5.1972) | Total Time 10.00(10.00)
Iter 13820 | Time 4.9898(5.0578) | Bit/dim 1.0235(1.0262) | Steps 664(674.14) | Grad Norm 1.8297(5.2873) | Total Time 10.00(10.00)
Iter 13830 | Time 5.4550(5.0790) | Bit/dim 1.0395(1.0225) | Steps 700(675.16) | Grad Norm 13.9440(5.3944) | Total Time 10.00(10.00)
Iter 13840 | Time 4.9308(5.0772) | Bit/dim 1.0290(1.0229) | Steps 664(676.58) | Grad Norm 5.6506(5.7766) | Total Time 10.00(10.00)
Iter 13850 | Time 4.6292(5.0601) | Bit/dim 1.0365(1.0238) | Steps 652(676.02) | Grad Norm 5.7448(5.9860) | Total Time 10.00(10.00)
Iter 13860 | Time 4.9979(5.0316) | Bit/dim 1.0360(1.0225) | Steps 670(673.84) | Grad Norm 4.7636(5.8117) | Total Time 10.00(10.00)
Iter 13870 | Time 4.6249(5.0475) | Bit/dim 1.0010(1.0182) | Steps 652(674.14) | Grad Norm 5.2189(5.3640) | Total Time 10.00(10.00)
Iter 13880 | Time 4.8578(5.0674) | Bit/dim 1.0209(1.0202) | Steps 652(674.75) | Grad Norm 4.6821(6.3822) | Total Time 10.00(10.00)
Iter 13890 | Time 4.9868(5.0529) | Bit/dim 1.0315(1.0236) | Steps 670(674.51) | Grad Norm 4.7666(5.9882) | Total Time 10.00(10.00)
Iter 13900 | Time 4.9848(5.0763) | Bit/dim 1.0182(1.0233) | Steps 670(676.36) | Grad Norm 3.3989(5.4592) | Total Time 10.00(10.00)
Iter 13910 | Time 5.1195(5.1023) | Bit/dim 1.0363(1.0243) | Steps 694(677.97) | Grad Norm 2.2208(4.6922) | Total Time 10.00(10.00)
Iter 13920 | Time 5.4038(5.1160) | Bit/dim 1.0653(1.0223) | Steps 700(679.65) | Grad Norm 1.2735(4.2511) | Total Time 10.00(10.00)
Iter 13930 | Time 5.0992(5.1051) | Bit/dim 1.0012(1.0198) | Steps 670(676.29) | Grad Norm 3.6145(4.1523) | Total Time 10.00(10.00)
Iter 13940 | Time 5.5533(5.1173) | Bit/dim 1.0096(1.0202) | Steps 706(675.88) | Grad Norm 19.6769(5.3284) | Total Time 10.00(10.00)
Iter 13950 | Time 4.9725(5.1089) | Bit/dim 1.0869(1.0387) | Steps 664(676.22) | Grad Norm 4.4783(6.3613) | Total Time 10.00(10.00)
Iter 13960 | Time 5.2600(5.1743) | Bit/dim 1.0401(1.0424) | Steps 676(679.90) | Grad Norm 5.6994(6.0487) | Total Time 10.00(10.00)
Iter 13970 | Time 5.4264(5.2034) | Bit/dim 1.0615(1.0409) | Steps 706(681.41) | Grad Norm 3.1838(5.1369) | Total Time 10.00(10.00)
Iter 13980 | Time 5.1980(5.2004) | Bit/dim 1.0286(1.0362) | Steps 664(681.80) | Grad Norm 2.7199(4.5291) | Total Time 10.00(10.00)
Iter 13990 | Time 4.9233(5.1671) | Bit/dim 1.0389(1.0320) | Steps 664(678.72) | Grad Norm 2.1819(4.8508) | Total Time 10.00(10.00)
Iter 14000 | Time 4.8300(5.1063) | Bit/dim 1.0290(1.0288) | Steps 664(675.01) | Grad Norm 5.4698(5.4545) | Total Time 10.00(10.00)
Iter 14010 | Time 4.9871(5.0994) | Bit/dim 1.0367(1.0249) | Steps 670(675.59) | Grad Norm 2.6576(5.1757) | Total Time 10.00(10.00)
Iter 14020 | Time 4.7312(5.0716) | Bit/dim 1.0467(1.0248) | Steps 652(674.01) | Grad Norm 8.2272(6.0222) | Total Time 10.00(10.00)
Iter 14030 | Time 5.2777(5.0573) | Bit/dim 1.0050(1.0254) | Steps 682(674.29) | Grad Norm 1.9047(6.1037) | Total Time 10.00(10.00)
Iter 14040 | Time 5.1248(5.0447) | Bit/dim 1.0256(1.0283) | Steps 694(674.86) | Grad Norm 6.1263(6.3884) | Total Time 10.00(10.00)
Iter 14050 | Time 5.2594(5.0788) | Bit/dim 0.9780(1.0267) | Steps 676(675.59) | Grad Norm 2.9482(5.9487) | Total Time 10.00(10.00)
Iter 14060 | Time 5.4554(5.1008) | Bit/dim 1.0380(1.0256) | Steps 700(677.61) | Grad Norm 3.9222(5.3416) | Total Time 10.00(10.00)
Iter 14070 | Time 5.4364(5.1564) | Bit/dim 1.0083(1.0218) | Steps 700(681.36) | Grad Norm 4.4075(4.6463) | Total Time 10.00(10.00)
Iter 14080 | Time 5.3176(5.1632) | Bit/dim 1.0349(1.0232) | Steps 682(682.05) | Grad Norm 9.9926(5.5235) | Total Time 10.00(10.00)
Iter 14090 | Time 5.6718(5.1676) | Bit/dim 1.0482(1.0334) | Steps 706(682.97) | Grad Norm 13.3496(6.2974) | Total Time 10.00(10.00)
validating...
Epoch 0047 | Time 68.3328, Bit/dim 1.0269
===> Using batch size 200. Total 300 iterations/epoch.
Iter 14100 | Time 5.3499(5.1548) | Bit/dim 1.0133(1.0334) | Steps 700(681.56) | Grad Norm 3.3341(5.7766) | Total Time 10.00(10.00)
Iter 14110 | Time 4.9976(5.1666) | Bit/dim 1.0041(1.0306) | Steps 670(681.03) | Grad Norm 2.7919(5.1619) | Total Time 10.00(10.00)
Iter 14120 | Time 5.1380(5.1727) | Bit/dim 1.0056(1.0258) | Steps 694(681.48) | Grad Norm 6.4372(4.8963) | Total Time 10.00(10.00)
Iter 14130 | Time 5.4508(5.1783) | Bit/dim 1.0241(1.0223) | Steps 700(679.80) | Grad Norm 2.5708(4.4148) | Total Time 10.00(10.00)
Iter 14140 | Time 5.0066(5.1567) | Bit/dim 1.0084(1.0200) | Steps 664(676.69) | Grad Norm 3.3538(3.9165) | Total Time 10.00(10.00)
Iter 14150 | Time 4.9750(5.1306) | Bit/dim 1.0002(1.0144) | Steps 664(674.45) | Grad Norm 1.9320(3.5003) | Total Time 10.00(10.00)
Iter 14160 | Time 5.0180(5.1258) | Bit/dim 1.0366(1.0145) | Steps 664(674.56) | Grad Norm 9.6977(3.4569) | Total Time 10.00(10.00)
Iter 14170 | Time 5.2553(5.1212) | Bit/dim 1.0742(1.0340) | Steps 676(673.87) | Grad Norm 3.0911(4.8307) | Total Time 10.00(10.00)
Iter 14180 | Time 5.2726(5.1359) | Bit/dim 1.0119(1.0376) | Steps 688(676.85) | Grad Norm 3.4440(4.9031) | Total Time 10.00(10.00)
Iter 14190 | Time 5.3083(5.1477) | Bit/dim 1.0063(1.0341) | Steps 676(676.94) | Grad Norm 1.3514(4.3023) | Total Time 10.00(10.00)
Iter 14200 | Time 5.0153(5.1558) | Bit/dim 0.9878(1.0297) | Steps 670(678.13) | Grad Norm 3.1520(4.0890) | Total Time 10.00(10.00)
Iter 14210 | Time 5.3783(5.1129) | Bit/dim 1.0556(1.0401) | Steps 700(675.27) | Grad Norm 4.7203(5.6268) | Total Time 10.00(10.00)
Iter 14220 | Time 4.9397(5.1105) | Bit/dim 1.0400(1.0417) | Steps 664(676.17) | Grad Norm 3.1004(5.5575) | Total Time 10.00(10.00)
Iter 14230 | Time 5.0148(5.1232) | Bit/dim 1.0381(1.0367) | Steps 670(677.51) | Grad Norm 2.6763(4.9917) | Total Time 10.00(10.00)
Iter 14240 | Time 5.0202(5.1336) | Bit/dim 1.0205(1.0327) | Steps 670(678.60) | Grad Norm 3.7953(4.4069) | Total Time 10.00(10.00)
Iter 14250 | Time 5.2911(5.1421) | Bit/dim 1.0598(1.0288) | Steps 676(676.73) | Grad Norm 1.8011(3.8864) | Total Time 10.00(10.00)
Iter 14260 | Time 5.0961(5.1451) | Bit/dim 1.0521(1.0262) | Steps 670(674.96) | Grad Norm 8.6744(4.6631) | Total Time 10.00(10.00)
Iter 14270 | Time 4.7342(5.1413) | Bit/dim 1.0211(1.0341) | Steps 652(676.69) | Grad Norm 5.1818(5.8914) | Total Time 10.00(10.00)
Iter 14280 | Time 5.4862(5.1582) | Bit/dim 0.9883(1.0387) | Steps 706(679.07) | Grad Norm 2.1651(5.6216) | Total Time 10.00(10.00)
Iter 14290 | Time 5.4318(5.2061) | Bit/dim 1.0076(1.0357) | Steps 700(682.04) | Grad Norm 1.5628(4.7313) | Total Time 10.00(10.00)
Iter 14300 | Time 5.0348(5.2383) | Bit/dim 1.0262(1.0292) | Steps 676(682.82) | Grad Norm 3.1865(3.9532) | Total Time 10.00(10.00)
Iter 14310 | Time 4.9702(5.2273) | Bit/dim 1.0721(1.0292) | Steps 664(684.07) | Grad Norm 8.2847(4.5066) | Total Time 10.00(10.00)
Iter 14320 | Time 4.7764(5.2008) | Bit/dim 1.0467(1.0388) | Steps 664(682.76) | Grad Norm 6.0378(5.6045) | Total Time 10.00(10.00)
Iter 14330 | Time 5.3717(5.2037) | Bit/dim 1.0180(1.0383) | Steps 700(684.08) | Grad Norm 2.0565(5.3334) | Total Time 10.00(10.00)
Iter 14340 | Time 5.3107(5.2288) | Bit/dim 1.0294(1.0335) | Steps 682(684.22) | Grad Norm 1.4301(4.6472) | Total Time 10.00(10.00)
Iter 14350 | Time 5.0647(5.2560) | Bit/dim 0.9884(1.0302) | Steps 676(685.50) | Grad Norm 1.1339(3.8049) | Total Time 10.00(10.00)
Iter 14360 | Time 4.9529(5.2393) | Bit/dim 1.0207(1.0255) | Steps 664(684.93) | Grad Norm 3.8690(3.3578) | Total Time 10.00(10.00)
Iter 14370 | Time 5.0843(5.2118) | Bit/dim 0.9908(1.0220) | Steps 676(682.08) | Grad Norm 2.1188(3.2169) | Total Time 10.00(10.00)
Iter 14380 | Time 5.5021(5.1878) | Bit/dim 1.0449(1.0222) | Steps 688(679.89) | Grad Norm 7.1951(3.4368) | Total Time 10.00(10.00)
Iter 14390 | Time 4.8655(5.1605) | Bit/dim 1.0900(1.0305) | Steps 670(678.91) | Grad Norm 7.9996(5.2716) | Total Time 10.00(10.00)
validating...
Epoch 0048 | Time 65.9216, Bit/dim 1.0307
===> Using batch size 200. Total 300 iterations/epoch.
Iter 14400 | Time 5.5243(5.1814) | Bit/dim 1.0331(1.0337) | Steps 694(680.92) | Grad Norm 2.0787(5.1113) | Total Time 10.00(10.00)
Iter 14410 | Time 5.2989(5.1830) | Bit/dim 1.0310(1.0343) | Steps 700(681.89) | Grad Norm 5.4136(4.8453) | Total Time 10.00(10.00)
Iter 14420 | Time 5.3987(5.2082) | Bit/dim 1.0215(1.0304) | Steps 694(683.34) | Grad Norm 5.4003(4.3226) | Total Time 10.00(10.00)
Iter 14430 | Time 5.5104(5.1770) | Bit/dim 1.0352(1.0282) | Steps 712(681.84) | Grad Norm 13.5659(5.2893) | Total Time 10.00(10.00)
Iter 14440 | Time 4.8878(5.1298) | Bit/dim 1.0018(1.0281) | Steps 658(679.84) | Grad Norm 4.1536(5.7187) | Total Time 10.00(10.00)
Iter 14450 | Time 5.0280(5.1238) | Bit/dim 1.0394(1.0257) | Steps 682(677.71) | Grad Norm 7.6991(5.6259) | Total Time 10.00(10.00)
Iter 14460 | Time 5.2236(5.1440) | Bit/dim 1.0074(1.0222) | Steps 670(678.02) | Grad Norm 1.1477(4.9633) | Total Time 10.00(10.00)
Iter 14470 | Time 5.4689(5.1614) | Bit/dim 1.0009(1.0205) | Steps 706(679.37) | Grad Norm 3.9320(5.1770) | Total Time 10.00(10.00)
Iter 14480 | Time 5.4889(5.1818) | Bit/dim 1.0100(1.0184) | Steps 712(680.05) | Grad Norm 13.3297(4.9253) | Total Time 10.00(10.00)
Iter 14490 | Time 4.5585(5.1662) | Bit/dim 1.0318(1.0236) | Steps 646(679.35) | Grad Norm 6.9180(6.1638) | Total Time 10.00(10.00)
Iter 14500 | Time 5.2068(5.1226) | Bit/dim 1.0555(1.0309) | Steps 682(678.77) | Grad Norm 5.0845(6.1644) | Total Time 10.00(10.00)
Iter 14510 | Time 5.2103(5.1653) | Bit/dim 1.0012(1.0293) | Steps 682(679.16) | Grad Norm 3.2451(5.4165) | Total Time 10.00(10.00)
Iter 14520 | Time 5.3961(5.2026) | Bit/dim 1.0339(1.0267) | Steps 694(680.18) | Grad Norm 2.1620(4.6378) | Total Time 10.00(10.00)
Iter 14530 | Time 5.2644(5.2168) | Bit/dim 1.0150(1.0220) | Steps 688(681.40) | Grad Norm 6.1937(4.4305) | Total Time 10.00(10.00)
Iter 14540 | Time 5.6248(5.2073) | Bit/dim 1.0320(1.0207) | Steps 706(681.82) | Grad Norm 8.8357(5.1221) | Total Time 10.00(10.00)
Iter 14550 | Time 5.4367(5.1777) | Bit/dim 1.0170(1.0221) | Steps 700(681.08) | Grad Norm 4.4478(5.4036) | Total Time 10.00(10.00)
Iter 14560 | Time 4.9818(5.1644) | Bit/dim 1.0409(1.0194) | Steps 664(679.32) | Grad Norm 6.3427(5.5310) | Total Time 10.00(10.00)
Iter 14570 | Time 5.0597(5.1558) | Bit/dim 0.9841(1.0163) | Steps 676(678.96) | Grad Norm 3.6475(5.2800) | Total Time 10.00(10.00)
Iter 14580 | Time 5.4261(5.1963) | Bit/dim 1.0089(1.0132) | Steps 694(680.87) | Grad Norm 3.7816(4.7091) | Total Time 10.00(10.00)
Iter 14590 | Time 4.9993(5.1879) | Bit/dim 1.0315(1.0122) | Steps 670(679.49) | Grad Norm 4.8327(4.4457) | Total Time 10.00(10.00)
Iter 14600 | Time 4.6888(5.1834) | Bit/dim 1.0083(1.0116) | Steps 658(678.64) | Grad Norm 6.9541(4.2549) | Total Time 10.00(10.00)
Iter 14610 | Time 4.8482(5.1808) | Bit/dim 1.0253(1.0115) | Steps 664(678.76) | Grad Norm 4.6864(4.1856) | Total Time 10.00(10.00)
Iter 14620 | Time 5.0021(5.1976) | Bit/dim 1.1744(1.0345) | Steps 676(680.14) | Grad Norm 5.4263(5.7979) | Total Time 10.00(10.00)
Iter 14630 | Time 5.0148(5.1419) | Bit/dim 1.0529(1.0503) | Steps 670(679.72) | Grad Norm 2.9366(5.3859) | Total Time 10.00(10.00)
Iter 14640 | Time 4.9446(5.1189) | Bit/dim 1.0420(1.0481) | Steps 664(678.74) | Grad Norm 1.9876(5.3360) | Total Time 10.00(10.00)
Iter 14650 | Time 5.2480(5.1166) | Bit/dim 1.0076(1.0411) | Steps 676(676.35) | Grad Norm 2.4607(4.5667) | Total Time 10.00(10.00)
Iter 14660 | Time 5.0625(5.1627) | Bit/dim 1.0200(1.0333) | Steps 688(679.44) | Grad Norm 3.9802(3.9454) | Total Time 10.00(10.00)
Iter 14670 | Time 5.3992(5.1352) | Bit/dim 1.0132(1.0277) | Steps 694(678.89) | Grad Norm 1.1856(3.9627) | Total Time 10.00(10.00)
Iter 14680 | Time 5.1327(5.1316) | Bit/dim 1.0783(1.0432) | Steps 688(681.19) | Grad Norm 3.5835(5.3537) | Total Time 10.00(10.00)
Iter 14690 | Time 4.6912(5.1454) | Bit/dim 1.0513(1.0476) | Steps 664(684.18) | Grad Norm 3.6457(5.2265) | Total Time 10.00(10.00)
validating...
Epoch 0049 | Time 62.3462, Bit/dim 1.0172
===> Using batch size 200. Total 300 iterations/epoch.
Iter 14700 | Time 4.6206(5.1677) | Bit/dim 1.0241(1.0439) | Steps 652(685.76) | Grad Norm 2.7773(4.7188) | Total Time 10.00(10.00)
Iter 14710 | Time 4.9251(5.1797) | Bit/dim 1.0153(1.0371) | Steps 664(685.98) | Grad Norm 3.5128(4.2615) | Total Time 10.00(10.00)
Iter 14720 | Time 4.8676(5.1454) | Bit/dim 1.0030(1.0353) | Steps 652(683.08) | Grad Norm 4.2547(5.4383) | Total Time 10.00(10.00)
Iter 14730 | Time 5.4019(5.1369) | Bit/dim 1.0333(1.0298) | Steps 694(682.17) | Grad Norm 2.8913(5.2339) | Total Time 10.00(10.00)
Iter 14740 | Time 5.4140(5.1040) | Bit/dim 1.0158(1.0270) | Steps 694(680.38) | Grad Norm 7.1808(5.3463) | Total Time 10.00(10.00)
Iter 14750 | Time 5.3800(5.1651) | Bit/dim 1.0039(1.0216) | Steps 688(683.28) | Grad Norm 2.8011(4.7172) | Total Time 10.00(10.00)
Iter 14760 | Time 4.8420(5.1691) | Bit/dim 1.0039(1.0204) | Steps 646(681.42) | Grad Norm 7.2436(4.9211) | Total Time 10.00(10.00)
Iter 14770 | Time 5.3711(5.1786) | Bit/dim 1.0045(1.0239) | Steps 712(682.71) | Grad Norm 6.7018(5.7499) | Total Time 10.00(10.00)
Iter 14780 | Time 5.1713(5.1657) | Bit/dim 1.0136(1.0210) | Steps 688(682.69) | Grad Norm 7.1136(5.5514) | Total Time 10.00(10.00)
Iter 14790 | Time 4.6812(5.1442) | Bit/dim 1.0290(1.0203) | Steps 658(681.07) | Grad Norm 6.0257(5.4662) | Total Time 10.00(10.00)
Iter 14800 | Time 4.9957(5.1425) | Bit/dim 1.0070(1.0194) | Steps 670(680.56) | Grad Norm 5.5846(5.9717) | Total Time 10.00(10.00)
Iter 14810 | Time 5.3362(5.1047) | Bit/dim 0.9794(1.0183) | Steps 682(677.05) | Grad Norm 1.6203(5.9462) | Total Time 10.00(10.00)
Iter 14820 | Time 4.6233(5.1015) | Bit/dim 1.0240(1.0149) | Steps 646(674.13) | Grad Norm 6.6129(5.5975) | Total Time 10.00(10.00)
Iter 14830 | Time 4.9436(5.1066) | Bit/dim 1.0559(1.0270) | Steps 664(674.91) | Grad Norm 5.9806(6.8904) | Total Time 10.00(10.00)
Iter 14840 | Time 5.6321(5.1653) | Bit/dim 1.0439(1.0362) | Steps 712(679.43) | Grad Norm 4.1612(6.3465) | Total Time 10.00(10.00)
Iter 14850 | Time 5.3235(5.1966) | Bit/dim 1.0450(1.0341) | Steps 688(681.89) | Grad Norm 3.1251(5.4826) | Total Time 10.00(10.00)
Iter 14860 | Time 5.0932(5.2180) | Bit/dim 1.0065(1.0302) | Steps 688(684.58) | Grad Norm 1.7190(4.5583) | Total Time 10.00(10.00)
Iter 14870 | Time 5.4545(5.2388) | Bit/dim 1.0246(1.0267) | Steps 700(685.64) | Grad Norm 2.1357(3.7133) | Total Time 10.00(10.00)
Iter 14880 | Time 5.7202(5.2070) | Bit/dim 1.0649(1.0251) | Steps 706(682.96) | Grad Norm 20.2241(4.4149) | Total Time 10.00(10.00)
Iter 14890 | Time 5.3922(5.2129) | Bit/dim 1.0472(1.0370) | Steps 694(683.87) | Grad Norm 3.5916(5.5857) | Total Time 10.00(10.00)
Iter 14900 | Time 4.5586(5.1711) | Bit/dim 1.0403(1.0403) | Steps 646(682.96) | Grad Norm 4.7653(5.6577) | Total Time 10.00(10.00)
Iter 14910 | Time 5.4328(5.2151) | Bit/dim 1.0119(1.0366) | Steps 700(685.27) | Grad Norm 3.1404(5.0323) | Total Time 10.00(10.00)
Iter 14920 | Time 5.4243(5.2296) | Bit/dim 1.0395(1.0310) | Steps 694(683.51) | Grad Norm 2.3653(4.4233) | Total Time 10.00(10.00)
Iter 14930 | Time 5.3489(5.2238) | Bit/dim 0.9940(1.0263) | Steps 682(680.65) | Grad Norm 4.1125(4.2520) | Total Time 10.00(10.00)
Iter 14940 | Time 5.1573(5.2097) | Bit/dim 1.0461(1.0255) | Steps 676(678.56) | Grad Norm 9.5072(4.8827) | Total Time 10.00(10.00)
Iter 14950 | Time 5.7206(5.2129) | Bit/dim 1.0820(1.0353) | Steps 706(679.28) | Grad Norm 13.1974(6.0577) | Total Time 10.00(10.00)
Iter 14960 | Time 5.2616(5.1589) | Bit/dim 1.0666(1.0388) | Steps 688(676.97) | Grad Norm 2.2931(5.7975) | Total Time 10.00(10.00)
Iter 14970 | Time 5.4133(5.1508) | Bit/dim 1.0236(1.0369) | Steps 694(677.89) | Grad Norm 4.3729(5.4792) | Total Time 10.00(10.00)
Iter 14980 | Time 5.4609(5.2056) | Bit/dim 1.0219(1.0309) | Steps 694(681.13) | Grad Norm 1.4436(4.6039) | Total Time 10.00(10.00)
Iter 14990 | Time 5.2593(5.2258) | Bit/dim 1.0080(1.0251) | Steps 688(681.45) | Grad Norm 5.9212(4.2856) | Total Time 10.00(10.00)
validating...
Epoch 0050 | Time 67.5067, Bit/dim 1.0323
===> Using batch size 200. Total 300 iterations/epoch.
Iter 15000 | Time 5.6017(5.2014) | Bit/dim 1.0326(1.0214) | Steps 706(680.58) | Grad Norm 17.9689(5.3866) | Total Time 10.00(10.00)
Iter 15010 | Time 5.9167(5.2275) | Bit/dim 1.0350(1.0251) | Steps 718(682.89) | Grad Norm 17.7730(6.0071) | Total Time 10.00(10.00)
Iter 15020 | Time 5.3216(5.1987) | Bit/dim 1.0607(1.0269) | Steps 694(681.44) | Grad Norm 5.0316(5.8117) | Total Time 10.00(10.00)
Iter 15030 | Time 5.3588(5.2456) | Bit/dim 0.9648(1.0225) | Steps 688(682.49) | Grad Norm 2.3800(4.9038) | Total Time 10.00(10.00)
Iter 15040 | Time 5.4077(5.2683) | Bit/dim 1.0263(1.0199) | Steps 694(682.00) | Grad Norm 1.2456(4.0431) | Total Time 10.00(10.00)
Iter 15050 | Time 5.3147(5.2847) | Bit/dim 1.0042(1.0151) | Steps 676(680.90) | Grad Norm 1.3346(3.3359) | Total Time 10.00(10.00)
Iter 15060 | Time 5.3384(5.2850) | Bit/dim 1.0066(1.0134) | Steps 676(680.43) | Grad Norm 2.4164(2.8609) | Total Time 10.00(10.00)
Iter 15070 | Time 4.6765(5.2329) | Bit/dim 1.0003(1.0123) | Steps 658(679.08) | Grad Norm 5.8136(2.9972) | Total Time 10.00(10.00)
Iter 15080 | Time 5.2151(5.2312) | Bit/dim 1.0774(1.0328) | Steps 682(679.91) | Grad Norm 2.5171(4.7071) | Total Time 10.00(10.00)
Iter 15090 | Time 5.0194(5.2258) | Bit/dim 1.0216(1.0402) | Steps 676(680.62) | Grad Norm 5.3764(4.9177) | Total Time 10.00(10.00)
Iter 15100 | Time 5.4017(5.2553) | Bit/dim 1.0318(1.0360) | Steps 694(682.94) | Grad Norm 3.8460(4.3892) | Total Time 10.00(10.00)
Iter 15110 | Time 5.4600(5.2656) | Bit/dim 1.0154(1.0303) | Steps 694(682.73) | Grad Norm 3.7581(4.0115) | Total Time 10.00(10.00)
Iter 15120 | Time 4.9454(5.2273) | Bit/dim 1.0455(1.0253) | Steps 664(678.82) | Grad Norm 7.4815(4.1135) | Total Time 10.00(10.00)
Iter 15130 | Time 4.6421(5.2059) | Bit/dim 1.0635(1.0307) | Steps 658(677.85) | Grad Norm 4.8171(5.2706) | Total Time 10.00(10.00)
Iter 15140 | Time 5.4501(5.2455) | Bit/dim 1.0223(1.0274) | Steps 700(681.38) | Grad Norm 3.8378(4.7128) | Total Time 10.00(10.00)
Iter 15150 | Time 5.3762(5.2180) | Bit/dim 1.0244(1.0263) | Steps 688(679.71) | Grad Norm 1.5955(4.8678) | Total Time 10.00(10.00)
Iter 15160 | Time 5.3722(5.1948) | Bit/dim 1.0041(1.0234) | Steps 682(678.21) | Grad Norm 1.3706(4.6312) | Total Time 10.00(10.00)
Iter 15170 | Time 5.3473(5.1991) | Bit/dim 1.0028(1.0184) | Steps 682(676.16) | Grad Norm 1.6520(4.2805) | Total Time 10.00(10.00)
Iter 15180 | Time 5.0722(5.2050) | Bit/dim 1.0389(1.0179) | Steps 676(675.56) | Grad Norm 2.8253(3.8831) | Total Time 10.00(10.00)
Iter 15190 | Time 5.0075(5.1891) | Bit/dim 1.0178(1.0172) | Steps 664(674.82) | Grad Norm 5.2963(4.6340) | Total Time 10.00(10.00)
Iter 15200 | Time 5.2924(5.1749) | Bit/dim 1.1170(1.0549) | Steps 718(680.77) | Grad Norm 1.9946(5.5656) | Total Time 10.00(10.00)
Iter 15210 | Time 5.2555(5.1896) | Bit/dim 1.0315(1.0582) | Steps 676(682.97) | Grad Norm 3.3621(5.1222) | Total Time 10.00(10.00)
Iter 15220 | Time 5.2969(5.2199) | Bit/dim 1.0536(1.0504) | Steps 682(683.56) | Grad Norm 2.6039(4.4149) | Total Time 10.00(10.00)
Iter 15230 | Time 4.9952(5.2175) | Bit/dim 0.9846(1.0404) | Steps 670(681.82) | Grad Norm 2.6444(3.6823) | Total Time 10.00(10.00)
Iter 15240 | Time 4.9870(5.1893) | Bit/dim 1.0138(1.0324) | Steps 664(679.60) | Grad Norm 1.0798(3.1777) | Total Time 10.00(10.00)
Iter 15250 | Time 4.8748(5.1405) | Bit/dim 1.0167(1.0249) | Steps 670(675.54) | Grad Norm 5.2732(3.0135) | Total Time 10.00(10.00)
Iter 15260 | Time 4.6791(5.1322) | Bit/dim 1.0533(1.0264) | Steps 658(675.75) | Grad Norm 8.9750(4.6573) | Total Time 10.00(10.00)
Iter 15270 | Time 5.0309(5.1237) | Bit/dim 1.0016(1.0298) | Steps 670(675.30) | Grad Norm 2.7990(5.0516) | Total Time 10.00(10.00)
Iter 15280 | Time 5.2345(5.1374) | Bit/dim 1.0147(1.0282) | Steps 682(675.47) | Grad Norm 7.6641(4.8927) | Total Time 10.00(10.00)
Iter 15290 | Time 4.9860(5.1350) | Bit/dim 1.0301(1.0244) | Steps 664(674.80) | Grad Norm 9.2659(4.8075) | Total Time 10.00(10.00)
validating...
Epoch 0051 | Time 66.1492, Bit/dim 1.0060
===> Using batch size 200. Total 300 iterations/epoch.
Iter 15300 | Time 5.0422(5.1191) | Bit/dim 1.0154(1.0200) | Steps 670(673.70) | Grad Norm 3.4855(5.2225) | Total Time 10.00(10.00)
Iter 15310 | Time 5.2458(5.1369) | Bit/dim 0.9950(1.0157) | Steps 670(672.90) | Grad Norm 1.2915(4.5136) | Total Time 10.00(10.00)
Iter 15320 | Time 5.2731(5.1497) | Bit/dim 1.1065(1.0240) | Steps 688(674.28) | Grad Norm 9.3568(5.8409) | Total Time 10.00(10.00)
Iter 15330 | Time 5.4112(5.1409) | Bit/dim 1.0236(1.0334) | Steps 688(677.27) | Grad Norm 9.1352(6.1681) | Total Time 10.00(10.00)
Iter 15340 | Time 5.4475(5.1827) | Bit/dim 1.0309(1.0310) | Steps 706(679.20) | Grad Norm 2.7037(5.5446) | Total Time 10.00(10.00)
Iter 15350 | Time 5.2582(5.2198) | Bit/dim 1.0204(1.0263) | Steps 676(681.29) | Grad Norm 2.5967(4.8629) | Total Time 10.00(10.00)
Iter 15360 | Time 5.4442(5.2665) | Bit/dim 1.0036(1.0208) | Steps 694(683.37) | Grad Norm 3.7242(4.1586) | Total Time 10.00(10.00)
Iter 15370 | Time 4.6688(5.2580) | Bit/dim 1.0423(1.0200) | Steps 664(683.75) | Grad Norm 8.5620(4.8454) | Total Time 10.00(10.00)
Iter 15380 | Time 4.6330(5.2204) | Bit/dim 1.0386(1.0239) | Steps 652(681.92) | Grad Norm 6.3798(5.6897) | Total Time 10.00(10.00)
Iter 15390 | Time 4.9076(5.1760) | Bit/dim 1.0625(1.0259) | Steps 664(679.36) | Grad Norm 6.2206(5.7150) | Total Time 10.00(10.00)
Iter 15400 | Time 5.4723(5.2204) | Bit/dim 1.0251(1.0211) | Steps 700(681.54) | Grad Norm 1.4068(5.2870) | Total Time 10.00(10.00)
Iter 15410 | Time 5.3664(5.2228) | Bit/dim 1.0195(1.0198) | Steps 688(680.95) | Grad Norm 4.0748(4.9941) | Total Time 10.00(10.00)
Iter 15420 | Time 5.4124(5.2549) | Bit/dim 0.9902(1.0170) | Steps 688(681.66) | Grad Norm 1.5145(4.6079) | Total Time 10.00(10.00)
Iter 15430 | Time 5.3852(5.2869) | Bit/dim 1.0202(1.0173) | Steps 688(682.66) | Grad Norm 0.9174(4.1108) | Total Time 10.00(10.00)
Iter 15440 | Time 5.4484(5.2909) | Bit/dim 1.0133(1.0142) | Steps 694(683.14) | Grad Norm 0.7914(3.9305) | Total Time 10.00(10.00)
Iter 15450 | Time 5.4135(5.3174) | Bit/dim 1.0560(1.0134) | Steps 694(684.30) | Grad Norm 1.2238(3.5983) | Total Time 10.00(10.00)
Iter 15460 | Time 5.5297(5.2842) | Bit/dim 1.0412(1.0216) | Steps 706(683.23) | Grad Norm 6.4997(4.8886) | Total Time 10.00(10.00)
Iter 15470 | Time 5.3872(5.2761) | Bit/dim 1.0309(1.0291) | Steps 694(685.38) | Grad Norm 2.3550(5.1176) | Total Time 10.00(10.00)
Iter 15480 | Time 5.3045(5.2564) | Bit/dim 1.0321(1.0277) | Steps 682(685.27) | Grad Norm 4.4353(5.1616) | Total Time 10.00(10.00)
Iter 15490 | Time 5.3580(5.2799) | Bit/dim 0.9840(1.0219) | Steps 682(686.84) | Grad Norm 3.5226(4.6481) | Total Time 10.00(10.00)
Iter 15500 | Time 5.3731(5.2994) | Bit/dim 1.0268(1.0202) | Steps 688(686.80) | Grad Norm 1.1821(4.1857) | Total Time 10.00(10.00)
Iter 15510 | Time 5.3595(5.2410) | Bit/dim 1.0084(1.0169) | Steps 682(683.01) | Grad Norm 3.7064(4.9480) | Total Time 10.00(10.00)
Iter 15520 | Time 5.3519(5.2353) | Bit/dim 0.9779(1.0139) | Steps 682(682.25) | Grad Norm 9.0926(5.4769) | Total Time 10.00(10.00)
Iter 15530 | Time 5.4058(5.2206) | Bit/dim 1.0107(1.0161) | Steps 694(682.62) | Grad Norm 5.1088(5.5411) | Total Time 10.00(10.00)
Iter 15540 | Time 5.4253(5.2468) | Bit/dim 1.0309(1.0180) | Steps 694(683.65) | Grad Norm 1.4406(5.2436) | Total Time 10.00(10.00)
Iter 15550 | Time 4.6323(5.2025) | Bit/dim 1.0293(1.0186) | Steps 658(681.42) | Grad Norm 6.8460(6.1910) | Total Time 10.00(10.00)
Iter 15560 | Time 5.4539(5.2483) | Bit/dim 1.0096(1.0159) | Steps 700(683.86) | Grad Norm 1.6041(5.7410) | Total Time 10.00(10.00)
Iter 15570 | Time 5.5306(5.2806) | Bit/dim 1.0111(1.0138) | Steps 706(685.24) | Grad Norm 1.4672(5.2378) | Total Time 10.00(10.00)
Iter 15580 | Time 5.4462(5.3151) | Bit/dim 0.9959(1.0105) | Steps 700(686.91) | Grad Norm 2.1527(4.4943) | Total Time 10.00(10.00)
Iter 15590 | Time 5.3547(5.3247) | Bit/dim 1.0045(1.0092) | Steps 682(686.51) | Grad Norm 1.3209(4.0794) | Total Time 10.00(10.00)
validating...
Epoch 0052 | Time 61.2538, Bit/dim 1.0167
===> Using batch size 200. Total 300 iterations/epoch.
Iter 15600 | Time 4.9776(5.3071) | Bit/dim 1.0206(1.0110) | Steps 664(684.57) | Grad Norm 8.3665(4.3537) | Total Time 10.00(10.00)
Iter 15610 | Time 5.4548(5.2660) | Bit/dim 1.1177(1.0378) | Steps 706(684.41) | Grad Norm 9.1678(5.4967) | Total Time 10.00(10.00)
Iter 15620 | Time 4.7548(5.2407) | Bit/dim 1.0387(1.0423) | Steps 676(685.82) | Grad Norm 4.5281(5.1322) | Total Time 10.00(10.00)
Iter 15630 | Time 5.6623(5.2839) | Bit/dim 1.0298(1.0390) | Steps 718(688.63) | Grad Norm 1.5635(4.3925) | Total Time 10.00(10.00)
Iter 15640 | Time 5.3131(5.2961) | Bit/dim 1.0137(1.0338) | Steps 682(689.08) | Grad Norm 1.4230(3.6310) | Total Time 10.00(10.00)
Iter 15650 | Time 5.0150(5.3052) | Bit/dim 0.9802(1.0252) | Steps 676(689.04) | Grad Norm 3.6447(3.1736) | Total Time 10.00(10.00)
Iter 15660 | Time 5.4457(5.3025) | Bit/dim 1.0232(1.0196) | Steps 700(688.00) | Grad Norm 2.3062(3.1914) | Total Time 10.00(10.00)
Iter 15670 | Time 4.9905(5.2573) | Bit/dim 1.1288(1.0435) | Steps 682(689.01) | Grad Norm 4.5506(4.8116) | Total Time 10.00(10.00)
Iter 15680 | Time 5.3242(5.2445) | Bit/dim 1.0618(1.0498) | Steps 688(689.97) | Grad Norm 4.2756(4.7584) | Total Time 10.00(10.00)
Iter 15690 | Time 5.4412(5.2977) | Bit/dim 1.0430(1.0459) | Steps 706(693.12) | Grad Norm 3.2968(4.1802) | Total Time 10.00(10.00)
Iter 15700 | Time 5.4275(5.3146) | Bit/dim 0.9975(1.0379) | Steps 700(693.48) | Grad Norm 4.3212(3.8447) | Total Time 10.00(10.00)
Iter 15710 | Time 5.2675(5.2929) | Bit/dim 1.0533(1.0355) | Steps 682(692.03) | Grad Norm 9.2667(4.7206) | Total Time 10.00(10.00)
Iter 15720 | Time 5.4606(5.2743) | Bit/dim 1.0871(1.0487) | Steps 688(692.28) | Grad Norm 8.4257(5.8216) | Total Time 10.00(10.00)
Iter 15730 | Time 5.2855(5.2845) | Bit/dim 1.0541(1.0477) | Steps 682(693.11) | Grad Norm 2.3623(5.4455) | Total Time 10.00(10.00)
Iter 15740 | Time 5.2688(5.3305) | Bit/dim 1.0070(1.0409) | Steps 700(695.52) | Grad Norm 3.5591(4.5720) | Total Time 10.00(10.00)
Iter 15750 | Time 5.3316(5.3401) | Bit/dim 1.0268(1.0315) | Steps 688(694.66) | Grad Norm 1.0802(3.8604) | Total Time 10.00(10.00)
Iter 15760 | Time 5.3957(5.3459) | Bit/dim 1.0057(1.0253) | Steps 700(695.07) | Grad Norm 4.1988(3.6511) | Total Time 10.00(10.00)
Iter 15770 | Time 5.4117(5.3064) | Bit/dim 1.0442(1.0272) | Steps 700(692.97) | Grad Norm 25.9113(5.3178) | Total Time 10.00(10.00)
Iter 15780 | Time 5.3804(5.2816) | Bit/dim 1.0267(1.0329) | Steps 706(692.78) | Grad Norm 3.8809(5.6936) | Total Time 10.00(10.00)
Iter 15790 | Time 4.9769(5.2751) | Bit/dim 1.0267(1.0316) | Steps 670(692.50) | Grad Norm 4.4055(5.6783) | Total Time 10.00(10.00)
Iter 15800 | Time 5.5902(5.3028) | Bit/dim 0.9962(1.0277) | Steps 706(693.11) | Grad Norm 1.4107(4.9320) | Total Time 10.00(10.00)
Iter 15810 | Time 5.2759(5.3197) | Bit/dim 1.0104(1.0219) | Steps 676(693.24) | Grad Norm 3.3511(4.3180) | Total Time 10.00(10.00)
Iter 15820 | Time 5.4244(5.3262) | Bit/dim 0.9927(1.0178) | Steps 700(691.95) | Grad Norm 2.0716(3.9446) | Total Time 10.00(10.00)
Iter 15830 | Time 5.0147(5.2940) | Bit/dim 1.0139(1.0161) | Steps 676(688.43) | Grad Norm 5.3584(4.8156) | Total Time 10.00(10.00)
Iter 15840 | Time 5.3958(5.2585) | Bit/dim 0.9957(1.0179) | Steps 688(684.91) | Grad Norm 2.5172(5.5180) | Total Time 10.00(10.00)
Iter 15850 | Time 5.4695(5.2488) | Bit/dim 0.9901(1.0184) | Steps 706(684.81) | Grad Norm 1.4368(5.7083) | Total Time 10.00(10.00)
Iter 15860 | Time 5.2653(5.2588) | Bit/dim 1.0328(1.0152) | Steps 688(685.23) | Grad Norm 3.7908(5.4116) | Total Time 10.00(10.00)
Iter 15870 | Time 5.3787(5.2916) | Bit/dim 1.0360(1.0142) | Steps 688(686.50) | Grad Norm 2.2465(4.9848) | Total Time 10.00(10.00)
Iter 15880 | Time 5.2534(5.2972) | Bit/dim 0.9991(1.0123) | Steps 688(685.74) | Grad Norm 7.1227(4.7616) | Total Time 10.00(10.00)
Iter 15890 | Time 5.4260(5.3197) | Bit/dim 1.0123(1.0101) | Steps 694(687.13) | Grad Norm 4.7028(4.2950) | Total Time 10.00(10.00)
validating...
Epoch 0053 | Time 69.2294, Bit/dim 1.0249
===> Using batch size 200. Total 300 iterations/epoch.
Iter 15900 | Time 5.5728(5.3123) | Bit/dim 1.0442(1.0139) | Steps 700(687.50) | Grad Norm 17.4047(5.1425) | Total Time 10.00(10.00)
Iter 15910 | Time 5.4410(5.2797) | Bit/dim 1.0325(1.0248) | Steps 700(686.76) | Grad Norm 3.4784(5.8485) | Total Time 10.00(10.00)
Iter 15920 | Time 5.4026(5.2927) | Bit/dim 1.0283(1.0274) | Steps 694(687.33) | Grad Norm 3.5809(5.7537) | Total Time 10.00(10.00)
Iter 15930 | Time 5.5941(5.3508) | Bit/dim 1.0004(1.0235) | Steps 706(692.15) | Grad Norm 1.4023(4.9066) | Total Time 10.00(10.00)
Iter 15940 | Time 5.4095(5.3770) | Bit/dim 0.9879(1.0212) | Steps 700(696.10) | Grad Norm 2.3163(4.1184) | Total Time 10.00(10.00)
Iter 15950 | Time 5.0636(5.3703) | Bit/dim 1.0207(1.0194) | Steps 694(697.08) | Grad Norm 6.9515(4.4753) | Total Time 10.00(10.00)
Iter 15960 | Time 5.4111(5.3310) | Bit/dim 0.9940(1.0193) | Steps 700(695.86) | Grad Norm 5.9285(5.2063) | Total Time 10.00(10.00)
Iter 15970 | Time 5.1401(5.2928) | Bit/dim 1.0133(1.0165) | Steps 676(692.95) | Grad Norm 3.8796(5.0502) | Total Time 10.00(10.00)
Iter 15980 | Time 5.3922(5.2994) | Bit/dim 1.0242(1.0143) | Steps 694(692.01) | Grad Norm 1.7508(4.7146) | Total Time 10.00(10.00)
Iter 15990 | Time 5.3910(5.3334) | Bit/dim 1.0267(1.0092) | Steps 694(693.58) | Grad Norm 1.7811(3.9798) | Total Time 10.00(10.00)
Iter 16000 | Time 5.3442(5.3442) | Bit/dim 1.0146(1.0050) | Steps 688(692.91) | Grad Norm 1.2735(3.3667) | Total Time 10.00(10.00)
Iter 16010 | Time 5.2783(5.3492) | Bit/dim 1.0016(1.0043) | Steps 694(693.05) | Grad Norm 5.9334(3.4478) | Total Time 10.00(10.00)
Iter 16020 | Time 5.0917(5.3244) | Bit/dim 1.0285(1.0167) | Steps 670(691.52) | Grad Norm 4.4261(4.8791) | Total Time 10.00(10.00)
Iter 16030 | Time 5.7221(5.3276) | Bit/dim 1.0595(1.0222) | Steps 718(693.22) | Grad Norm 2.0764(4.6242) | Total Time 10.00(10.00)
Iter 16040 | Time 5.4553(5.3507) | Bit/dim 0.9725(1.0197) | Steps 694(693.76) | Grad Norm 1.5963(4.6650) | Total Time 10.00(10.00)
Iter 16050 | Time 5.4417(5.3645) | Bit/dim 1.0081(1.0183) | Steps 700(694.31) | Grad Norm 1.5071(4.2472) | Total Time 10.00(10.00)
Iter 16060 | Time 5.5867(5.4087) | Bit/dim 1.0177(1.0152) | Steps 694(694.95) | Grad Norm 2.4535(3.6751) | Total Time 10.00(10.00)
Iter 16070 | Time 5.5276(5.4145) | Bit/dim 1.0171(1.0144) | Steps 688(694.34) | Grad Norm 5.2471(3.5648) | Total Time 10.00(10.00)
Iter 16080 | Time 5.4175(5.4316) | Bit/dim 1.0184(1.0130) | Steps 694(694.51) | Grad Norm 7.1723(3.7747) | Total Time 10.00(10.00)
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    # Daniel's code
    with torch.no_grad():
        trace = torch.jit.trace(model, (x, zero))
        torch.jit.save('/experiments/ffjord_trace.pth')
    print('saved')
    exit()

    z, delta_logp = model(x, zero)  # run model forward





    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # torch.save
    # exit()

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='experiments/cnf/figs/mnist_ffjord_50.pth', rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    # Daniel's code
    with torch.no_grad():
        trace = torch.jit.trace(model, (x, zero))
        torch.jit.save('/experiments/ffjord_trace.pth')
    print('saved')
    exit()

    z, delta_logp = model(x, zero)  # run model forward





    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # torch.save
    # exit()

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='experiments/cnf/figs/mnist_ffjord_50.pth', rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    # Daniel's code
    with torch.no_grad():
        trace = torch.jit.trace(model, (x, zero))
        torch.jit.save('/experiments/ffjord_trace.pth')
    print('saved')
    exit()

    z, delta_logp = model(x, zero)  # run model forward





    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # torch.save
    # exit()

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='/home/ray/Documents/ffjord/experiments/cnf/mnist_ffjord_50.pth', rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    # Daniel's code
    with torch.no_grad():
        trace = torch.jit.trace(model, (x, zero))
        torch.jit.save('/ffjord_trace.pth')
    print('SAVED')
    exit()

    z, delta_logp = model(x, zero)  # run model forward





    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # torch.save
    # exit()

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='/home/ray/Documents/ffjord/experiments/cnf/mnist_ffjord_50.pth', rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    # Daniel's code
    with torch.no_grad():
        trace = torch.jit.trace(model, (x, zero))
        torch.jit.save('/ffjord_trace.pth')
    print('SAVED')
    # exit()

    z, delta_logp = model(x, zero)  # run model forward





    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # torch.save
    # exit()

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='/home/ray/Documents/ffjord/experiments/cnf/mnist_ffjord_50.pth', rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    # Daniel's code
    with torch.no_grad():
        trace = torch.jit.trace(model, (x, zero))
        torch.jit.save('/ffjord_trace.pth')
    print('SAVED')
    # exit()

    z, delta_logp = model(x, zero)  # run model forward





    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    print("HELLLOOOOOOO")

    # torch.save
    # exit()

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            print("HELLLOOOOOOO")

            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='/home/ray/Documents/ffjord/experiments/cnf/mnist_ffjord_50.pth', rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    # Daniel's code
    with torch.no_grad():
        trace = torch.jit.trace(model, (x, zero))
        torch.jit.save('/ffjord_trace.pth')
    print('SAVED')
    # exit()

    z, delta_logp = model(x, zero)  # run model forward





    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    print("HELLLOOOOOOO")

    # torch.save
    # exit()

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            print("HELLLOOOOOOO")

            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='/home/ray/Documents/ffjord/experiments/cnf/mnist_ffjord_50.pth', rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    # Daniel's code
    # with torch.no_grad():
    #     trace = torch.jit.trace(model, (x, zero))
    #     torch.jit.save('/ffjord_trace.pth')
    # print('SAVED')
    # exit()

    z, delta_logp = model(x, zero)  # run model forward





    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    print("HELLLOOOOOOO")

    # torch.save
    # exit()

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            print("HELLLOOOOOOO")

            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='/home/ray/Documents/ffjord/experiments/cnf/mnist_ffjord_50.pth', rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 6.1711(6.1711) | Bit/dim 1.0076(1.0076) | Steps 664(664.00) | Grad Norm 2.8518(2.8518) | Total Time 10.00(10.00)
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape

def save_trace(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)
    trace = torch.jit.trace(model, (x, zero))
    torch.jit.save('/ffjord_trace.pth')
    print('SAVED')

def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    # Daniel's code
    # with torch.no_grad():
    #     trace = torch.jit.trace(model, (x, zero))
    #     torch.jit.save('/ffjord_trace.pth')
    # print('SAVED')
    # exit()

    z, delta_logp = model(x, zero)  # run model forward





    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    print("HELLLOOOOOOO")

    # torch.save
    # exit()

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            print("HELLLOOOOOOO")


            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1
        model.eval()
        save_trace(x, model)

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='/home/ray/Documents/ffjord/experiments/cnf/mnist_ffjord_50.pth', rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 6.2600(6.2600) | Bit/dim 1.0187(1.0187) | Steps 670(670.00) | Grad Norm 3.0077(3.0077) | Total Time 10.00(10.00)
/home/ray/Documents/ffjord/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape

def save_trace(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)
    trace = torch.jit.trace(model, (x, zero))
    torch.jit.save('/ffjord_trace.pth')
    print('SAVED')

def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    # Daniel's code
    # with torch.no_grad():
    #     trace = torch.jit.trace(model, (x, zero))
    #     torch.jit.save('/ffjord_trace.pth')
    # print('SAVED')
    # exit()

    z, delta_logp = model(x, zero)  # run model forward





    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    print("HELLLOOOOOOO")

    # torch.save
    # exit()

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            print("HELLLOOOOOOO")


            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1
            model.eval()
            save_trace(x, model)

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.cpu())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=200, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='/home/ray/Documents/ffjord/experiments/cnf/mnist_ffjord_50.pth', rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 6.3237(6.3237) | Bit/dim 1.0045(1.0045) | Steps 676(676.00) | Grad Norm 2.8345(2.8345) | Total Time 10.00(10.00)
